{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import pandas as pd\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Resizing, Dropout, BatchNormalization, Activation, Add, GlobalAveragePooling2D, Input, Reshape, Conv2DTranspose, Cropping2D\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "from tools import define_cnn, extract_features, build_model, predict_with_model, set_seed, build_model_and_predict, extract_features_2\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load clean gdp data, keep only year, region and real_gdp columns\n",
    "poland = pd.read_csv(\"data/tabular_data_poland.csv\")\n",
    "log_bin_columns = [\"allangle_snow_free_hq\" + \"_log_\" + str(i) for i in range(1, 11)]\n",
    "\n",
    "# get the data for 2021, 2022 and before 2022\n",
    "poland_2022 = poland[poland[\"year\"] == 2022]\n",
    "poland = poland[poland[\"year\"].astype(int) < 2022]\n",
    "poland_2021 = poland[poland[\"year\"] == 2021]\n",
    "poland_2021.reset_index(drop=True, inplace=True)\n",
    "poland_2022.reset_index(drop=True, inplace=True)\n",
    "poland.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Initialise a three dimensional array to store the images with the shape (number of images, height, width, channels)\n",
    "X = np.zeros((len(poland), 609, 911, 1))\n",
    "y = np.zeros(len(poland))\n",
    "\n",
    "# load the images\n",
    "for i in range(len(poland)):\n",
    "\n",
    "    # get year, region, and gdp\n",
    "    year = poland[\"year\"][i]\n",
    "    region = poland[\"region\"][i]\n",
    "    gdp_value = poland[\"real_gdp\"][i]\n",
    "\n",
    "    # load the image\n",
    "    file_name = f\"{year}_{region}_hq.h5\"\n",
    "    file_path = f\"data/annual_region_images/{file_name}\"\n",
    "    \n",
    "    with h5py.File(file_path, 'r') as annual_region:\n",
    "        allangle_snow_free = annual_region[\"AllAngle_Composite_Snow_Free\"][:]\n",
    "\n",
    "    # add the values\n",
    "    y[i] = gdp_value\n",
    "    X[i, :, :, 0] = allangle_snow_free\n",
    "\n",
    "# normalise the images and gdp data\n",
    "maximum_x = X.max()\n",
    "X = X / maximum_x\n",
    "\n",
    "maximum_y = y.max()\n",
    "y = y / maximum_y\n",
    "\n",
    "# get indices for observations with  year = 2021: this is the test set\n",
    "test_indices = np.where(poland[\"year\"] == 2021)[0]\n",
    "train_indices = np.where(poland[\"year\"] != 2021)[0]\n",
    "\n",
    "# get the train and test sets\n",
    "X_train, y_train, X_test, y_test = X[train_indices], y[train_indices], X[test_indices], y[test_indices]\n",
    "\n",
    "# get the prediction data\n",
    "X_pred = np.zeros((len(poland_2022),  609, 911, 1))\n",
    "for i in range(len(poland_2022)):\n",
    "\n",
    "    year = poland_2022[\"year\"][i]\n",
    "    region = poland_2022[\"region\"][i]\n",
    "\n",
    "    file_name = f\"{year}_{region}_hq.h5\"\n",
    "    file_path = f\"data/annual_region_images/{file_name}\"\n",
    "\n",
    "    with h5py.File(file_path, 'r') as annual_region:\n",
    "        allangle_snow_free = annual_region[\"AllAngle_Composite_Snow_Free\"][:]\n",
    "    \n",
    "    X_pred[i, :, :, 0] = allangle_snow_free\n",
    "\n",
    "X_pred = X_pred / maximum_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\jakub\\anaconda\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\jakub\\anaconda\\lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:From c:\\Users\\jakub\\anaconda\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\jakub\\anaconda\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "2/2 [==============================] - 8s 2s/step - loss: 0.1004 - mae: 0.2383 - val_loss: 0.0223 - val_mae: 0.1082\n",
      "Epoch 2/50\n",
      "2/2 [==============================] - 1s 620ms/step - loss: 0.0627 - mae: 0.1836 - val_loss: 0.0291 - val_mae: 0.1458\n",
      "Epoch 3/50\n",
      "2/2 [==============================] - 1s 687ms/step - loss: 0.0489 - mae: 0.1546 - val_loss: 0.0142 - val_mae: 0.0943\n",
      "Epoch 4/50\n",
      "2/2 [==============================] - 1s 714ms/step - loss: 0.0484 - mae: 0.1364 - val_loss: 0.0172 - val_mae: 0.1145\n",
      "Epoch 5/50\n",
      "2/2 [==============================] - 1s 768ms/step - loss: 0.0470 - mae: 0.1539 - val_loss: 0.0217 - val_mae: 0.1364\n",
      "Epoch 6/50\n",
      "2/2 [==============================] - 1s 594ms/step - loss: 0.0443 - mae: 0.1599 - val_loss: 0.0161 - val_mae: 0.1153\n",
      "Epoch 7/50\n",
      "2/2 [==============================] - 2s 968ms/step - loss: 0.0459 - mae: 0.1399 - val_loss: 0.0143 - val_mae: 0.1036\n",
      "Epoch 8/50\n",
      "2/2 [==============================] - 1s 762ms/step - loss: 0.0440 - mae: 0.1359 - val_loss: 0.0174 - val_mae: 0.1261\n",
      "Epoch 9/50\n",
      "2/2 [==============================] - 1s 730ms/step - loss: 0.0416 - mae: 0.1485 - val_loss: 0.0206 - val_mae: 0.1345\n",
      "Epoch 10/50\n",
      "2/2 [==============================] - 1s 679ms/step - loss: 0.0431 - mae: 0.1582 - val_loss: 0.0157 - val_mae: 0.1197\n",
      "Epoch 11/50\n",
      "2/2 [==============================] - 1s 680ms/step - loss: 0.0414 - mae: 0.1349 - val_loss: 0.0134 - val_mae: 0.1060\n",
      "Epoch 12/50\n",
      "2/2 [==============================] - 1s 610ms/step - loss: 0.0410 - mae: 0.1316 - val_loss: 0.0174 - val_mae: 0.1266\n",
      "Epoch 13/50\n",
      "2/2 [==============================] - 1s 601ms/step - loss: 0.0401 - mae: 0.1474 - val_loss: 0.0189 - val_mae: 0.1309\n",
      "Epoch 14/50\n",
      "2/2 [==============================] - 1s 618ms/step - loss: 0.0391 - mae: 0.1419 - val_loss: 0.0138 - val_mae: 0.1115\n",
      "Epoch 15/50\n",
      "2/2 [==============================] - 1s 619ms/step - loss: 0.0387 - mae: 0.1306 - val_loss: 0.0151 - val_mae: 0.1182\n",
      "Epoch 16/50\n",
      "2/2 [==============================] - 1s 649ms/step - loss: 0.0376 - mae: 0.1345 - val_loss: 0.0169 - val_mae: 0.1255\n",
      "Epoch 17/50\n",
      "2/2 [==============================] - 1s 658ms/step - loss: 0.0371 - mae: 0.1342 - val_loss: 0.0159 - val_mae: 0.1215\n",
      "Epoch 18/50\n",
      "2/2 [==============================] - 1s 632ms/step - loss: 0.0355 - mae: 0.1319 - val_loss: 0.0149 - val_mae: 0.1181\n",
      "Epoch 19/50\n",
      "2/2 [==============================] - 1s 591ms/step - loss: 0.0344 - mae: 0.1281 - val_loss: 0.0148 - val_mae: 0.1178\n",
      "Epoch 20/50\n",
      "2/2 [==============================] - 1s 620ms/step - loss: 0.0341 - mae: 0.1210 - val_loss: 0.0159 - val_mae: 0.1199\n",
      "Epoch 21/50\n",
      "2/2 [==============================] - 1s 572ms/step - loss: 0.0318 - mae: 0.1230 - val_loss: 0.0168 - val_mae: 0.1240\n",
      "Epoch 22/50\n",
      "2/2 [==============================] - 1s 546ms/step - loss: 0.0306 - mae: 0.1218 - val_loss: 0.0139 - val_mae: 0.1137\n",
      "Epoch 23/50\n",
      "2/2 [==============================] - 1s 629ms/step - loss: 0.0285 - mae: 0.1126 - val_loss: 0.0150 - val_mae: 0.1174\n",
      "Epoch 24/50\n",
      "2/2 [==============================] - 1s 581ms/step - loss: 0.0312 - mae: 0.1295 - val_loss: 0.0133 - val_mae: 0.0930\n",
      "Epoch 25/50\n",
      "2/2 [==============================] - 1s 642ms/step - loss: 0.0301 - mae: 0.1088 - val_loss: 0.0140 - val_mae: 0.1131\n",
      "Epoch 26/50\n",
      "2/2 [==============================] - 1s 610ms/step - loss: 0.0227 - mae: 0.1027 - val_loss: 0.0220 - val_mae: 0.1359\n",
      "Epoch 27/50\n",
      "2/2 [==============================] - 1s 627ms/step - loss: 0.0241 - mae: 0.1148 - val_loss: 0.0131 - val_mae: 0.1022\n",
      "Epoch 28/50\n",
      "2/2 [==============================] - 1s 640ms/step - loss: 0.0235 - mae: 0.0991 - val_loss: 0.0134 - val_mae: 0.1096\n",
      "Epoch 29/50\n",
      "2/2 [==============================] - 1s 665ms/step - loss: 0.0215 - mae: 0.1065 - val_loss: 0.0142 - val_mae: 0.1155\n",
      "Epoch 30/50\n",
      "2/2 [==============================] - 1s 544ms/step - loss: 0.0180 - mae: 0.0943 - val_loss: 0.0125 - val_mae: 0.0964\n",
      "Epoch 31/50\n",
      "2/2 [==============================] - 1s 548ms/step - loss: 0.0172 - mae: 0.0886 - val_loss: 0.0120 - val_mae: 0.0961\n",
      "Epoch 32/50\n",
      "2/2 [==============================] - 1s 610ms/step - loss: 0.0133 - mae: 0.0824 - val_loss: 0.0163 - val_mae: 0.1216\n",
      "Epoch 33/50\n",
      "2/2 [==============================] - 1s 599ms/step - loss: 0.0128 - mae: 0.0825 - val_loss: 0.0121 - val_mae: 0.0943\n",
      "Epoch 34/50\n",
      "2/2 [==============================] - 1s 573ms/step - loss: 0.0101 - mae: 0.0735 - val_loss: 0.0134 - val_mae: 0.1119\n",
      "Epoch 35/50\n",
      "2/2 [==============================] - 1s 631ms/step - loss: 0.0092 - mae: 0.0746 - val_loss: 0.0124 - val_mae: 0.0908\n",
      "Epoch 36/50\n",
      "2/2 [==============================] - 1s 662ms/step - loss: 0.0077 - mae: 0.0689 - val_loss: 0.0114 - val_mae: 0.0962\n",
      "Epoch 37/50\n",
      "2/2 [==============================] - 1s 614ms/step - loss: 0.0064 - mae: 0.0639 - val_loss: 0.0114 - val_mae: 0.0912\n",
      "Epoch 38/50\n",
      "2/2 [==============================] - 1s 624ms/step - loss: 0.0052 - mae: 0.0591 - val_loss: 0.0123 - val_mae: 0.0872\n",
      "Epoch 39/50\n",
      "2/2 [==============================] - 1s 575ms/step - loss: 0.0041 - mae: 0.0523 - val_loss: 0.0113 - val_mae: 0.0980\n",
      "Epoch 40/50\n",
      "2/2 [==============================] - 1s 604ms/step - loss: 0.0033 - mae: 0.0462 - val_loss: 0.0140 - val_mae: 0.0824\n",
      "Epoch 41/50\n",
      "2/2 [==============================] - 1s 695ms/step - loss: 0.0035 - mae: 0.0493 - val_loss: 0.0112 - val_mae: 0.0858\n",
      "Epoch 42/50\n",
      "2/2 [==============================] - 1s 673ms/step - loss: 0.0028 - mae: 0.0439 - val_loss: 0.0113 - val_mae: 0.0862\n",
      "Epoch 43/50\n",
      "2/2 [==============================] - 1s 695ms/step - loss: 0.0023 - mae: 0.0394 - val_loss: 0.0112 - val_mae: 0.0865\n",
      "Epoch 44/50\n",
      "2/2 [==============================] - 1s 727ms/step - loss: 0.0020 - mae: 0.0376 - val_loss: 0.0117 - val_mae: 0.0823\n",
      "Epoch 45/50\n",
      "2/2 [==============================] - 1s 751ms/step - loss: 0.0016 - mae: 0.0326 - val_loss: 0.0107 - val_mae: 0.0922\n",
      "Epoch 46/50\n",
      "2/2 [==============================] - 1s 676ms/step - loss: 0.0017 - mae: 0.0330 - val_loss: 0.0119 - val_mae: 0.0812\n",
      "Epoch 47/50\n",
      "2/2 [==============================] - 1s 663ms/step - loss: 0.0018 - mae: 0.0350 - val_loss: 0.0110 - val_mae: 0.0848\n",
      "Epoch 48/50\n",
      "2/2 [==============================] - 1s 607ms/step - loss: 0.0014 - mae: 0.0304 - val_loss: 0.0106 - val_mae: 0.0892\n",
      "Epoch 49/50\n",
      "2/2 [==============================] - 1s 655ms/step - loss: 0.0015 - mae: 0.0300 - val_loss: 0.0122 - val_mae: 0.0799\n",
      "Epoch 50/50\n",
      "2/2 [==============================] - 1s 762ms/step - loss: 0.0013 - mae: 0.0282 - val_loss: 0.0106 - val_mae: 0.0859\n",
      "5/5 [==============================] - 1s 79ms/step\n",
      "1/1 [==============================] - 0s 275ms/step\n"
     ]
    }
   ],
   "source": [
    "poland = pd.read_csv(\"data/tabular_data_poland.csv\")\n",
    "poland = poland[poland[\"year\"] < 2023]\n",
    "poland_sum = poland[['year', 'region', 'allangle_snow_free_hq_sum', 'allangle_snow_free_hq_mean'] + log_bin_columns]\n",
    "\n",
    "model = define_cnn(n_features = 4, n_conv = 2, n_dense = 4, input_shape = (609, 911, 1), res_x = 305, res_y = 455)\n",
    "model.compile(optimizer=Adam(), loss='mean_squared_error', metrics=['mae'])\n",
    "model.fit(X, y, epochs=50, batch_size=64, validation_split=0.2)\n",
    "\n",
    "poland_stage_2, poland_pred = extract_features_2(model, poland, 2022, X, X_pred, 4)\n",
    "selected_columns =  [\"feature_\" + str(i) for i in range(1, 5)] + log_bin_columns + ['allangle_snow_free_hq_sum', 'allangle_snow_free_hq_mean']\n",
    "\n",
    "full_data = pd.concat([poland_stage_2, poland_pred])\n",
    "full_data.sort_values(by=['region', 'year'], inplace=True)\n",
    "full_data = pd.merge(full_data, poland_sum, on=['year', 'region'])\n",
    "full_data_diff = full_data.groupby('region').diff()\n",
    "full_data_diff['region'] = full_data['region']\n",
    "full_data_diff['year'] = full_data['year']\n",
    "full_data_diff.reset_index(drop=True, inplace=True)\n",
    "full_data_diff = full_data_diff[full_data_diff[\"year\"] != 2012]\n",
    "\n",
    "train_data_diff = full_data_diff[full_data_diff[\"year\"] != 2022]\n",
    "pred_data_diff = full_data_diff[full_data_diff[\"year\"] == 2022]\n",
    "pred_data_diff.reset_index(drop=True, inplace=True)\n",
    "train_data_diff.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a neural network\n",
    "train_data_nn = train_data_diff.copy()\n",
    "pred_data_nn = pred_data_diff.copy()\n",
    "\n",
    "train_data_nn = pd.get_dummies(train_data_nn, columns=[\"region\"])\n",
    "pred_data_nn = pd.get_dummies(pred_data_nn, columns=[\"region\"])\n",
    "\n",
    "X_train_nn = train_data_nn.drop(columns=[\"year\", \"real_gdp\"])\n",
    "X_train_nn = X_train_nn.drop(columns=log_bin_columns)\n",
    "X_train_nn = X_train_nn.drop(columns=[\"allangle_snow_free_hq_mean\"])\n",
    "y_train_nn = train_data_nn[\"real_gdp\"]\n",
    "\n",
    "X_test_nn = pred_data_nn.drop(columns=[\"year\", \"real_gdp\"])\n",
    "X_test_nn = X_test_nn.drop(columns=log_bin_columns)\n",
    "X_test_nn = X_test_nn.drop(columns=[\"allangle_snow_free_hq_mean\"])\n",
    "y_test_nn = pred_data_nn[\"real_gdp\"]\n",
    "\n",
    "X_train_nn = np.array(X_train_nn, dtype=np.float32)\n",
    "X_test_nn = np.array(X_test_nn, dtype=np.float32)\n",
    "y_train_nn = np.array(y_train_nn, dtype=np.float32)\n",
    "y_test_nn = np.array(y_test_nn, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\jakub\\anaconda\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/500\n",
      "2/2 [==============================] - 1s 210ms/step - loss: 60760072.0000 - mae: 5033.7319 - val_loss: 25966238.0000 - val_mae: 3766.6853\n",
      "Epoch 2/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 60759448.0000 - mae: 5033.6890 - val_loss: 25966002.0000 - val_mae: 3766.6545\n",
      "Epoch 3/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 60758808.0000 - mae: 5033.6455 - val_loss: 25965752.0000 - val_mae: 3766.6206\n",
      "Epoch 4/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 60758240.0000 - mae: 5033.6030 - val_loss: 25965484.0000 - val_mae: 3766.5852\n",
      "Epoch 5/500\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 60757624.0000 - mae: 5033.5605 - val_loss: 25965214.0000 - val_mae: 3766.5549\n",
      "Epoch 6/500\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 60756960.0000 - mae: 5033.5176 - val_loss: 25964932.0000 - val_mae: 3766.5237\n",
      "Epoch 7/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 60756340.0000 - mae: 5033.4731 - val_loss: 25964634.0000 - val_mae: 3766.4900\n",
      "Epoch 8/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 60755784.0000 - mae: 5033.4307 - val_loss: 25964308.0000 - val_mae: 3766.4546\n",
      "Epoch 9/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 60755048.0000 - mae: 5033.3853 - val_loss: 25963970.0000 - val_mae: 3766.4185\n",
      "Epoch 10/500\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 60754356.0000 - mae: 5033.3369 - val_loss: 25963612.0000 - val_mae: 3766.3813\n",
      "Epoch 11/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 60753568.0000 - mae: 5033.2900 - val_loss: 25963228.0000 - val_mae: 3766.3447\n",
      "Epoch 12/500\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 60752780.0000 - mae: 5033.2349 - val_loss: 25962832.0000 - val_mae: 3766.3076\n",
      "Epoch 13/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 60752040.0000 - mae: 5033.1870 - val_loss: 25962382.0000 - val_mae: 3766.2666\n",
      "Epoch 14/500\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 60751048.0000 - mae: 5033.1260 - val_loss: 25961894.0000 - val_mae: 3766.2231\n",
      "Epoch 15/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 60750164.0000 - mae: 5033.0640 - val_loss: 25961382.0000 - val_mae: 3766.1768\n",
      "Epoch 16/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 60749220.0000 - mae: 5033.0010 - val_loss: 25960846.0000 - val_mae: 3766.1279\n",
      "Epoch 17/500\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 60748036.0000 - mae: 5032.9292 - val_loss: 25960284.0000 - val_mae: 3766.0776\n",
      "Epoch 18/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 60746884.0000 - mae: 5032.8589 - val_loss: 25959716.0000 - val_mae: 3766.0259\n",
      "Epoch 19/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 60745692.0000 - mae: 5032.7783 - val_loss: 25959112.0000 - val_mae: 3765.9712\n",
      "Epoch 20/500\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 60744216.0000 - mae: 5032.6943 - val_loss: 25958502.0000 - val_mae: 3765.9155\n",
      "Epoch 21/500\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 60742816.0000 - mae: 5032.6050 - val_loss: 25957868.0000 - val_mae: 3765.8577\n",
      "Epoch 22/500\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 60741356.0000 - mae: 5032.5103 - val_loss: 25957210.0000 - val_mae: 3765.7979\n",
      "Epoch 23/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 60739556.0000 - mae: 5032.4097 - val_loss: 25956542.0000 - val_mae: 3765.7368\n",
      "Epoch 24/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 60737676.0000 - mae: 5032.3022 - val_loss: 25955852.0000 - val_mae: 3765.6738\n",
      "Epoch 25/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 60735700.0000 - mae: 5032.1895 - val_loss: 25955144.0000 - val_mae: 3765.6086\n",
      "Epoch 26/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 60733812.0000 - mae: 5032.0776 - val_loss: 25954404.0000 - val_mae: 3765.5408\n",
      "Epoch 27/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 60731828.0000 - mae: 5031.9502 - val_loss: 25953644.0000 - val_mae: 3765.4709\n",
      "Epoch 28/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 60729276.0000 - mae: 5031.8218 - val_loss: 25952870.0000 - val_mae: 3765.3992\n",
      "Epoch 29/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 60726680.0000 - mae: 5031.6782 - val_loss: 25952066.0000 - val_mae: 3765.3247\n",
      "Epoch 30/500\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 60724240.0000 - mae: 5031.5273 - val_loss: 25951226.0000 - val_mae: 3765.2463\n",
      "Epoch 31/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 60721696.0000 - mae: 5031.3784 - val_loss: 25950354.0000 - val_mae: 3765.1641\n",
      "Epoch 32/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 60718552.0000 - mae: 5031.2075 - val_loss: 25949458.0000 - val_mae: 3765.0798\n",
      "Epoch 33/500\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 60715260.0000 - mae: 5031.0254 - val_loss: 25948536.0000 - val_mae: 3764.9924\n",
      "Epoch 34/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 60712024.0000 - mae: 5030.8394 - val_loss: 25947568.0000 - val_mae: 3764.9011\n",
      "Epoch 35/500\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 60708212.0000 - mae: 5030.6401 - val_loss: 25946566.0000 - val_mae: 3764.8066\n",
      "Epoch 36/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 60704808.0000 - mae: 5030.4292 - val_loss: 25945522.0000 - val_mae: 3764.7075\n",
      "Epoch 37/500\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 60700788.0000 - mae: 5030.2124 - val_loss: 25944428.0000 - val_mae: 3764.6038\n",
      "Epoch 38/500\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 60695872.0000 - mae: 5029.9619 - val_loss: 25943314.0000 - val_mae: 3764.4971\n",
      "Epoch 39/500\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 60691776.0000 - mae: 5029.7227 - val_loss: 25942126.0000 - val_mae: 3764.3843\n",
      "Epoch 40/500\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 60686732.0000 - mae: 5029.4712 - val_loss: 25940900.0000 - val_mae: 3764.2673\n",
      "Epoch 41/500\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 60682060.0000 - mae: 5029.1870 - val_loss: 25939626.0000 - val_mae: 3764.1462\n",
      "Epoch 42/500\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 60676164.0000 - mae: 5028.8843 - val_loss: 25938306.0000 - val_mae: 3764.0198\n",
      "Epoch 43/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 60669508.0000 - mae: 5028.5557 - val_loss: 25936942.0000 - val_mae: 3763.8899\n",
      "Epoch 44/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 60663408.0000 - mae: 5028.2510 - val_loss: 25935494.0000 - val_mae: 3763.7520\n",
      "Epoch 45/500\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 60656480.0000 - mae: 5027.8882 - val_loss: 25933974.0000 - val_mae: 3763.6072\n",
      "Epoch 46/500\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 60649928.0000 - mae: 5027.5176 - val_loss: 25932358.0000 - val_mae: 3763.4546\n",
      "Epoch 47/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 60640736.0000 - mae: 5027.0894 - val_loss: 25930696.0000 - val_mae: 3763.2979\n",
      "Epoch 48/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 60632576.0000 - mae: 5026.6836 - val_loss: 25928932.0000 - val_mae: 3763.1335\n",
      "Epoch 49/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 60623936.0000 - mae: 5026.2441 - val_loss: 25927086.0000 - val_mae: 3762.9614\n",
      "Epoch 50/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 60614332.0000 - mae: 5025.7759 - val_loss: 25925118.0000 - val_mae: 3762.7795\n",
      "Epoch 51/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 60604244.0000 - mae: 5025.2319 - val_loss: 25923066.0000 - val_mae: 3762.5911\n",
      "Epoch 52/500\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 60593436.0000 - mae: 5024.7085 - val_loss: 25920914.0000 - val_mae: 3762.3936\n",
      "Epoch 53/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 60581364.0000 - mae: 5024.1353 - val_loss: 25918674.0000 - val_mae: 3762.1877\n",
      "Epoch 54/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 60568772.0000 - mae: 5023.5093 - val_loss: 25916354.0000 - val_mae: 3761.9746\n",
      "Epoch 55/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 60556712.0000 - mae: 5022.8887 - val_loss: 25913940.0000 - val_mae: 3761.7532\n",
      "Epoch 56/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 60541056.0000 - mae: 5022.1357 - val_loss: 25911476.0000 - val_mae: 3761.5269\n",
      "Epoch 57/500\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 60526628.0000 - mae: 5021.4360 - val_loss: 25908900.0000 - val_mae: 3761.2917\n",
      "Epoch 58/500\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 60511736.0000 - mae: 5020.6831 - val_loss: 25906214.0000 - val_mae: 3761.0474\n",
      "Epoch 59/500\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 60495304.0000 - mae: 5019.8813 - val_loss: 25903476.0000 - val_mae: 3760.7991\n",
      "Epoch 60/500\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 60478120.0000 - mae: 5019.0029 - val_loss: 25900664.0000 - val_mae: 3760.5447\n",
      "Epoch 61/500\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 60458012.0000 - mae: 5018.0674 - val_loss: 25897808.0000 - val_mae: 3760.2866\n",
      "Epoch 62/500\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 60437496.0000 - mae: 5017.1206 - val_loss: 25894860.0000 - val_mae: 3760.0215\n",
      "Epoch 63/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 60415760.0000 - mae: 5016.1099 - val_loss: 25891824.0000 - val_mae: 3759.7478\n",
      "Epoch 64/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 60393712.0000 - mae: 5015.0806 - val_loss: 25888676.0000 - val_mae: 3759.4644\n",
      "Epoch 65/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 60370528.0000 - mae: 5013.9673 - val_loss: 25885410.0000 - val_mae: 3759.1709\n",
      "Epoch 66/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 60348256.0000 - mae: 5012.8428 - val_loss: 25882020.0000 - val_mae: 3758.8662\n",
      "Epoch 67/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 60323944.0000 - mae: 5011.5850 - val_loss: 25878548.0000 - val_mae: 3758.5552\n",
      "Epoch 68/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 60290536.0000 - mae: 5010.2261 - val_loss: 25875110.0000 - val_mae: 3758.2476\n",
      "Epoch 69/500\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 60266984.0000 - mae: 5008.9966 - val_loss: 25871494.0000 - val_mae: 3757.9246\n",
      "Epoch 70/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 60235488.0000 - mae: 5007.5664 - val_loss: 25867812.0000 - val_mae: 3757.5945\n",
      "Epoch 71/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 60201772.0000 - mae: 5006.0801 - val_loss: 25864048.0000 - val_mae: 3757.2583\n",
      "Epoch 72/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 60167788.0000 - mae: 5004.5186 - val_loss: 25860166.0000 - val_mae: 3756.9114\n",
      "Epoch 73/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 60139528.0000 - mae: 5003.0454 - val_loss: 25856106.0000 - val_mae: 3756.5483\n",
      "Epoch 74/500\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 60101856.0000 - mae: 5001.2227 - val_loss: 25852002.0000 - val_mae: 3756.1816\n",
      "Epoch 75/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 60061056.0000 - mae: 4999.4727 - val_loss: 25847848.0000 - val_mae: 3755.8105\n",
      "Epoch 76/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 60021296.0000 - mae: 4997.6196 - val_loss: 25843578.0000 - val_mae: 3755.4280\n",
      "Epoch 77/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 59982844.0000 - mae: 4995.7705 - val_loss: 25839146.0000 - val_mae: 3755.0305\n",
      "Epoch 78/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 59932664.0000 - mae: 4993.7197 - val_loss: 25834696.0000 - val_mae: 3754.6321\n",
      "Epoch 79/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 59892156.0000 - mae: 4991.7168 - val_loss: 25830032.0000 - val_mae: 3754.2144\n",
      "Epoch 80/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 59836560.0000 - mae: 4989.3223 - val_loss: 25825364.0000 - val_mae: 3753.7959\n",
      "Epoch 81/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 59788608.0000 - mae: 4987.1543 - val_loss: 25820478.0000 - val_mae: 3753.3579\n",
      "Epoch 82/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 59736324.0000 - mae: 4984.7842 - val_loss: 25815420.0000 - val_mae: 3752.9043\n",
      "Epoch 83/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 59680540.0000 - mae: 4982.2412 - val_loss: 25810210.0000 - val_mae: 3752.4368\n",
      "Epoch 84/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 59619736.0000 - mae: 4979.5640 - val_loss: 25804866.0000 - val_mae: 3751.9580\n",
      "Epoch 85/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 59558804.0000 - mae: 4976.7949 - val_loss: 25799360.0000 - val_mae: 3751.4656\n",
      "Epoch 86/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 59502132.0000 - mae: 4974.0825 - val_loss: 25793636.0000 - val_mae: 3750.9541\n",
      "Epoch 87/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 59427212.0000 - mae: 4970.9741 - val_loss: 25787882.0000 - val_mae: 3750.4419\n",
      "Epoch 88/500\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 59358948.0000 - mae: 4967.9644 - val_loss: 25781982.0000 - val_mae: 3749.9177\n",
      "Epoch 89/500\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 59281856.0000 - mae: 4964.5781 - val_loss: 25775982.0000 - val_mae: 3749.3848\n",
      "Epoch 90/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 59219716.0000 - mae: 4961.5479 - val_loss: 25769724.0000 - val_mae: 3748.8281\n",
      "Epoch 91/500\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 59140344.0000 - mae: 4958.1206 - val_loss: 25763350.0000 - val_mae: 3748.2620\n",
      "Epoch 92/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 59050636.0000 - mae: 4954.2437 - val_loss: 25756930.0000 - val_mae: 3747.6929\n",
      "Epoch 93/500\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 58965588.0000 - mae: 4950.4033 - val_loss: 25750372.0000 - val_mae: 3747.1150\n",
      "Epoch 94/500\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 58873880.0000 - mae: 4946.4165 - val_loss: 25743682.0000 - val_mae: 3746.5266\n",
      "Epoch 95/500\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 58777860.0000 - mae: 4942.4600 - val_loss: 25736830.0000 - val_mae: 3745.9268\n",
      "Epoch 96/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 58685940.0000 - mae: 4938.2163 - val_loss: 25729750.0000 - val_mae: 3745.3076\n",
      "Epoch 97/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 58590644.0000 - mae: 4934.1099 - val_loss: 25722482.0000 - val_mae: 3744.6726\n",
      "Epoch 98/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 58492944.0000 - mae: 4929.5908 - val_loss: 25715058.0000 - val_mae: 3744.0242\n",
      "Epoch 99/500\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 58365140.0000 - mae: 4924.4429 - val_loss: 25707712.0000 - val_mae: 3743.3821\n",
      "Epoch 100/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 58276356.0000 - mae: 4920.1357 - val_loss: 25700092.0000 - val_mae: 3742.7161\n",
      "Epoch 101/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 58151780.0000 - mae: 4914.8042 - val_loss: 25692472.0000 - val_mae: 3742.0515\n",
      "Epoch 102/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 58021808.0000 - mae: 4909.5347 - val_loss: 25684750.0000 - val_mae: 3741.3794\n",
      "Epoch 103/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 57907076.0000 - mae: 4904.2886 - val_loss: 25676756.0000 - val_mae: 3740.6855\n",
      "Epoch 104/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 57772204.0000 - mae: 4898.7251 - val_loss: 25668676.0000 - val_mae: 3739.9849\n",
      "Epoch 105/500\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 57649660.0000 - mae: 4893.2158 - val_loss: 25660314.0000 - val_mae: 3739.2607\n",
      "Epoch 106/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 57505688.0000 - mae: 4887.2529 - val_loss: 25651908.0000 - val_mae: 3738.5344\n",
      "Epoch 107/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 57372600.0000 - mae: 4881.5693 - val_loss: 25643246.0000 - val_mae: 3737.7893\n",
      "Epoch 108/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 57226276.0000 - mae: 4875.3716 - val_loss: 25634424.0000 - val_mae: 3737.0334\n",
      "Epoch 109/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 57092568.0000 - mae: 4869.3374 - val_loss: 25625458.0000 - val_mae: 3736.2671\n",
      "Epoch 110/500\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 56909532.0000 - mae: 4861.7905 - val_loss: 25616658.0000 - val_mae: 3735.5178\n",
      "Epoch 111/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 56773660.0000 - mae: 4855.8799 - val_loss: 25607488.0000 - val_mae: 3734.7378\n",
      "Epoch 112/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 56600656.0000 - mae: 4848.2925 - val_loss: 25598206.0000 - val_mae: 3733.9534\n",
      "Epoch 113/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 56418604.0000 - mae: 4841.1294 - val_loss: 25588886.0000 - val_mae: 3733.1665\n",
      "Epoch 114/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 56248880.0000 - mae: 4834.2749 - val_loss: 25579260.0000 - val_mae: 3732.3540\n",
      "Epoch 115/500\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 56092728.0000 - mae: 4827.1650 - val_loss: 25569236.0000 - val_mae: 3731.5120\n",
      "Epoch 116/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 55902448.0000 - mae: 4819.9409 - val_loss: 25559244.0000 - val_mae: 3730.6741\n",
      "Epoch 117/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 55719772.0000 - mae: 4812.1875 - val_loss: 25549074.0000 - val_mae: 3729.8186\n",
      "Epoch 118/500\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 55524344.0000 - mae: 4804.4517 - val_loss: 25538794.0000 - val_mae: 3728.9561\n",
      "Epoch 119/500\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 55302924.0000 - mae: 4796.0557 - val_loss: 25528594.0000 - val_mae: 3728.0981\n",
      "Epoch 120/500\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 55132748.0000 - mae: 4788.5972 - val_loss: 25517966.0000 - val_mae: 3727.2031\n",
      "Epoch 121/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 54891840.0000 - mae: 4779.2983 - val_loss: 25507478.0000 - val_mae: 3726.3210\n",
      "Epoch 122/500\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 54713288.0000 - mae: 4771.3267 - val_loss: 25496536.0000 - val_mae: 3725.4041\n",
      "Epoch 123/500\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 54465704.0000 - mae: 4762.3267 - val_loss: 25485680.0000 - val_mae: 3724.4993\n",
      "Epoch 124/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 54235928.0000 - mae: 4752.9565 - val_loss: 25474602.0000 - val_mae: 3723.5828\n",
      "Epoch 125/500\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 54004340.0000 - mae: 4743.8335 - val_loss: 25463274.0000 - val_mae: 3722.6450\n",
      "Epoch 126/500\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 53772564.0000 - mae: 4734.1792 - val_loss: 25451672.0000 - val_mae: 3721.6853\n",
      "Epoch 127/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 53552564.0000 - mae: 4724.4780 - val_loss: 25439700.0000 - val_mae: 3720.7300\n",
      "Epoch 128/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 53299512.0000 - mae: 4714.7026 - val_loss: 25427678.0000 - val_mae: 3719.7925\n",
      "Epoch 129/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 53043360.0000 - mae: 4704.7954 - val_loss: 25415400.0000 - val_mae: 3718.8354\n",
      "Epoch 130/500\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 52771100.0000 - mae: 4693.2295 - val_loss: 25402904.0000 - val_mae: 3717.8616\n",
      "Epoch 131/500\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 52520180.0000 - mae: 4682.4517 - val_loss: 25390050.0000 - val_mae: 3716.8601\n",
      "Epoch 132/500\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 52237684.0000 - mae: 4670.0674 - val_loss: 25377062.0000 - val_mae: 3715.8516\n",
      "Epoch 133/500\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 51970760.0000 - mae: 4659.0596 - val_loss: 25363806.0000 - val_mae: 3714.8276\n",
      "Epoch 134/500\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 51662596.0000 - mae: 4647.0815 - val_loss: 25350596.0000 - val_mae: 3713.8103\n",
      "Epoch 135/500\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 51384716.0000 - mae: 4635.1191 - val_loss: 25337016.0000 - val_mae: 3712.7646\n",
      "Epoch 136/500\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 51088148.0000 - mae: 4622.1533 - val_loss: 25323262.0000 - val_mae: 3711.7058\n",
      "Epoch 137/500\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 50812760.0000 - mae: 4610.6621 - val_loss: 25309166.0000 - val_mae: 3710.6267\n",
      "Epoch 138/500\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 50467144.0000 - mae: 4595.6880 - val_loss: 25295166.0000 - val_mae: 3709.5566\n",
      "Epoch 139/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 50166360.0000 - mae: 4583.5317 - val_loss: 25281010.0000 - val_mae: 3708.4763\n",
      "Epoch 140/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 49872452.0000 - mae: 4570.4536 - val_loss: 25266578.0000 - val_mae: 3707.3755\n",
      "Epoch 141/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 49511344.0000 - mae: 4555.4028 - val_loss: 25252424.0000 - val_mae: 3706.3032\n",
      "Epoch 142/500\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 49189212.0000 - mae: 4541.6333 - val_loss: 25237870.0000 - val_mae: 3705.1997\n",
      "Epoch 143/500\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 48861248.0000 - mae: 4527.2114 - val_loss: 25223024.0000 - val_mae: 3704.0759\n",
      "Epoch 144/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 48490496.0000 - mae: 4511.1870 - val_loss: 25208100.0000 - val_mae: 3702.9417\n",
      "Epoch 145/500\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 48163448.0000 - mae: 4496.8599 - val_loss: 25192728.0000 - val_mae: 3701.7717\n",
      "Epoch 146/500\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 47806216.0000 - mae: 4480.4531 - val_loss: 25177140.0000 - val_mae: 3700.5828\n",
      "Epoch 147/500\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 47410736.0000 - mae: 4463.3169 - val_loss: 25161510.0000 - val_mae: 3699.3855\n",
      "Epoch 148/500\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 47069684.0000 - mae: 4448.1211 - val_loss: 25145406.0000 - val_mae: 3698.1487\n",
      "Epoch 149/500\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 46786080.0000 - mae: 4433.5854 - val_loss: 25128634.0000 - val_mae: 3696.8650\n",
      "Epoch 150/500\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 46310364.0000 - mae: 4414.5181 - val_loss: 25112734.0000 - val_mae: 3695.6511\n",
      "Epoch 151/500\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 45949400.0000 - mae: 4397.0801 - val_loss: 25096392.0000 - val_mae: 3694.4016\n",
      "Epoch 152/500\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 45611132.0000 - mae: 4380.4927 - val_loss: 25079446.0000 - val_mae: 3693.0959\n",
      "Epoch 153/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 45235244.0000 - mae: 4362.7744 - val_loss: 25062396.0000 - val_mae: 3691.7778\n",
      "Epoch 154/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 44777312.0000 - mae: 4343.6929 - val_loss: 25045640.0000 - val_mae: 3690.4800\n",
      "Epoch 155/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 44455624.0000 - mae: 4326.5195 - val_loss: 25028036.0000 - val_mae: 3689.1116\n",
      "Epoch 156/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 44118576.0000 - mae: 4309.5229 - val_loss: 25010276.0000 - val_mae: 3687.7346\n",
      "Epoch 157/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 43599796.0000 - mae: 4287.5688 - val_loss: 24993284.0000 - val_mae: 3686.4209\n",
      "Epoch 158/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 43264836.0000 - mae: 4270.4258 - val_loss: 24975472.0000 - val_mae: 3685.0442\n",
      "Epoch 159/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 42914488.0000 - mae: 4251.5259 - val_loss: 24957328.0000 - val_mae: 3683.6467\n",
      "Epoch 160/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 42469328.0000 - mae: 4230.7007 - val_loss: 24939474.0000 - val_mae: 3682.2759\n",
      "Epoch 161/500\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 42040560.0000 - mae: 4209.8418 - val_loss: 24921796.0000 - val_mae: 3680.9253\n",
      "Epoch 162/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 41680264.0000 - mae: 4191.0659 - val_loss: 24903268.0000 - val_mae: 3679.5042\n",
      "Epoch 163/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 41232160.0000 - mae: 4169.1694 - val_loss: 24884688.0000 - val_mae: 3678.0786\n",
      "Epoch 164/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 40861892.0000 - mae: 4149.4331 - val_loss: 24865416.0000 - val_mae: 3676.5991\n",
      "Epoch 165/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 40416272.0000 - mae: 4128.0879 - val_loss: 24846484.0000 - val_mae: 3675.1431\n",
      "Epoch 166/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 40031200.0000 - mae: 4106.4453 - val_loss: 24827050.0000 - val_mae: 3673.6477\n",
      "Epoch 167/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 39573344.0000 - mae: 4083.4934 - val_loss: 24807668.0000 - val_mae: 3672.1536\n",
      "Epoch 168/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 39206480.0000 - mae: 4063.3523 - val_loss: 24787748.0000 - val_mae: 3670.6167\n",
      "Epoch 169/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 38794084.0000 - mae: 4040.1946 - val_loss: 24767522.0000 - val_mae: 3669.0615\n",
      "Epoch 170/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 38342616.0000 - mae: 4015.4060 - val_loss: 24747374.0000 - val_mae: 3667.5112\n",
      "Epoch 171/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 37893560.0000 - mae: 3991.7441 - val_loss: 24727260.0000 - val_mae: 3665.9592\n",
      "Epoch 172/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 37489432.0000 - mae: 3969.1042 - val_loss: 24706732.0000 - val_mae: 3664.3748\n",
      "Epoch 173/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 37057444.0000 - mae: 3944.0581 - val_loss: 24685998.0000 - val_mae: 3662.7681\n",
      "Epoch 174/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 36678448.0000 - mae: 3923.1265 - val_loss: 24664712.0000 - val_mae: 3661.1172\n",
      "Epoch 175/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 36219052.0000 - mae: 3898.1428 - val_loss: 24643800.0000 - val_mae: 3659.4941\n",
      "Epoch 176/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 35864712.0000 - mae: 3875.8718 - val_loss: 24622224.0000 - val_mae: 3657.8108\n",
      "Epoch 177/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 35402052.0000 - mae: 3850.1787 - val_loss: 24600958.0000 - val_mae: 3656.1501\n",
      "Epoch 178/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 34969820.0000 - mae: 3825.3228 - val_loss: 24579836.0000 - val_mae: 3654.4937\n",
      "Epoch 179/500\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 34601280.0000 - mae: 3801.1978 - val_loss: 24558114.0000 - val_mae: 3652.7866\n",
      "Epoch 180/500\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 34182572.0000 - mae: 3776.8218 - val_loss: 24536550.0000 - val_mae: 3651.0894\n",
      "Epoch 181/500\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 33777048.0000 - mae: 3751.2158 - val_loss: 24514984.0000 - val_mae: 3649.3892\n",
      "Epoch 182/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 33348924.0000 - mae: 3724.4033 - val_loss: 24493470.0000 - val_mae: 3647.6958\n",
      "Epoch 183/500\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 32954048.0000 - mae: 3698.3430 - val_loss: 24471870.0000 - val_mae: 3645.9968\n",
      "Epoch 184/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 32568750.0000 - mae: 3672.9922 - val_loss: 24450060.0000 - val_mae: 3644.2793\n",
      "Epoch 185/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 32214292.0000 - mae: 3647.9934 - val_loss: 24427908.0000 - val_mae: 3642.5317\n",
      "Epoch 186/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 31765774.0000 - mae: 3620.2424 - val_loss: 24406206.0000 - val_mae: 3640.8123\n",
      "Epoch 187/500\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 31391610.0000 - mae: 3596.0164 - val_loss: 24384254.0000 - val_mae: 3639.0701\n",
      "Epoch 188/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 31077122.0000 - mae: 3571.4570 - val_loss: 24361744.0000 - val_mae: 3637.2832\n",
      "Epoch 189/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 30646970.0000 - mae: 3543.1001 - val_loss: 24339708.0000 - val_mae: 3635.5305\n",
      "Epoch 190/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 30347086.0000 - mae: 3523.9807 - val_loss: 24317140.0000 - val_mae: 3633.7312\n",
      "Epoch 191/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 29921588.0000 - mae: 3498.3923 - val_loss: 24295364.0000 - val_mae: 3631.9927\n",
      "Epoch 192/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 29606680.0000 - mae: 3475.1130 - val_loss: 24273304.0000 - val_mae: 3630.2234\n",
      "Epoch 193/500\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 29217324.0000 - mae: 3451.3013 - val_loss: 24251540.0000 - val_mae: 3628.4688\n",
      "Epoch 194/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 28924262.0000 - mae: 3429.8999 - val_loss: 24229184.0000 - val_mae: 3626.6689\n",
      "Epoch 195/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 28592172.0000 - mae: 3406.2239 - val_loss: 24206970.0000 - val_mae: 3624.8806\n",
      "Epoch 196/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 28235580.0000 - mae: 3382.1052 - val_loss: 24185108.0000 - val_mae: 3623.1194\n",
      "Epoch 197/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 27956882.0000 - mae: 3360.1323 - val_loss: 24162862.0000 - val_mae: 3621.3250\n",
      "Epoch 198/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 27624278.0000 - mae: 3337.6614 - val_loss: 24141156.0000 - val_mae: 3619.5684\n",
      "Epoch 199/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 27329204.0000 - mae: 3315.7141 - val_loss: 24119294.0000 - val_mae: 3617.7979\n",
      "Epoch 200/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 26982172.0000 - mae: 3291.7251 - val_loss: 24098446.0000 - val_mae: 3616.1047\n",
      "Epoch 201/500\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 26767592.0000 - mae: 3272.3716 - val_loss: 24076504.0000 - val_mae: 3614.3193\n",
      "Epoch 202/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 26446384.0000 - mae: 3248.6680 - val_loss: 24055112.0000 - val_mae: 3612.5771\n",
      "Epoch 203/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 26216362.0000 - mae: 3236.0903 - val_loss: 24033420.0000 - val_mae: 3610.8096\n",
      "Epoch 204/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 25896374.0000 - mae: 3215.6506 - val_loss: 24012568.0000 - val_mae: 3609.1018\n",
      "Epoch 205/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 25635480.0000 - mae: 3199.7239 - val_loss: 23991558.0000 - val_mae: 3607.3752\n",
      "Epoch 206/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 25400106.0000 - mae: 3183.2434 - val_loss: 23970064.0000 - val_mae: 3605.6021\n",
      "Epoch 207/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 25187760.0000 - mae: 3168.6985 - val_loss: 23948540.0000 - val_mae: 3603.8232\n",
      "Epoch 208/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 24966096.0000 - mae: 3152.2532 - val_loss: 23926998.0000 - val_mae: 3602.0356\n",
      "Epoch 209/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 24705786.0000 - mae: 3138.8831 - val_loss: 23906554.0000 - val_mae: 3600.3367\n",
      "Epoch 210/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 24503608.0000 - mae: 3127.7786 - val_loss: 23885934.0000 - val_mae: 3598.6191\n",
      "Epoch 211/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 24290442.0000 - mae: 3115.5591 - val_loss: 23865798.0000 - val_mae: 3596.9397\n",
      "Epoch 212/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 24096340.0000 - mae: 3101.7588 - val_loss: 23845810.0000 - val_mae: 3595.2649\n",
      "Epoch 213/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 23905134.0000 - mae: 3089.1726 - val_loss: 23826140.0000 - val_mae: 3593.6116\n",
      "Epoch 214/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 23711298.0000 - mae: 3077.9180 - val_loss: 23807054.0000 - val_mae: 3592.0017\n",
      "Epoch 215/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 23549592.0000 - mae: 3069.2461 - val_loss: 23788270.0000 - val_mae: 3590.4072\n",
      "Epoch 216/500\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 23372522.0000 - mae: 3056.8684 - val_loss: 23769756.0000 - val_mae: 3588.8328\n",
      "Epoch 217/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 23170414.0000 - mae: 3044.2188 - val_loss: 23752664.0000 - val_mae: 3587.3716\n",
      "Epoch 218/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 23068708.0000 - mae: 3037.1628 - val_loss: 23734364.0000 - val_mae: 3585.8037\n",
      "Epoch 219/500\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 22887344.0000 - mae: 3023.8000 - val_loss: 23717156.0000 - val_mae: 3584.3262\n",
      "Epoch 220/500\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 22768614.0000 - mae: 3014.7451 - val_loss: 23699828.0000 - val_mae: 3582.8333\n",
      "Epoch 221/500\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 22606218.0000 - mae: 3005.8308 - val_loss: 23683080.0000 - val_mae: 3581.3857\n",
      "Epoch 222/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 22524816.0000 - mae: 3000.0769 - val_loss: 23665768.0000 - val_mae: 3579.8865\n",
      "Epoch 223/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 22361706.0000 - mae: 2990.3708 - val_loss: 23649670.0000 - val_mae: 3578.4861\n",
      "Epoch 224/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 22218114.0000 - mae: 2982.4067 - val_loss: 23634388.0000 - val_mae: 3577.1528\n",
      "Epoch 225/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 22140932.0000 - mae: 2977.1475 - val_loss: 23617672.0000 - val_mae: 3575.6978\n",
      "Epoch 226/500\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 22049904.0000 - mae: 2972.6860 - val_loss: 23601054.0000 - val_mae: 3574.2515\n",
      "Epoch 227/500\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 21919406.0000 - mae: 2965.3245 - val_loss: 23585848.0000 - val_mae: 3572.9194\n",
      "Epoch 228/500\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 21798826.0000 - mae: 2958.6206 - val_loss: 23571094.0000 - val_mae: 3571.6255\n",
      "Epoch 229/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 21730340.0000 - mae: 2954.1992 - val_loss: 23555854.0000 - val_mae: 3570.2837\n",
      "Epoch 230/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 21616798.0000 - mae: 2947.4272 - val_loss: 23541316.0000 - val_mae: 3569.0000\n",
      "Epoch 231/500\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 21519514.0000 - mae: 2941.3103 - val_loss: 23527162.0000 - val_mae: 3567.7468\n",
      "Epoch 232/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 21452336.0000 - mae: 2936.7593 - val_loss: 23512184.0000 - val_mae: 3566.4209\n",
      "Epoch 233/500\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 21378138.0000 - mae: 2931.0212 - val_loss: 23497424.0000 - val_mae: 3565.1094\n",
      "Epoch 234/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 21270572.0000 - mae: 2924.9365 - val_loss: 23484454.0000 - val_mae: 3563.9617\n",
      "Epoch 235/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 21229844.0000 - mae: 2920.6450 - val_loss: 23469930.0000 - val_mae: 3562.6782\n",
      "Epoch 236/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 21124266.0000 - mae: 2914.0544 - val_loss: 23457178.0000 - val_mae: 3561.5496\n",
      "Epoch 237/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 21058218.0000 - mae: 2908.9456 - val_loss: 23444212.0000 - val_mae: 3560.3994\n",
      "Epoch 238/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 20984452.0000 - mae: 2904.3599 - val_loss: 23431578.0000 - val_mae: 3559.2778\n",
      "Epoch 239/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 20907310.0000 - mae: 2897.9978 - val_loss: 23419794.0000 - val_mae: 3558.2283\n",
      "Epoch 240/500\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 20872122.0000 - mae: 2893.8384 - val_loss: 23406472.0000 - val_mae: 3557.0427\n",
      "Epoch 241/500\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 20781426.0000 - mae: 2887.1699 - val_loss: 23394928.0000 - val_mae: 3556.0112\n",
      "Epoch 242/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 20719608.0000 - mae: 2882.8235 - val_loss: 23383566.0000 - val_mae: 3554.9927\n",
      "Epoch 243/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 20657282.0000 - mae: 2877.6609 - val_loss: 23372392.0000 - val_mae: 3553.9880\n",
      "Epoch 244/500\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 20603228.0000 - mae: 2873.7788 - val_loss: 23360440.0000 - val_mae: 3552.9141\n",
      "Epoch 245/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 20572528.0000 - mae: 2869.4238 - val_loss: 23348194.0000 - val_mae: 3551.8118\n",
      "Epoch 246/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 20483522.0000 - mae: 2864.3608 - val_loss: 23338046.0000 - val_mae: 3550.8948\n",
      "Epoch 247/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 20441716.0000 - mae: 2860.0103 - val_loss: 23327262.0000 - val_mae: 3549.9229\n",
      "Epoch 248/500\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 20380284.0000 - mae: 2855.4836 - val_loss: 23317466.0000 - val_mae: 3549.0386\n",
      "Epoch 249/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 20324552.0000 - mae: 2850.6099 - val_loss: 23308010.0000 - val_mae: 3548.1794\n",
      "Epoch 250/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 20271498.0000 - mae: 2846.6685 - val_loss: 23298824.0000 - val_mae: 3547.3418\n",
      "Epoch 251/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 20218182.0000 - mae: 2842.0635 - val_loss: 23290990.0000 - val_mae: 3546.6245\n",
      "Epoch 252/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 20171506.0000 - mae: 2837.8218 - val_loss: 23281094.0000 - val_mae: 3545.7190\n",
      "Epoch 253/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 20126812.0000 - mae: 2834.0022 - val_loss: 23271450.0000 - val_mae: 3544.8330\n",
      "Epoch 254/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 20072952.0000 - mae: 2829.5352 - val_loss: 23263198.0000 - val_mae: 3544.0681\n",
      "Epoch 255/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 20024036.0000 - mae: 2825.3000 - val_loss: 23253938.0000 - val_mae: 3543.2224\n",
      "Epoch 256/500\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 19966736.0000 - mae: 2821.1348 - val_loss: 23245598.0000 - val_mae: 3542.4680\n",
      "Epoch 257/500\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 19921836.0000 - mae: 2816.8359 - val_loss: 23237006.0000 - val_mae: 3541.6890\n",
      "Epoch 258/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 19871510.0000 - mae: 2812.5566 - val_loss: 23228776.0000 - val_mae: 3540.9438\n",
      "Epoch 259/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 19831506.0000 - mae: 2808.7998 - val_loss: 23220532.0000 - val_mae: 3540.1936\n",
      "Epoch 260/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 19782470.0000 - mae: 2804.3967 - val_loss: 23211436.0000 - val_mae: 3539.3652\n",
      "Epoch 261/500\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 19735076.0000 - mae: 2800.6392 - val_loss: 23202798.0000 - val_mae: 3538.5757\n",
      "Epoch 262/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 19697512.0000 - mae: 2796.6519 - val_loss: 23193326.0000 - val_mae: 3537.7073\n",
      "Epoch 263/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 19639396.0000 - mae: 2791.9543 - val_loss: 23184082.0000 - val_mae: 3536.8601\n",
      "Epoch 264/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 19597620.0000 - mae: 2788.2180 - val_loss: 23174936.0000 - val_mae: 3536.0190\n",
      "Epoch 265/500\n",
      "2/2 [==============================] - 0s 108ms/step - loss: 19555730.0000 - mae: 2784.2683 - val_loss: 23165192.0000 - val_mae: 3535.1235\n",
      "Epoch 266/500\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 19503952.0000 - mae: 2780.0635 - val_loss: 23156198.0000 - val_mae: 3534.2947\n",
      "Epoch 267/500\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 19456558.0000 - mae: 2775.7332 - val_loss: 23147060.0000 - val_mae: 3533.4529\n",
      "Epoch 268/500\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 19420700.0000 - mae: 2772.3301 - val_loss: 23138544.0000 - val_mae: 3532.6658\n",
      "Epoch 269/500\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 19388444.0000 - mae: 2769.3779 - val_loss: 23128168.0000 - val_mae: 3531.7090\n",
      "Epoch 270/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 19331096.0000 - mae: 2764.8530 - val_loss: 23119204.0000 - val_mae: 3530.8806\n",
      "Epoch 271/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 19289634.0000 - mae: 2761.4641 - val_loss: 23111258.0000 - val_mae: 3530.1448\n",
      "Epoch 272/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 19244268.0000 - mae: 2757.8022 - val_loss: 23102804.0000 - val_mae: 3529.3618\n",
      "Epoch 273/500\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 19201358.0000 - mae: 2754.6355 - val_loss: 23094594.0000 - val_mae: 3528.6008\n",
      "Epoch 274/500\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 19158564.0000 - mae: 2751.0186 - val_loss: 23086394.0000 - val_mae: 3527.8381\n",
      "Epoch 275/500\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 19129080.0000 - mae: 2748.1301 - val_loss: 23077368.0000 - val_mae: 3526.9951\n",
      "Epoch 276/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 19082138.0000 - mae: 2744.7124 - val_loss: 23070384.0000 - val_mae: 3526.3411\n",
      "Epoch 277/500\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 19037918.0000 - mae: 2741.2620 - val_loss: 23062320.0000 - val_mae: 3525.5886\n",
      "Epoch 278/500\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 18996538.0000 - mae: 2737.8179 - val_loss: 23052978.0000 - val_mae: 3524.7188\n",
      "Epoch 279/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 18955020.0000 - mae: 2734.5378 - val_loss: 23044132.0000 - val_mae: 3523.8933\n",
      "Epoch 280/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 18918678.0000 - mae: 2731.6042 - val_loss: 23034188.0000 - val_mae: 3522.9653\n",
      "Epoch 281/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 18886564.0000 - mae: 2728.0149 - val_loss: 23023722.0000 - val_mae: 3521.9861\n",
      "Epoch 282/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 18843430.0000 - mae: 2724.9773 - val_loss: 23014862.0000 - val_mae: 3521.1545\n",
      "Epoch 283/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 18798814.0000 - mae: 2721.4695 - val_loss: 23005572.0000 - val_mae: 3520.2839\n",
      "Epoch 284/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 18770054.0000 - mae: 2718.2957 - val_loss: 22996002.0000 - val_mae: 3519.3833\n",
      "Epoch 285/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 18725310.0000 - mae: 2714.9434 - val_loss: 22987934.0000 - val_mae: 3518.6233\n",
      "Epoch 286/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 18685850.0000 - mae: 2712.1028 - val_loss: 22978284.0000 - val_mae: 3517.7095\n",
      "Epoch 287/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 18653402.0000 - mae: 2709.4629 - val_loss: 22968216.0000 - val_mae: 3516.7532\n",
      "Epoch 288/500\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 18615022.0000 - mae: 2707.0955 - val_loss: 22959676.0000 - val_mae: 3515.9424\n",
      "Epoch 289/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 18581572.0000 - mae: 2705.1492 - val_loss: 22949368.0000 - val_mae: 3514.9619\n",
      "Epoch 290/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 18541946.0000 - mae: 2702.6428 - val_loss: 22939964.0000 - val_mae: 3514.0654\n",
      "Epoch 291/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 18507912.0000 - mae: 2700.1465 - val_loss: 22929946.0000 - val_mae: 3513.1045\n",
      "Epoch 292/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 18468260.0000 - mae: 2697.7026 - val_loss: 22921740.0000 - val_mae: 3512.3130\n",
      "Epoch 293/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 18437618.0000 - mae: 2695.6516 - val_loss: 22912832.0000 - val_mae: 3511.4546\n",
      "Epoch 294/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 18400978.0000 - mae: 2693.2024 - val_loss: 22904882.0000 - val_mae: 3510.6838\n",
      "Epoch 295/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 18365010.0000 - mae: 2690.8386 - val_loss: 22895372.0000 - val_mae: 3509.7666\n",
      "Epoch 296/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 18326236.0000 - mae: 2688.3311 - val_loss: 22885530.0000 - val_mae: 3508.8188\n",
      "Epoch 297/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 18292506.0000 - mae: 2686.2139 - val_loss: 22876898.0000 - val_mae: 3507.9827\n",
      "Epoch 298/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 18255540.0000 - mae: 2683.6748 - val_loss: 22867880.0000 - val_mae: 3507.1089\n",
      "Epoch 299/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 18226342.0000 - mae: 2681.5615 - val_loss: 22859172.0000 - val_mae: 3506.2654\n",
      "Epoch 300/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 18193912.0000 - mae: 2679.2766 - val_loss: 22849640.0000 - val_mae: 3505.3428\n",
      "Epoch 301/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 18152866.0000 - mae: 2676.7551 - val_loss: 22841524.0000 - val_mae: 3504.5518\n",
      "Epoch 302/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 18118848.0000 - mae: 2674.3779 - val_loss: 22834456.0000 - val_mae: 3503.8577\n",
      "Epoch 303/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 18087130.0000 - mae: 2672.0706 - val_loss: 22827286.0000 - val_mae: 3503.1533\n",
      "Epoch 304/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 18054868.0000 - mae: 2669.6953 - val_loss: 22818232.0000 - val_mae: 3502.2673\n",
      "Epoch 305/500\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 18028252.0000 - mae: 2667.8533 - val_loss: 22809148.0000 - val_mae: 3501.3806\n",
      "Epoch 306/500\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 17989502.0000 - mae: 2665.0852 - val_loss: 22801934.0000 - val_mae: 3500.6667\n",
      "Epoch 307/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 17962242.0000 - mae: 2662.9785 - val_loss: 22794950.0000 - val_mae: 3499.9795\n",
      "Epoch 308/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 17924602.0000 - mae: 2660.8108 - val_loss: 22785642.0000 - val_mae: 3499.0701\n",
      "Epoch 309/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 17884834.0000 - mae: 2657.9919 - val_loss: 22777374.0000 - val_mae: 3498.2634\n",
      "Epoch 310/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 17858062.0000 - mae: 2655.9114 - val_loss: 22769136.0000 - val_mae: 3497.4541\n",
      "Epoch 311/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 17818582.0000 - mae: 2653.5554 - val_loss: 22759608.0000 - val_mae: 3496.5203\n",
      "Epoch 312/500\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 17787074.0000 - mae: 2651.3972 - val_loss: 22750728.0000 - val_mae: 3495.6475\n",
      "Epoch 313/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 17753698.0000 - mae: 2649.4614 - val_loss: 22741968.0000 - val_mae: 3494.7830\n",
      "Epoch 314/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 17727622.0000 - mae: 2647.3718 - val_loss: 22733100.0000 - val_mae: 3493.9060\n",
      "Epoch 315/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 17691512.0000 - mae: 2645.1843 - val_loss: 22723988.0000 - val_mae: 3493.0059\n",
      "Epoch 316/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 17663608.0000 - mae: 2643.1667 - val_loss: 22716120.0000 - val_mae: 3492.2205\n",
      "Epoch 317/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 17628038.0000 - mae: 2640.9236 - val_loss: 22707350.0000 - val_mae: 3491.3472\n",
      "Epoch 318/500\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 17600972.0000 - mae: 2638.8438 - val_loss: 22699330.0000 - val_mae: 3490.5442\n",
      "Epoch 319/500\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 17568272.0000 - mae: 2636.6343 - val_loss: 22690640.0000 - val_mae: 3489.6746\n",
      "Epoch 320/500\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 17530804.0000 - mae: 2634.2131 - val_loss: 22680860.0000 - val_mae: 3488.6951\n",
      "Epoch 321/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 17499696.0000 - mae: 2632.3179 - val_loss: 22670906.0000 - val_mae: 3487.6985\n",
      "Epoch 322/500\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 17479564.0000 - mae: 2631.0500 - val_loss: 22659760.0000 - val_mae: 3486.5867\n",
      "Epoch 323/500\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 17439906.0000 - mae: 2628.6162 - val_loss: 22650894.0000 - val_mae: 3485.6897\n",
      "Epoch 324/500\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 17408630.0000 - mae: 2626.5137 - val_loss: 22641908.0000 - val_mae: 3484.7832\n",
      "Epoch 325/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 17378464.0000 - mae: 2624.4441 - val_loss: 22631674.0000 - val_mae: 3483.7490\n",
      "Epoch 326/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 17342422.0000 - mae: 2622.5271 - val_loss: 22622630.0000 - val_mae: 3482.8330\n",
      "Epoch 327/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 17316958.0000 - mae: 2620.9421 - val_loss: 22612714.0000 - val_mae: 3481.8281\n",
      "Epoch 328/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 17281808.0000 - mae: 2618.3582 - val_loss: 22602990.0000 - val_mae: 3480.8452\n",
      "Epoch 329/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 17254250.0000 - mae: 2616.8550 - val_loss: 22593094.0000 - val_mae: 3479.8418\n",
      "Epoch 330/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 17221232.0000 - mae: 2614.4553 - val_loss: 22582974.0000 - val_mae: 3478.8125\n",
      "Epoch 331/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 17195768.0000 - mae: 2612.7559 - val_loss: 22573070.0000 - val_mae: 3477.8018\n",
      "Epoch 332/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 17164952.0000 - mae: 2610.5747 - val_loss: 22564476.0000 - val_mae: 3476.9185\n",
      "Epoch 333/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 17135700.0000 - mae: 2608.8379 - val_loss: 22554374.0000 - val_mae: 3475.8853\n",
      "Epoch 334/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 17103186.0000 - mae: 2606.5894 - val_loss: 22545466.0000 - val_mae: 3474.9661\n",
      "Epoch 335/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 17069508.0000 - mae: 2604.3838 - val_loss: 22536618.0000 - val_mae: 3474.0527\n",
      "Epoch 336/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 17041542.0000 - mae: 2602.5435 - val_loss: 22528150.0000 - val_mae: 3473.1775\n",
      "Epoch 337/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 17011854.0000 - mae: 2600.4043 - val_loss: 22519950.0000 - val_mae: 3472.3281\n",
      "Epoch 338/500\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 16983164.0000 - mae: 2598.3782 - val_loss: 22511654.0000 - val_mae: 3471.4673\n",
      "Epoch 339/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 16951504.0000 - mae: 2596.3115 - val_loss: 22503658.0000 - val_mae: 3470.6382\n",
      "Epoch 340/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 16928760.0000 - mae: 2594.5078 - val_loss: 22497114.0000 - val_mae: 3469.9531\n",
      "Epoch 341/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 16901554.0000 - mae: 2592.2842 - val_loss: 22488904.0000 - val_mae: 3469.1030\n",
      "Epoch 342/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 16866724.0000 - mae: 2590.0674 - val_loss: 22479432.0000 - val_mae: 3468.1250\n",
      "Epoch 343/500\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 16836780.0000 - mae: 2587.9055 - val_loss: 22471064.0000 - val_mae: 3467.2556\n",
      "Epoch 344/500\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 16814136.0000 - mae: 2586.1619 - val_loss: 22462792.0000 - val_mae: 3466.3979\n",
      "Epoch 345/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 16781586.0000 - mae: 2583.8108 - val_loss: 22453812.0000 - val_mae: 3465.4683\n",
      "Epoch 346/500\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 16753827.0000 - mae: 2581.8892 - val_loss: 22445210.0000 - val_mae: 3464.5764\n",
      "Epoch 347/500\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 16730928.0000 - mae: 2580.1785 - val_loss: 22436792.0000 - val_mae: 3463.7014\n",
      "Epoch 348/500\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 16704989.0000 - mae: 2578.0750 - val_loss: 22428562.0000 - val_mae: 3462.8452\n",
      "Epoch 349/500\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 16674976.0000 - mae: 2576.0647 - val_loss: 22419300.0000 - val_mae: 3461.8860\n",
      "Epoch 350/500\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 16643446.0000 - mae: 2574.1914 - val_loss: 22409266.0000 - val_mae: 3460.8508\n",
      "Epoch 351/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 16623016.0000 - mae: 2572.5837 - val_loss: 22399134.0000 - val_mae: 3459.8054\n",
      "Epoch 352/500\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 16598163.0000 - mae: 2570.7727 - val_loss: 22388066.0000 - val_mae: 3458.6677\n",
      "Epoch 353/500\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 16566304.0000 - mae: 2569.0173 - val_loss: 22378286.0000 - val_mae: 3457.6572\n",
      "Epoch 354/500\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 16541517.0000 - mae: 2567.3928 - val_loss: 22368590.0000 - val_mae: 3456.6533\n",
      "Epoch 355/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 16518420.0000 - mae: 2565.7854 - val_loss: 22357980.0000 - val_mae: 3455.5505\n",
      "Epoch 356/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 16493456.0000 - mae: 2564.0576 - val_loss: 22348940.0000 - val_mae: 3454.6123\n",
      "Epoch 357/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 16463173.0000 - mae: 2562.0110 - val_loss: 22337708.0000 - val_mae: 3453.4509\n",
      "Epoch 358/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 16437630.0000 - mae: 2560.5168 - val_loss: 22328068.0000 - val_mae: 3452.4458\n",
      "Epoch 359/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 16409460.0000 - mae: 2558.7112 - val_loss: 22317900.0000 - val_mae: 3451.3838\n",
      "Epoch 360/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 16384512.0000 - mae: 2557.0581 - val_loss: 22307470.0000 - val_mae: 3450.2942\n",
      "Epoch 361/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 16363464.0000 - mae: 2555.3733 - val_loss: 22296312.0000 - val_mae: 3449.1292\n",
      "Epoch 362/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 16334003.0000 - mae: 2553.7053 - val_loss: 22286392.0000 - val_mae: 3448.0859\n",
      "Epoch 363/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 16315347.0000 - mae: 2552.1975 - val_loss: 22276894.0000 - val_mae: 3447.0850\n",
      "Epoch 364/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 16286397.0000 - mae: 2550.3398 - val_loss: 22265852.0000 - val_mae: 3445.9287\n",
      "Epoch 365/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 16259109.0000 - mae: 2548.3550 - val_loss: 22256640.0000 - val_mae: 3444.9583\n",
      "Epoch 366/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 16236058.0000 - mae: 2546.6797 - val_loss: 22246850.0000 - val_mae: 3443.9255\n",
      "Epoch 367/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 16211630.0000 - mae: 2545.0283 - val_loss: 22237794.0000 - val_mae: 3442.9680\n",
      "Epoch 368/500\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 16185847.0000 - mae: 2543.1472 - val_loss: 22227170.0000 - val_mae: 3441.8440\n",
      "Epoch 369/500\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 16162994.0000 - mae: 2541.6089 - val_loss: 22218010.0000 - val_mae: 3440.8792\n",
      "Epoch 370/500\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 16141641.0000 - mae: 2540.2195 - val_loss: 22206580.0000 - val_mae: 3439.6760\n",
      "Epoch 371/500\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 16116283.0000 - mae: 2538.5586 - val_loss: 22196262.0000 - val_mae: 3438.5879\n",
      "Epoch 372/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 16084759.0000 - mae: 2536.5022 - val_loss: 22186470.0000 - val_mae: 3437.5583\n",
      "Epoch 373/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 16065880.0000 - mae: 2535.0935 - val_loss: 22176252.0000 - val_mae: 3436.4907\n",
      "Epoch 374/500\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 16043306.0000 - mae: 2533.4304 - val_loss: 22166406.0000 - val_mae: 3435.4590\n",
      "Epoch 375/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 16018091.0000 - mae: 2531.8110 - val_loss: 22156676.0000 - val_mae: 3434.4392\n",
      "Epoch 376/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 15992892.0000 - mae: 2529.7549 - val_loss: 22147778.0000 - val_mae: 3433.5017\n",
      "Epoch 377/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 15971519.0000 - mae: 2528.2930 - val_loss: 22139480.0000 - val_mae: 3432.6240\n",
      "Epoch 378/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 15944080.0000 - mae: 2526.2783 - val_loss: 22131384.0000 - val_mae: 3431.7683\n",
      "Epoch 379/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 15920823.0000 - mae: 2524.5923 - val_loss: 22123038.0000 - val_mae: 3430.8831\n",
      "Epoch 380/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 15899570.0000 - mae: 2522.9133 - val_loss: 22115022.0000 - val_mae: 3430.0366\n",
      "Epoch 381/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 15874994.0000 - mae: 2521.0464 - val_loss: 22106714.0000 - val_mae: 3429.1572\n",
      "Epoch 382/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 15851393.0000 - mae: 2519.3103 - val_loss: 22097858.0000 - val_mae: 3428.2144\n",
      "Epoch 383/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 15832850.0000 - mae: 2517.7925 - val_loss: 22088996.0000 - val_mae: 3427.2803\n",
      "Epoch 384/500\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 15810935.0000 - mae: 2515.9668 - val_loss: 22078102.0000 - val_mae: 3426.1267\n",
      "Epoch 385/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 15785503.0000 - mae: 2514.4702 - val_loss: 22068388.0000 - val_mae: 3425.1008\n",
      "Epoch 386/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 15758492.0000 - mae: 2512.4744 - val_loss: 22058730.0000 - val_mae: 3424.0815\n",
      "Epoch 387/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 15736420.0000 - mae: 2510.7815 - val_loss: 22048568.0000 - val_mae: 3423.0081\n",
      "Epoch 388/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 15718540.0000 - mae: 2509.3865 - val_loss: 22038488.0000 - val_mae: 3421.9385\n",
      "Epoch 389/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 15690275.0000 - mae: 2507.5281 - val_loss: 22028856.0000 - val_mae: 3420.9182\n",
      "Epoch 390/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 15678312.0000 - mae: 2506.0647 - val_loss: 22020196.0000 - val_mae: 3420.0020\n",
      "Epoch 391/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 15646205.0000 - mae: 2504.0305 - val_loss: 22010360.0000 - val_mae: 3418.9524\n",
      "Epoch 392/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 15625176.0000 - mae: 2502.3101 - val_loss: 22000204.0000 - val_mae: 3417.8730\n",
      "Epoch 393/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 15603397.0000 - mae: 2500.7788 - val_loss: 21990248.0000 - val_mae: 3416.8062\n",
      "Epoch 394/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 15581293.0000 - mae: 2499.2100 - val_loss: 21979858.0000 - val_mae: 3415.6882\n",
      "Epoch 395/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 15566165.0000 - mae: 2497.6780 - val_loss: 21968924.0000 - val_mae: 3414.5137\n",
      "Epoch 396/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 15549402.0000 - mae: 2496.4480 - val_loss: 21959924.0000 - val_mae: 3413.5554\n",
      "Epoch 397/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 15519587.0000 - mae: 2494.4224 - val_loss: 21949572.0000 - val_mae: 3412.4421\n",
      "Epoch 398/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 15494322.0000 - mae: 2492.6233 - val_loss: 21939288.0000 - val_mae: 3411.3340\n",
      "Epoch 399/500\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 15478707.0000 - mae: 2491.5020 - val_loss: 21929186.0000 - val_mae: 3410.2522\n",
      "Epoch 400/500\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 15455742.0000 - mae: 2489.8669 - val_loss: 21917732.0000 - val_mae: 3409.0256\n",
      "Epoch 401/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 15441016.0000 - mae: 2488.9016 - val_loss: 21905950.0000 - val_mae: 3407.7546\n",
      "Epoch 402/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 15415080.0000 - mae: 2487.0835 - val_loss: 21895188.0000 - val_mae: 3406.6033\n",
      "Epoch 403/500\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 15395907.0000 - mae: 2485.6521 - val_loss: 21885212.0000 - val_mae: 3405.5381\n",
      "Epoch 404/500\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 15376602.0000 - mae: 2484.0891 - val_loss: 21875502.0000 - val_mae: 3404.4932\n",
      "Epoch 405/500\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 15355989.0000 - mae: 2482.4927 - val_loss: 21865486.0000 - val_mae: 3403.4160\n",
      "Epoch 406/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 15337987.0000 - mae: 2480.9072 - val_loss: 21854716.0000 - val_mae: 3402.2498\n",
      "Epoch 407/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 15326796.0000 - mae: 2479.9688 - val_loss: 21843752.0000 - val_mae: 3401.0549\n",
      "Epoch 408/500\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 15297934.0000 - mae: 2478.0940 - val_loss: 21834248.0000 - val_mae: 3400.0212\n",
      "Epoch 409/500\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 15277762.0000 - mae: 2476.5095 - val_loss: 21825650.0000 - val_mae: 3399.0906\n",
      "Epoch 410/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 15260875.0000 - mae: 2475.0930 - val_loss: 21816072.0000 - val_mae: 3398.0537\n",
      "Epoch 411/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 15247938.0000 - mae: 2473.7954 - val_loss: 21807704.0000 - val_mae: 3397.1509\n",
      "Epoch 412/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 15227599.0000 - mae: 2472.3657 - val_loss: 21798386.0000 - val_mae: 3396.1357\n",
      "Epoch 413/500\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 15204819.0000 - mae: 2470.5291 - val_loss: 21787836.0000 - val_mae: 3394.9849\n",
      "Epoch 414/500\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 15184175.0000 - mae: 2469.2437 - val_loss: 21776550.0000 - val_mae: 3393.7498\n",
      "Epoch 415/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 15167265.0000 - mae: 2467.8855 - val_loss: 21764990.0000 - val_mae: 3392.4783\n",
      "Epoch 416/500\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 15146260.0000 - mae: 2466.4756 - val_loss: 21754060.0000 - val_mae: 3391.2742\n",
      "Epoch 417/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 15128747.0000 - mae: 2465.0811 - val_loss: 21742204.0000 - val_mae: 3389.9629\n",
      "Epoch 418/500\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 15113948.0000 - mae: 2463.6675 - val_loss: 21729534.0000 - val_mae: 3388.5598\n",
      "Epoch 419/500\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 15094702.0000 - mae: 2462.5891 - val_loss: 21718642.0000 - val_mae: 3387.3562\n",
      "Epoch 420/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 15073685.0000 - mae: 2461.1775 - val_loss: 21707084.0000 - val_mae: 3386.0745\n",
      "Epoch 421/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 15053298.0000 - mae: 2459.5776 - val_loss: 21696256.0000 - val_mae: 3384.8657\n",
      "Epoch 422/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 15037594.0000 - mae: 2458.2756 - val_loss: 21685410.0000 - val_mae: 3383.6545\n",
      "Epoch 423/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 15019018.0000 - mae: 2456.9353 - val_loss: 21674526.0000 - val_mae: 3382.4351\n",
      "Epoch 424/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 15007451.0000 - mae: 2455.7864 - val_loss: 21664548.0000 - val_mae: 3381.3179\n",
      "Epoch 425/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 14982189.0000 - mae: 2454.0479 - val_loss: 21653896.0000 - val_mae: 3380.1191\n",
      "Epoch 426/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 14967733.0000 - mae: 2452.6667 - val_loss: 21643074.0000 - val_mae: 3378.8889\n",
      "Epoch 427/500\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 14950772.0000 - mae: 2451.4751 - val_loss: 21631610.0000 - val_mae: 3377.5896\n",
      "Epoch 428/500\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 14931795.0000 - mae: 2449.8777 - val_loss: 21620340.0000 - val_mae: 3376.3159\n",
      "Epoch 429/500\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 14914108.0000 - mae: 2448.7300 - val_loss: 21609520.0000 - val_mae: 3375.0889\n",
      "Epoch 430/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 14907539.0000 - mae: 2447.7803 - val_loss: 21600332.0000 - val_mae: 3374.0405\n",
      "Epoch 431/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 14884104.0000 - mae: 2445.9631 - val_loss: 21590234.0000 - val_mae: 3372.8806\n",
      "Epoch 432/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 14862336.0000 - mae: 2444.4978 - val_loss: 21579412.0000 - val_mae: 3371.6287\n",
      "Epoch 433/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 14846791.0000 - mae: 2443.5713 - val_loss: 21568444.0000 - val_mae: 3370.3599\n",
      "Epoch 434/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 14832462.0000 - mae: 2442.2002 - val_loss: 21556220.0000 - val_mae: 3368.9460\n",
      "Epoch 435/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 14814743.0000 - mae: 2441.3909 - val_loss: 21544504.0000 - val_mae: 3367.5881\n",
      "Epoch 436/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 14806479.0000 - mae: 2440.5747 - val_loss: 21532240.0000 - val_mae: 3366.1726\n",
      "Epoch 437/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 14785187.0000 - mae: 2439.1025 - val_loss: 21521344.0000 - val_mae: 3364.9150\n",
      "Epoch 438/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 14765613.0000 - mae: 2437.9082 - val_loss: 21511858.0000 - val_mae: 3363.8223\n",
      "Epoch 439/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 14746381.0000 - mae: 2436.3555 - val_loss: 21502928.0000 - val_mae: 3362.7883\n",
      "Epoch 440/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 14733813.0000 - mae: 2435.2720 - val_loss: 21494050.0000 - val_mae: 3361.7520\n",
      "Epoch 441/500\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 14719544.0000 - mae: 2434.0149 - val_loss: 21484302.0000 - val_mae: 3360.6045\n",
      "Epoch 442/500\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 14700420.0000 - mae: 2432.6951 - val_loss: 21475748.0000 - val_mae: 3359.6052\n",
      "Epoch 443/500\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 14683133.0000 - mae: 2431.4875 - val_loss: 21466922.0000 - val_mae: 3358.5701\n",
      "Epoch 444/500\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 14669306.0000 - mae: 2430.4441 - val_loss: 21457534.0000 - val_mae: 3357.4670\n",
      "Epoch 445/500\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 14659039.0000 - mae: 2429.2559 - val_loss: 21448228.0000 - val_mae: 3356.3772\n",
      "Epoch 446/500\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 14636678.0000 - mae: 2427.6631 - val_loss: 21438348.0000 - val_mae: 3355.2153\n",
      "Epoch 447/500\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 14625534.0000 - mae: 2426.6873 - val_loss: 21428070.0000 - val_mae: 3354.0032\n",
      "Epoch 448/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 14604813.0000 - mae: 2425.3608 - val_loss: 21416706.0000 - val_mae: 3352.6584\n",
      "Epoch 449/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 14589384.0000 - mae: 2424.0852 - val_loss: 21405828.0000 - val_mae: 3351.3694\n",
      "Epoch 450/500\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 14577444.0000 - mae: 2423.1472 - val_loss: 21394384.0000 - val_mae: 3350.0142\n",
      "Epoch 451/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 14560138.0000 - mae: 2421.8892 - val_loss: 21384180.0000 - val_mae: 3348.8003\n",
      "Epoch 452/500\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 14547940.0000 - mae: 2420.8948 - val_loss: 21374536.0000 - val_mae: 3347.6501\n",
      "Epoch 453/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 14531515.0000 - mae: 2419.6204 - val_loss: 21363852.0000 - val_mae: 3346.3740\n",
      "Epoch 454/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 14513909.0000 - mae: 2418.2500 - val_loss: 21352396.0000 - val_mae: 3344.9963\n",
      "Epoch 455/500\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 14497102.0000 - mae: 2417.1104 - val_loss: 21340910.0000 - val_mae: 3343.6248\n",
      "Epoch 456/500\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 14481030.0000 - mae: 2416.0466 - val_loss: 21329708.0000 - val_mae: 3342.2783\n",
      "Epoch 457/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 14477517.0000 - mae: 2415.6951 - val_loss: 21317530.0000 - val_mae: 3340.8071\n",
      "Epoch 458/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 14453044.0000 - mae: 2413.8967 - val_loss: 21307984.0000 - val_mae: 3339.6406\n",
      "Epoch 459/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 14437717.0000 - mae: 2412.6543 - val_loss: 21297978.0000 - val_mae: 3338.4226\n",
      "Epoch 460/500\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 14421327.0000 - mae: 2411.3447 - val_loss: 21288058.0000 - val_mae: 3337.2075\n",
      "Epoch 461/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 14413389.0000 - mae: 2410.6267 - val_loss: 21279240.0000 - val_mae: 3336.1353\n",
      "Epoch 462/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 14392457.0000 - mae: 2409.1064 - val_loss: 21269658.0000 - val_mae: 3334.9575\n",
      "Epoch 463/500\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 14375938.0000 - mae: 2407.7163 - val_loss: 21259528.0000 - val_mae: 3333.7202\n",
      "Epoch 464/500\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 14360427.0000 - mae: 2406.4907 - val_loss: 21248400.0000 - val_mae: 3332.3616\n",
      "Epoch 465/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 14348418.0000 - mae: 2405.4470 - val_loss: 21238324.0000 - val_mae: 3331.1313\n",
      "Epoch 466/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 14331034.0000 - mae: 2404.2903 - val_loss: 21227724.0000 - val_mae: 3329.8376\n",
      "Epoch 467/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 14319057.0000 - mae: 2403.1365 - val_loss: 21217252.0000 - val_mae: 3328.5540\n",
      "Epoch 468/500\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 14300900.0000 - mae: 2401.7251 - val_loss: 21206804.0000 - val_mae: 3327.2629\n",
      "Epoch 469/500\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 14289277.0000 - mae: 2400.8037 - val_loss: 21196124.0000 - val_mae: 3325.9504\n",
      "Epoch 470/500\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 14277279.0000 - mae: 2399.8235 - val_loss: 21185008.0000 - val_mae: 3324.5757\n",
      "Epoch 471/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 14258779.0000 - mae: 2398.4495 - val_loss: 21175248.0000 - val_mae: 3323.3833\n",
      "Epoch 472/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 14245543.0000 - mae: 2397.2708 - val_loss: 21164996.0000 - val_mae: 3322.1245\n",
      "Epoch 473/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 14234611.0000 - mae: 2396.3362 - val_loss: 21155854.0000 - val_mae: 3320.9951\n",
      "Epoch 474/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 14216743.0000 - mae: 2394.8213 - val_loss: 21145662.0000 - val_mae: 3319.7327\n",
      "Epoch 475/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 14205044.0000 - mae: 2393.8801 - val_loss: 21135576.0000 - val_mae: 3318.4851\n",
      "Epoch 476/500\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 14190585.0000 - mae: 2392.7051 - val_loss: 21125644.0000 - val_mae: 3317.2639\n",
      "Epoch 477/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 14178629.0000 - mae: 2391.5762 - val_loss: 21117020.0000 - val_mae: 3316.1946\n",
      "Epoch 478/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 14161882.0000 - mae: 2390.1179 - val_loss: 21107170.0000 - val_mae: 3314.9666\n",
      "Epoch 479/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 14148447.0000 - mae: 2389.0215 - val_loss: 21097150.0000 - val_mae: 3313.7041\n",
      "Epoch 480/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 14139201.0000 - mae: 2388.2214 - val_loss: 21085956.0000 - val_mae: 3312.2915\n",
      "Epoch 481/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 14127551.0000 - mae: 2387.2434 - val_loss: 21075256.0000 - val_mae: 3310.9419\n",
      "Epoch 482/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 14110230.0000 - mae: 2385.8262 - val_loss: 21066862.0000 - val_mae: 3309.8965\n",
      "Epoch 483/500\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 14096264.0000 - mae: 2384.6653 - val_loss: 21057002.0000 - val_mae: 3308.6516\n",
      "Epoch 484/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 14083030.0000 - mae: 2383.5811 - val_loss: 21047402.0000 - val_mae: 3307.4397\n",
      "Epoch 485/500\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 14067069.0000 - mae: 2382.2004 - val_loss: 21039044.0000 - val_mae: 3306.3721\n",
      "Epoch 486/500\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 14057479.0000 - mae: 2381.2314 - val_loss: 21030648.0000 - val_mae: 3305.2971\n",
      "Epoch 487/500\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 14041481.0000 - mae: 2380.0315 - val_loss: 21021364.0000 - val_mae: 3304.1118\n",
      "Epoch 488/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 14029919.0000 - mae: 2379.0559 - val_loss: 21011994.0000 - val_mae: 3302.9138\n",
      "Epoch 489/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 14020557.0000 - mae: 2378.1248 - val_loss: 21002536.0000 - val_mae: 3301.6917\n",
      "Epoch 490/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 14002790.0000 - mae: 2376.7566 - val_loss: 20991698.0000 - val_mae: 3300.2898\n",
      "Epoch 491/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 13988753.0000 - mae: 2375.7073 - val_loss: 20981256.0000 - val_mae: 3298.9431\n",
      "Epoch 492/500\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 13975541.0000 - mae: 2374.6260 - val_loss: 20970530.0000 - val_mae: 3297.5635\n",
      "Epoch 493/500\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 13965158.0000 - mae: 2373.8711 - val_loss: 20960204.0000 - val_mae: 3296.2466\n",
      "Epoch 494/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 13948162.0000 - mae: 2372.4907 - val_loss: 20949400.0000 - val_mae: 3294.8647\n",
      "Epoch 495/500\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 13936640.0000 - mae: 2371.5222 - val_loss: 20938808.0000 - val_mae: 3293.5173\n",
      "Epoch 496/500\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 13923956.0000 - mae: 2370.5452 - val_loss: 20927432.0000 - val_mae: 3292.0593\n",
      "Epoch 497/500\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 13914052.0000 - mae: 2369.6917 - val_loss: 20916110.0000 - val_mae: 3290.6077\n",
      "Epoch 498/500\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 13902162.0000 - mae: 2368.8865 - val_loss: 20905266.0000 - val_mae: 3289.2144\n",
      "Epoch 499/500\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 13887223.0000 - mae: 2367.7913 - val_loss: 20895518.0000 - val_mae: 3287.9619\n",
      "Epoch 500/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 13876600.0000 - mae: 2366.8044 - val_loss: 20885186.0000 - val_mae: 3286.6265\n",
      "1/1 [==============================] - 0s 101ms/step\n"
     ]
    }
   ],
   "source": [
    "# scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train_nn = scaler.fit_transform(X_train_nn)\n",
    "X_test_nn = scaler.transform(X_test_nn)\n",
    "\n",
    "# fit the model on the X_train and y_train\n",
    "model = Sequential()\n",
    "# model.add(Dense(256, activation=\"relu\", input_dim=X_train_nn.shape[1]))  # First layer\n",
    "# model.add(Dense(128, activation=\"relu\"))  # First layer\n",
    "# model.add(Dense(64, activation=\"relu\"))  # First layer\n",
    "model.add(Dense(32, activation=\"relu\", input_dim=X_train_nn.shape[1]))  # Fourth layer\n",
    "model.add(Dense(16, activation=\"relu\"))  # New additional layer\n",
    "model.add(Dense(8, activation=\"relu\"))  # New additional layer\n",
    "model.add(Dense(1)) \n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"mean_squared_error\", metrics=[\"mae\"])\n",
    "\n",
    "model.fit(X_train_nn, y_train_nn, epochs=500, batch_size=64, validation_split=0.2)\n",
    "\n",
    "# # get the predictions\n",
    "# boolean_columns = [col for col in X_test_nn.columns if X_test_nn[col].dtype == bool]\n",
    "# for col in boolean_columns:\n",
    "#     X_test_nn[col] = X_test_nn[col].astype(int)\n",
    "\n",
    "y_hat = model.predict(X_test_nn).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_data_diff[\"gdp_pred\"] = y_hat\n",
    "poland_pred = pred_data_diff[[\"year\", \"region\", \"gdp_pred\", \"real_gdp\"]]\n",
    "poland_pred.to_csv(\"poland_predictions.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
