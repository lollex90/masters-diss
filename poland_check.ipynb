{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jakub\\anaconda\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\jakub\\anaconda\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import pandas as pd\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Resizing, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "from tools import define_cnn, extract_features, build_model, predict_with_model, set_seed, build_model_and_predict, extract_features_2\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load clean gdp data, keep only year, region and real_gdp columns\n",
    "poland = pd.read_csv(\"data/tabular_data_poland.csv\")\n",
    "log_bin_columns = [\"allangle_snow_free_hq\" + \"_log_\" + str(i) for i in range(1, 11)]\n",
    "\n",
    "# get the data for 2021, 2022 and before 2022\n",
    "poland_2022 = poland[poland[\"year\"] == 2022]\n",
    "poland = poland[poland[\"year\"].astype(int) < 2022]\n",
    "poland_2021 = poland[poland[\"year\"] == 2021]\n",
    "poland_2021.reset_index(drop=True, inplace=True)\n",
    "poland_2022.reset_index(drop=True, inplace=True)\n",
    "poland.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Initialise a three dimensional array to store the images with the shape (number of images, height, width, channels)\n",
    "X = np.zeros((len(poland), 609, 911, 1))\n",
    "y = np.zeros(len(poland))\n",
    "\n",
    "# load the images\n",
    "for i in range(len(poland)):\n",
    "\n",
    "    # get year, region, and gdp\n",
    "    year = poland[\"year\"][i]\n",
    "    region = poland[\"region\"][i]\n",
    "    gdp_value = poland[\"real_gdp\"][i]\n",
    "\n",
    "    # load the image\n",
    "    file_name = f\"{year}_{region}_hq.h5\"\n",
    "    file_path = f\"data/annual_region_images/{file_name}\"\n",
    "    \n",
    "    with h5py.File(file_path, 'r') as annual_region:\n",
    "        allangle_snow_free = annual_region[\"AllAngle_Composite_Snow_Free\"][:]\n",
    "\n",
    "    # add the values\n",
    "    y[i] = gdp_value\n",
    "    X[i, :, :, 0] = allangle_snow_free\n",
    "\n",
    "# normalise the images and gdp data\n",
    "maximum_x = X.max()\n",
    "X = X / maximum_x\n",
    "\n",
    "maximum_y = y.max()\n",
    "y = y / maximum_y\n",
    "\n",
    "# get indices for observations with  year = 2021: this is the test set\n",
    "test_indices = np.where(poland[\"year\"] == 2021)[0]\n",
    "train_indices = np.where(poland[\"year\"] != 2021)[0]\n",
    "\n",
    "# get the train and test sets\n",
    "X_train, y_train, X_test, y_test = X[train_indices], y[train_indices], X[test_indices], y[test_indices]\n",
    "\n",
    "# get the prediction data\n",
    "X_pred = np.zeros((len(poland_2022),  609, 911, 1))\n",
    "for i in range(len(poland_2022)):\n",
    "\n",
    "    year = poland_2022[\"year\"][i]\n",
    "    region = poland_2022[\"region\"][i]\n",
    "\n",
    "    file_name = f\"{year}_{region}_hq.h5\"\n",
    "    file_path = f\"data/annual_region_images/{file_name}\"\n",
    "\n",
    "    with h5py.File(file_path, 'r') as annual_region:\n",
    "        allangle_snow_free = annual_region[\"AllAngle_Composite_Snow_Free\"][:]\n",
    "    \n",
    "    X_pred[i, :, :, 0] = allangle_snow_free\n",
    "\n",
    "X_pred = X_pred / maximum_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\jakub\\anaconda\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\jakub\\anaconda\\lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:From c:\\Users\\jakub\\anaconda\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\jakub\\anaconda\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "2/2 [==============================] - 9s 3s/step - loss: 0.9754 - mae: 0.6574 - val_loss: 0.0877 - val_mae: 0.2686\n",
      "Epoch 2/50\n",
      "2/2 [==============================] - 2s 859ms/step - loss: 0.1376 - mae: 0.2749 - val_loss: 0.2210 - val_mae: 0.4390\n",
      "Epoch 3/50\n",
      "2/2 [==============================] - 2s 962ms/step - loss: 0.0786 - mae: 0.2119 - val_loss: 0.3361 - val_mae: 0.5458\n",
      "Epoch 4/50\n",
      "2/2 [==============================] - 1s 830ms/step - loss: 0.0642 - mae: 0.1845 - val_loss: 0.4591 - val_mae: 0.6270\n",
      "Epoch 5/50\n",
      "2/2 [==============================] - 2s 988ms/step - loss: 0.0505 - mae: 0.1574 - val_loss: 0.5627 - val_mae: 0.6745\n",
      "Epoch 6/50\n",
      "2/2 [==============================] - 2s 1s/step - loss: 0.0394 - mae: 0.1409 - val_loss: 0.6516 - val_mae: 0.6999\n",
      "Epoch 7/50\n",
      "2/2 [==============================] - 2s 780ms/step - loss: 0.0310 - mae: 0.1237 - val_loss: 0.7176 - val_mae: 0.7163\n",
      "Epoch 8/50\n",
      "2/2 [==============================] - 1s 653ms/step - loss: 0.0266 - mae: 0.1152 - val_loss: 0.7539 - val_mae: 0.7400\n",
      "Epoch 9/50\n",
      "2/2 [==============================] - 1s 659ms/step - loss: 0.0210 - mae: 0.1055 - val_loss: 0.7563 - val_mae: 0.7466\n",
      "Epoch 10/50\n",
      "2/2 [==============================] - 1s 651ms/step - loss: 0.0156 - mae: 0.0910 - val_loss: 0.7369 - val_mae: 0.7404\n",
      "Epoch 11/50\n",
      "2/2 [==============================] - 1s 629ms/step - loss: 0.0123 - mae: 0.0808 - val_loss: 0.7164 - val_mae: 0.7429\n",
      "Epoch 12/50\n",
      "2/2 [==============================] - 1s 714ms/step - loss: 0.0082 - mae: 0.0646 - val_loss: 0.7023 - val_mae: 0.7495\n",
      "Epoch 13/50\n",
      "2/2 [==============================] - 1s 645ms/step - loss: 0.0059 - mae: 0.0561 - val_loss: 0.6820 - val_mae: 0.7514\n",
      "Epoch 14/50\n",
      "2/2 [==============================] - 1s 697ms/step - loss: 0.0051 - mae: 0.0517 - val_loss: 0.6630 - val_mae: 0.7485\n",
      "Epoch 15/50\n",
      "2/2 [==============================] - 1s 625ms/step - loss: 0.0042 - mae: 0.0443 - val_loss: 0.6406 - val_mae: 0.7405\n",
      "Epoch 16/50\n",
      "2/2 [==============================] - 1s 623ms/step - loss: 0.0038 - mae: 0.0439 - val_loss: 0.6220 - val_mae: 0.7314\n",
      "Epoch 17/50\n",
      "2/2 [==============================] - 1s 619ms/step - loss: 0.0033 - mae: 0.0411 - val_loss: 0.6111 - val_mae: 0.7248\n",
      "Epoch 18/50\n",
      "2/2 [==============================] - 1s 628ms/step - loss: 0.0028 - mae: 0.0407 - val_loss: 0.6005 - val_mae: 0.7173\n",
      "Epoch 19/50\n",
      "2/2 [==============================] - 1s 632ms/step - loss: 0.0020 - mae: 0.0339 - val_loss: 0.5865 - val_mae: 0.7091\n",
      "Epoch 20/50\n",
      "2/2 [==============================] - 1s 626ms/step - loss: 0.0021 - mae: 0.0327 - val_loss: 0.5753 - val_mae: 0.7019\n",
      "Epoch 21/50\n",
      "2/2 [==============================] - 1s 637ms/step - loss: 0.0020 - mae: 0.0341 - val_loss: 0.5594 - val_mae: 0.6927\n",
      "Epoch 22/50\n",
      "2/2 [==============================] - 1s 597ms/step - loss: 0.0015 - mae: 0.0292 - val_loss: 0.5511 - val_mae: 0.6860\n",
      "Epoch 23/50\n",
      "2/2 [==============================] - 1s 600ms/step - loss: 0.0015 - mae: 0.0291 - val_loss: 0.5452 - val_mae: 0.6817\n",
      "Epoch 24/50\n",
      "2/2 [==============================] - 1s 616ms/step - loss: 0.0015 - mae: 0.0304 - val_loss: 0.5398 - val_mae: 0.6766\n",
      "Epoch 25/50\n",
      "2/2 [==============================] - 1s 667ms/step - loss: 0.0012 - mae: 0.0261 - val_loss: 0.5277 - val_mae: 0.6690\n",
      "Epoch 26/50\n",
      "2/2 [==============================] - 1s 650ms/step - loss: 0.0010 - mae: 0.0244 - val_loss: 0.5162 - val_mae: 0.6606\n",
      "Epoch 27/50\n",
      "2/2 [==============================] - 1s 655ms/step - loss: 8.7881e-04 - mae: 0.0227 - val_loss: 0.5063 - val_mae: 0.6545\n",
      "Epoch 28/50\n",
      "2/2 [==============================] - 1s 611ms/step - loss: 9.8359e-04 - mae: 0.0244 - val_loss: 0.5029 - val_mae: 0.6513\n",
      "Epoch 29/50\n",
      "2/2 [==============================] - 1s 641ms/step - loss: 8.5637e-04 - mae: 0.0223 - val_loss: 0.4979 - val_mae: 0.6495\n",
      "Epoch 30/50\n",
      "2/2 [==============================] - 1s 619ms/step - loss: 0.0010 - mae: 0.0243 - val_loss: 0.4943 - val_mae: 0.6474\n",
      "Epoch 31/50\n",
      "2/2 [==============================] - 1s 599ms/step - loss: 6.9127e-04 - mae: 0.0196 - val_loss: 0.4877 - val_mae: 0.6444\n",
      "Epoch 32/50\n",
      "2/2 [==============================] - 1s 601ms/step - loss: 7.8319e-04 - mae: 0.0215 - val_loss: 0.4836 - val_mae: 0.6424\n",
      "Epoch 33/50\n",
      "2/2 [==============================] - 1s 607ms/step - loss: 5.7844e-04 - mae: 0.0180 - val_loss: 0.4772 - val_mae: 0.6402\n",
      "Epoch 34/50\n",
      "2/2 [==============================] - 1s 608ms/step - loss: 0.0011 - mae: 0.0262 - val_loss: 0.4682 - val_mae: 0.6372\n",
      "Epoch 35/50\n",
      "2/2 [==============================] - 1s 624ms/step - loss: 5.4465e-04 - mae: 0.0175 - val_loss: 0.4665 - val_mae: 0.6357\n",
      "Epoch 36/50\n",
      "2/2 [==============================] - 1s 610ms/step - loss: 5.4687e-04 - mae: 0.0173 - val_loss: 0.4640 - val_mae: 0.6338\n",
      "Epoch 37/50\n",
      "2/2 [==============================] - 1s 597ms/step - loss: 0.0010 - mae: 0.0246 - val_loss: 0.4565 - val_mae: 0.6312\n",
      "Epoch 38/50\n",
      "2/2 [==============================] - 1s 609ms/step - loss: 7.1688e-04 - mae: 0.0199 - val_loss: 0.4555 - val_mae: 0.6284\n",
      "Epoch 39/50\n",
      "2/2 [==============================] - 1s 614ms/step - loss: 4.6859e-04 - mae: 0.0162 - val_loss: 0.4516 - val_mae: 0.6254\n",
      "Epoch 40/50\n",
      "2/2 [==============================] - 1s 601ms/step - loss: 4.6154e-04 - mae: 0.0154 - val_loss: 0.4493 - val_mae: 0.6237\n",
      "Epoch 41/50\n",
      "2/2 [==============================] - 1s 627ms/step - loss: 4.2679e-04 - mae: 0.0149 - val_loss: 0.4493 - val_mae: 0.6247\n",
      "Epoch 42/50\n",
      "2/2 [==============================] - 1s 623ms/step - loss: 3.8015e-04 - mae: 0.0142 - val_loss: 0.4518 - val_mae: 0.6281\n",
      "Epoch 43/50\n",
      "2/2 [==============================] - 1s 650ms/step - loss: 4.5557e-04 - mae: 0.0157 - val_loss: 0.4536 - val_mae: 0.6303\n",
      "Epoch 44/50\n",
      "2/2 [==============================] - 1s 665ms/step - loss: 4.7147e-04 - mae: 0.0163 - val_loss: 0.4502 - val_mae: 0.6303\n",
      "Epoch 45/50\n",
      "2/2 [==============================] - 1s 610ms/step - loss: 3.3620e-04 - mae: 0.0131 - val_loss: 0.4462 - val_mae: 0.6283\n",
      "Epoch 46/50\n",
      "2/2 [==============================] - 1s 620ms/step - loss: 6.3480e-04 - mae: 0.0191 - val_loss: 0.4435 - val_mae: 0.6278\n",
      "Epoch 47/50\n",
      "2/2 [==============================] - 1s 618ms/step - loss: 3.7065e-04 - mae: 0.0139 - val_loss: 0.4458 - val_mae: 0.6294\n",
      "Epoch 48/50\n",
      "2/2 [==============================] - 1s 596ms/step - loss: 5.8806e-04 - mae: 0.0180 - val_loss: 0.4475 - val_mae: 0.6303\n",
      "Epoch 49/50\n",
      "2/2 [==============================] - 1s 623ms/step - loss: 3.9044e-04 - mae: 0.0147 - val_loss: 0.4443 - val_mae: 0.6298\n",
      "Epoch 50/50\n",
      "2/2 [==============================] - 1s 645ms/step - loss: 4.0181e-04 - mae: 0.0148 - val_loss: 0.4433 - val_mae: 0.6294\n",
      "5/5 [==============================] - 1s 107ms/step\n",
      "1/1 [==============================] - 0s 196ms/step\n"
     ]
    }
   ],
   "source": [
    "poland = pd.read_csv(\"data/tabular_data_poland.csv\")\n",
    "poland = poland[poland[\"year\"] < 2023]\n",
    "poland_sum = poland[['year', 'region', 'allangle_snow_free_hq_sum'] + log_bin_columns]\n",
    "\n",
    "model = define_cnn(n_features = 4, n_conv = 2, n_dense = 4, input_shape = (609, 911, 1), res_x = 305, res_y = 455)\n",
    "model.compile(optimizer=Adam(), loss='mean_squared_error', metrics=['mae'])\n",
    "model.fit(X, y, epochs=50, batch_size=64, validation_split=0.2)\n",
    "\n",
    "poland_stage_2, poland_pred = extract_features_2(model, poland, 2022, X, X_pred, 4)\n",
    "selected_columns =  [\"feature_\" + str(i) for i in range(1, 5)] + log_bin_columns + ['allangle_snow_free_hq_sum']\n",
    "\n",
    "full_data = pd.concat([poland_stage_2, poland_pred])\n",
    "full_data.sort_values(by=['region', 'year'], inplace=True)\n",
    "full_data = pd.merge(full_data, poland_sum, on=['year', 'region'])\n",
    "full_data_diff = full_data.groupby('region').diff()\n",
    "full_data_diff['region'] = full_data['region']\n",
    "full_data_diff['year'] = full_data['year']\n",
    "full_data_diff.reset_index(drop=True, inplace=True)\n",
    "full_data_diff = full_data_diff[full_data_diff[\"year\"] != 2012]\n",
    "\n",
    "train_data_diff = full_data_diff[full_data_diff[\"year\"] != 2022]\n",
    "pred_data_diff = full_data_diff[full_data_diff[\"year\"] == 2022]\n",
    "pred_data_diff.reset_index(drop=True, inplace=True)\n",
    "train_data_diff.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a neural network\n",
    "train_data_nn = train_data_diff.copy()\n",
    "pred_data_nn = pred_data_diff.copy()\n",
    "\n",
    "train_data_nn = pd.get_dummies(train_data_nn, columns=[\"region\"])\n",
    "pred_data_nn = pd.get_dummies(pred_data_nn, columns=[\"region\"])\n",
    "\n",
    "X_train_nn = train_data_nn.drop(columns=[\"year\", \"real_gdp\"])\n",
    "X_train_nn = X_train_nn.drop(columns=log_bin_columns)\n",
    "# X_train_nn = X_train_nn.drop(columns=[\"allangle_snow_free_hq_mean\"])\n",
    "y_train_nn = train_data_nn[\"real_gdp\"]\n",
    "\n",
    "X_test_nn = pred_data_nn.drop(columns=[\"year\", \"real_gdp\"])\n",
    "X_test_nn = X_test_nn.drop(columns=log_bin_columns)\n",
    "# X_test_nn = X_test_nn.drop(columns=[\"allangle_snow_free_hq_mean\"])\n",
    "y_test_nn = pred_data_nn[\"real_gdp\"]\n",
    "\n",
    "X_train_nn = np.array(X_train_nn, dtype=np.float32)\n",
    "X_test_nn = np.array(X_test_nn, dtype=np.float32)\n",
    "y_train_nn = np.array(y_train_nn, dtype=np.float32)\n",
    "y_test_nn = np.array(y_test_nn, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\jakub\\anaconda\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/500\n",
      "2/2 [==============================] - 1s 220ms/step - loss: 60758412.0000 - mae: 5033.5737 - val_loss: 25964048.0000 - val_mae: 3766.4683\n",
      "Epoch 2/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 60757660.0000 - mae: 5033.5225 - val_loss: 25963662.0000 - val_mae: 3766.4358\n",
      "Epoch 3/500\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 60756940.0000 - mae: 5033.4717 - val_loss: 25963262.0000 - val_mae: 3766.4036\n",
      "Epoch 4/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 60756192.0000 - mae: 5033.4219 - val_loss: 25962854.0000 - val_mae: 3766.3706\n",
      "Epoch 5/500\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 60755444.0000 - mae: 5033.3730 - val_loss: 25962446.0000 - val_mae: 3766.3367\n",
      "Epoch 6/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 60754764.0000 - mae: 5033.3252 - val_loss: 25962032.0000 - val_mae: 3766.3020\n",
      "Epoch 7/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 60753992.0000 - mae: 5033.2705 - val_loss: 25961612.0000 - val_mae: 3766.2673\n",
      "Epoch 8/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 60753288.0000 - mae: 5033.2202 - val_loss: 25961186.0000 - val_mae: 3766.2327\n",
      "Epoch 9/500\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 60752512.0000 - mae: 5033.1689 - val_loss: 25960754.0000 - val_mae: 3766.1980\n",
      "Epoch 10/500\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 60751720.0000 - mae: 5033.1123 - val_loss: 25960308.0000 - val_mae: 3766.1614\n",
      "Epoch 11/500\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 60750840.0000 - mae: 5033.0532 - val_loss: 25959858.0000 - val_mae: 3766.1245\n",
      "Epoch 12/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 60749936.0000 - mae: 5032.9902 - val_loss: 25959400.0000 - val_mae: 3766.0867\n",
      "Epoch 13/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 60749040.0000 - mae: 5032.9282 - val_loss: 25958928.0000 - val_mae: 3766.0479\n",
      "Epoch 14/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 60748140.0000 - mae: 5032.8643 - val_loss: 25958448.0000 - val_mae: 3766.0085\n",
      "Epoch 15/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 60747136.0000 - mae: 5032.7935 - val_loss: 25957940.0000 - val_mae: 3765.9673\n",
      "Epoch 16/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 60746192.0000 - mae: 5032.7271 - val_loss: 25957410.0000 - val_mae: 3765.9246\n",
      "Epoch 17/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 60745208.0000 - mae: 5032.6597 - val_loss: 25956864.0000 - val_mae: 3765.8804\n",
      "Epoch 18/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 60744100.0000 - mae: 5032.5859 - val_loss: 25956296.0000 - val_mae: 3765.8342\n",
      "Epoch 19/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 60742916.0000 - mae: 5032.5054 - val_loss: 25955718.0000 - val_mae: 3765.7871\n",
      "Epoch 20/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 60741836.0000 - mae: 5032.4243 - val_loss: 25955114.0000 - val_mae: 3765.7380\n",
      "Epoch 21/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 60740716.0000 - mae: 5032.3467 - val_loss: 25954490.0000 - val_mae: 3765.6868\n",
      "Epoch 22/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 60739380.0000 - mae: 5032.2598 - val_loss: 25953852.0000 - val_mae: 3765.6350\n",
      "Epoch 23/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 60738100.0000 - mae: 5032.1719 - val_loss: 25953190.0000 - val_mae: 3765.5808\n",
      "Epoch 24/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 60736692.0000 - mae: 5032.0723 - val_loss: 25952508.0000 - val_mae: 3765.5251\n",
      "Epoch 25/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 60735160.0000 - mae: 5031.9751 - val_loss: 25951810.0000 - val_mae: 3765.4683\n",
      "Epoch 26/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 60733664.0000 - mae: 5031.8735 - val_loss: 25951084.0000 - val_mae: 3765.4099\n",
      "Epoch 27/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 60732024.0000 - mae: 5031.7627 - val_loss: 25950336.0000 - val_mae: 3765.3499\n",
      "Epoch 28/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 60730344.0000 - mae: 5031.6455 - val_loss: 25949564.0000 - val_mae: 3765.2888\n",
      "Epoch 29/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 60728408.0000 - mae: 5031.5283 - val_loss: 25948760.0000 - val_mae: 3765.2251\n",
      "Epoch 30/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 60726504.0000 - mae: 5031.4023 - val_loss: 25947898.0000 - val_mae: 3765.1558\n",
      "Epoch 31/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 60724572.0000 - mae: 5031.2695 - val_loss: 25946988.0000 - val_mae: 3765.0823\n",
      "Epoch 32/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 60722400.0000 - mae: 5031.1313 - val_loss: 25946056.0000 - val_mae: 3765.0071\n",
      "Epoch 33/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 60720064.0000 - mae: 5030.9883 - val_loss: 25945090.0000 - val_mae: 3764.9290\n",
      "Epoch 34/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 60717672.0000 - mae: 5030.8335 - val_loss: 25944094.0000 - val_mae: 3764.8486\n",
      "Epoch 35/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 60715380.0000 - mae: 5030.6763 - val_loss: 25943054.0000 - val_mae: 3764.7651\n",
      "Epoch 36/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 60712668.0000 - mae: 5030.5054 - val_loss: 25941986.0000 - val_mae: 3764.6792\n",
      "Epoch 37/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 60709728.0000 - mae: 5030.3271 - val_loss: 25940890.0000 - val_mae: 3764.5911\n",
      "Epoch 38/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 60706728.0000 - mae: 5030.1455 - val_loss: 25939758.0000 - val_mae: 3764.5002\n",
      "Epoch 39/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 60703992.0000 - mae: 5029.9585 - val_loss: 25938568.0000 - val_mae: 3764.4043\n",
      "Epoch 40/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 60700788.0000 - mae: 5029.7583 - val_loss: 25937344.0000 - val_mae: 3764.3066\n",
      "Epoch 41/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 60697288.0000 - mae: 5029.5400 - val_loss: 25936082.0000 - val_mae: 3764.2063\n",
      "Epoch 42/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 60693856.0000 - mae: 5029.3213 - val_loss: 25934770.0000 - val_mae: 3764.1023\n",
      "Epoch 43/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 60690044.0000 - mae: 5029.0806 - val_loss: 25933422.0000 - val_mae: 3763.9954\n",
      "Epoch 44/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 60686284.0000 - mae: 5028.8516 - val_loss: 25932032.0000 - val_mae: 3763.8857\n",
      "Epoch 45/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 60681672.0000 - mae: 5028.5752 - val_loss: 25930630.0000 - val_mae: 3763.7744\n",
      "Epoch 46/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 60677252.0000 - mae: 5028.2979 - val_loss: 25929170.0000 - val_mae: 3763.6599\n",
      "Epoch 47/500\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 60673120.0000 - mae: 5028.0278 - val_loss: 25927644.0000 - val_mae: 3763.5408\n",
      "Epoch 48/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 60667772.0000 - mae: 5027.7173 - val_loss: 25926104.0000 - val_mae: 3763.4197\n",
      "Epoch 49/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 60663068.0000 - mae: 5027.4204 - val_loss: 25924486.0000 - val_mae: 3763.2937\n",
      "Epoch 50/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 60657716.0000 - mae: 5027.0850 - val_loss: 25922842.0000 - val_mae: 3763.1648\n",
      "Epoch 51/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 60651844.0000 - mae: 5026.7368 - val_loss: 25921160.0000 - val_mae: 3763.0337\n",
      "Epoch 52/500\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 60646220.0000 - mae: 5026.3701 - val_loss: 25919432.0000 - val_mae: 3762.8989\n",
      "Epoch 53/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 60640060.0000 - mae: 5025.9888 - val_loss: 25917650.0000 - val_mae: 3762.7603\n",
      "Epoch 54/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 60633624.0000 - mae: 5025.5918 - val_loss: 25915816.0000 - val_mae: 3762.6177\n",
      "Epoch 55/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 60627068.0000 - mae: 5025.2012 - val_loss: 25913950.0000 - val_mae: 3762.4724\n",
      "Epoch 56/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 60619312.0000 - mae: 5024.7314 - val_loss: 25912088.0000 - val_mae: 3762.3281\n",
      "Epoch 57/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 60611824.0000 - mae: 5024.2749 - val_loss: 25910156.0000 - val_mae: 3762.1780\n",
      "Epoch 58/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 60604584.0000 - mae: 5023.8130 - val_loss: 25908162.0000 - val_mae: 3762.0237\n",
      "Epoch 59/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 60596192.0000 - mae: 5023.3081 - val_loss: 25906114.0000 - val_mae: 3761.8652\n",
      "Epoch 60/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 60587448.0000 - mae: 5022.7964 - val_loss: 25904014.0000 - val_mae: 3761.7024\n",
      "Epoch 61/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 60578816.0000 - mae: 5022.2729 - val_loss: 25901854.0000 - val_mae: 3761.5349\n",
      "Epoch 62/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 60569884.0000 - mae: 5021.7065 - val_loss: 25899634.0000 - val_mae: 3761.3635\n",
      "Epoch 63/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 60559724.0000 - mae: 5021.1040 - val_loss: 25897388.0000 - val_mae: 3761.1895\n",
      "Epoch 64/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 60550156.0000 - mae: 5020.5020 - val_loss: 25895070.0000 - val_mae: 3761.0100\n",
      "Epoch 65/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 60539172.0000 - mae: 5019.8574 - val_loss: 25892724.0000 - val_mae: 3760.8291\n",
      "Epoch 66/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 60527996.0000 - mae: 5019.1880 - val_loss: 25890306.0000 - val_mae: 3760.6428\n",
      "Epoch 67/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 60517260.0000 - mae: 5018.5044 - val_loss: 25887802.0000 - val_mae: 3760.4500\n",
      "Epoch 68/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 60504932.0000 - mae: 5017.7793 - val_loss: 25885264.0000 - val_mae: 3760.2561\n",
      "Epoch 69/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 60491828.0000 - mae: 5017.0151 - val_loss: 25882690.0000 - val_mae: 3760.0610\n",
      "Epoch 70/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 60479140.0000 - mae: 5016.2539 - val_loss: 25880030.0000 - val_mae: 3759.8596\n",
      "Epoch 71/500\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 60466356.0000 - mae: 5015.4482 - val_loss: 25877300.0000 - val_mae: 3759.6526\n",
      "Epoch 72/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 60451964.0000 - mae: 5014.6401 - val_loss: 25874514.0000 - val_mae: 3759.4412\n",
      "Epoch 73/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 60438040.0000 - mae: 5013.7964 - val_loss: 25871660.0000 - val_mae: 3759.2256\n",
      "Epoch 74/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 60422456.0000 - mae: 5012.8311 - val_loss: 25868776.0000 - val_mae: 3759.0071\n",
      "Epoch 75/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 60407380.0000 - mae: 5011.9331 - val_loss: 25865836.0000 - val_mae: 3758.7856\n",
      "Epoch 76/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 60389624.0000 - mae: 5010.9258 - val_loss: 25862890.0000 - val_mae: 3758.5645\n",
      "Epoch 77/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 60374060.0000 - mae: 5009.9673 - val_loss: 25859796.0000 - val_mae: 3758.3325\n",
      "Epoch 78/500\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 60356048.0000 - mae: 5008.8813 - val_loss: 25856664.0000 - val_mae: 3758.0979\n",
      "Epoch 79/500\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 60338716.0000 - mae: 5007.8643 - val_loss: 25853452.0000 - val_mae: 3757.8574\n",
      "Epoch 80/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 60318668.0000 - mae: 5006.7480 - val_loss: 25850190.0000 - val_mae: 3757.6169\n",
      "Epoch 81/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 60300552.0000 - mae: 5005.5913 - val_loss: 25846834.0000 - val_mae: 3757.3701\n",
      "Epoch 82/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 60278144.0000 - mae: 5004.3745 - val_loss: 25843438.0000 - val_mae: 3757.1233\n",
      "Epoch 83/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 60259212.0000 - mae: 5003.1807 - val_loss: 25839896.0000 - val_mae: 3756.8674\n",
      "Epoch 84/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 60236288.0000 - mae: 5001.8804 - val_loss: 25836282.0000 - val_mae: 3756.6084\n",
      "Epoch 85/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 60214672.0000 - mae: 5000.5718 - val_loss: 25832494.0000 - val_mae: 3756.3362\n",
      "Epoch 86/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 60190816.0000 - mae: 4999.1812 - val_loss: 25828634.0000 - val_mae: 3756.0598\n",
      "Epoch 87/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 60167120.0000 - mae: 4997.7900 - val_loss: 25824606.0000 - val_mae: 3755.7732\n",
      "Epoch 88/500\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 60142012.0000 - mae: 4996.3154 - val_loss: 25820540.0000 - val_mae: 3755.4841\n",
      "Epoch 89/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 60117712.0000 - mae: 4994.8506 - val_loss: 25816348.0000 - val_mae: 3755.1870\n",
      "Epoch 90/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 60088684.0000 - mae: 4993.1689 - val_loss: 25812188.0000 - val_mae: 3754.8921\n",
      "Epoch 91/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 60062404.0000 - mae: 4991.6108 - val_loss: 25807876.0000 - val_mae: 3754.5874\n",
      "Epoch 92/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 60031188.0000 - mae: 4989.8608 - val_loss: 25803494.0000 - val_mae: 3754.2786\n",
      "Epoch 93/500\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 60003532.0000 - mae: 4988.1421 - val_loss: 25798900.0000 - val_mae: 3753.9548\n",
      "Epoch 94/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 59972180.0000 - mae: 4986.3618 - val_loss: 25794246.0000 - val_mae: 3753.6272\n",
      "Epoch 95/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 59938452.0000 - mae: 4984.5317 - val_loss: 25789520.0000 - val_mae: 3753.2964\n",
      "Epoch 96/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 59911428.0000 - mae: 4982.7251 - val_loss: 25784546.0000 - val_mae: 3752.9475\n",
      "Epoch 97/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 59875232.0000 - mae: 4980.6885 - val_loss: 25779534.0000 - val_mae: 3752.5991\n",
      "Epoch 98/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 59839896.0000 - mae: 4978.6792 - val_loss: 25774376.0000 - val_mae: 3752.2441\n",
      "Epoch 99/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 59801872.0000 - mae: 4976.4292 - val_loss: 25769126.0000 - val_mae: 3751.8826\n",
      "Epoch 100/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 59768492.0000 - mae: 4974.3687 - val_loss: 25763652.0000 - val_mae: 3751.5073\n",
      "Epoch 101/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 59728360.0000 - mae: 4972.1792 - val_loss: 25758110.0000 - val_mae: 3751.1272\n",
      "Epoch 102/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 59689692.0000 - mae: 4969.7632 - val_loss: 25752460.0000 - val_mae: 3750.7415\n",
      "Epoch 103/500\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 59646268.0000 - mae: 4967.3335 - val_loss: 25746788.0000 - val_mae: 3750.3560\n",
      "Epoch 104/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 59604976.0000 - mae: 4964.9980 - val_loss: 25741000.0000 - val_mae: 3749.9666\n",
      "Epoch 105/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 59562276.0000 - mae: 4962.6860 - val_loss: 25735062.0000 - val_mae: 3749.5684\n",
      "Epoch 106/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 59519216.0000 - mae: 4960.2056 - val_loss: 25729038.0000 - val_mae: 3749.1658\n",
      "Epoch 107/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 59468408.0000 - mae: 4957.3955 - val_loss: 25722960.0000 - val_mae: 3748.7595\n",
      "Epoch 108/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 59424820.0000 - mae: 4954.8525 - val_loss: 25716608.0000 - val_mae: 3748.3357\n",
      "Epoch 109/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 59375312.0000 - mae: 4952.2246 - val_loss: 25710140.0000 - val_mae: 3747.9055\n",
      "Epoch 110/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 59318852.0000 - mae: 4949.2617 - val_loss: 25703720.0000 - val_mae: 3747.4836\n",
      "Epoch 111/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 59275720.0000 - mae: 4946.4707 - val_loss: 25696932.0000 - val_mae: 3747.0361\n",
      "Epoch 112/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 59225072.0000 - mae: 4943.7881 - val_loss: 25690028.0000 - val_mae: 3746.5845\n",
      "Epoch 113/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 59164368.0000 - mae: 4940.5356 - val_loss: 25683234.0000 - val_mae: 3746.1433\n",
      "Epoch 114/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 59111060.0000 - mae: 4937.3706 - val_loss: 25676252.0000 - val_mae: 3745.6938\n",
      "Epoch 115/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 59050500.0000 - mae: 4934.1631 - val_loss: 25669086.0000 - val_mae: 3745.2361\n",
      "Epoch 116/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 58994416.0000 - mae: 4931.0850 - val_loss: 25661646.0000 - val_mae: 3744.7634\n",
      "Epoch 117/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 58930972.0000 - mae: 4927.5869 - val_loss: 25654014.0000 - val_mae: 3744.2839\n",
      "Epoch 118/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 58874000.0000 - mae: 4924.2896 - val_loss: 25646182.0000 - val_mae: 3743.7925\n",
      "Epoch 119/500\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 58805588.0000 - mae: 4920.7969 - val_loss: 25638278.0000 - val_mae: 3743.2988\n",
      "Epoch 120/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 58741564.0000 - mae: 4916.9966 - val_loss: 25630158.0000 - val_mae: 3742.7942\n",
      "Epoch 121/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 58674532.0000 - mae: 4913.4741 - val_loss: 25621874.0000 - val_mae: 3742.2871\n",
      "Epoch 122/500\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 58602408.0000 - mae: 4909.3955 - val_loss: 25613464.0000 - val_mae: 3741.7812\n",
      "Epoch 123/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 58532780.0000 - mae: 4905.4673 - val_loss: 25604760.0000 - val_mae: 3741.2576\n",
      "Epoch 124/500\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 58460768.0000 - mae: 4901.3501 - val_loss: 25595788.0000 - val_mae: 3740.7112\n",
      "Epoch 125/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 58386832.0000 - mae: 4897.0674 - val_loss: 25586612.0000 - val_mae: 3740.1465\n",
      "Epoch 126/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 58307004.0000 - mae: 4892.9849 - val_loss: 25577146.0000 - val_mae: 3739.5564\n",
      "Epoch 127/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 58229564.0000 - mae: 4888.7163 - val_loss: 25567578.0000 - val_mae: 3738.9683\n",
      "Epoch 128/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 58151352.0000 - mae: 4884.3887 - val_loss: 25557720.0000 - val_mae: 3738.3638\n",
      "Epoch 129/500\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 58067136.0000 - mae: 4879.4033 - val_loss: 25547664.0000 - val_mae: 3737.7456\n",
      "Epoch 130/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 57975664.0000 - mae: 4874.7607 - val_loss: 25537578.0000 - val_mae: 3737.1321\n",
      "Epoch 131/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 57899936.0000 - mae: 4870.3042 - val_loss: 25527056.0000 - val_mae: 3736.4888\n",
      "Epoch 132/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 57809044.0000 - mae: 4865.3145 - val_loss: 25516280.0000 - val_mae: 3735.8340\n",
      "Epoch 133/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 57718412.0000 - mae: 4860.0806 - val_loss: 25505532.0000 - val_mae: 3735.1858\n",
      "Epoch 134/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 57620384.0000 - mae: 4855.0625 - val_loss: 25494702.0000 - val_mae: 3734.5354\n",
      "Epoch 135/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 57531604.0000 - mae: 4849.8066 - val_loss: 25483568.0000 - val_mae: 3733.8647\n",
      "Epoch 136/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 57432736.0000 - mae: 4844.3955 - val_loss: 25472274.0000 - val_mae: 3733.1853\n",
      "Epoch 137/500\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 57336188.0000 - mae: 4838.7954 - val_loss: 25460616.0000 - val_mae: 3732.4839\n",
      "Epoch 138/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 57232028.0000 - mae: 4833.1006 - val_loss: 25448752.0000 - val_mae: 3731.7688\n",
      "Epoch 139/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 57132100.0000 - mae: 4827.6143 - val_loss: 25436688.0000 - val_mae: 3731.0442\n",
      "Epoch 140/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 57028280.0000 - mae: 4821.4023 - val_loss: 25424424.0000 - val_mae: 3730.3064\n",
      "Epoch 141/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 56915656.0000 - mae: 4815.4990 - val_loss: 25412230.0000 - val_mae: 3729.5752\n",
      "Epoch 142/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 56799372.0000 - mae: 4809.2827 - val_loss: 25399866.0000 - val_mae: 3728.8462\n",
      "Epoch 143/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 56694524.0000 - mae: 4802.7607 - val_loss: 25386946.0000 - val_mae: 3728.0791\n",
      "Epoch 144/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 56576792.0000 - mae: 4796.4512 - val_loss: 25373906.0000 - val_mae: 3727.2971\n",
      "Epoch 145/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 56466148.0000 - mae: 4790.2441 - val_loss: 25360478.0000 - val_mae: 3726.4927\n",
      "Epoch 146/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 56345320.0000 - mae: 4783.2393 - val_loss: 25346962.0000 - val_mae: 3725.6804\n",
      "Epoch 147/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 56226412.0000 - mae: 4776.2871 - val_loss: 25333208.0000 - val_mae: 3724.8562\n",
      "Epoch 148/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 56090776.0000 - mae: 4769.0566 - val_loss: 25319700.0000 - val_mae: 3724.0483\n",
      "Epoch 149/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 55972260.0000 - mae: 4762.0469 - val_loss: 25305586.0000 - val_mae: 3723.2007\n",
      "Epoch 150/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 55850724.0000 - mae: 4754.8540 - val_loss: 25291278.0000 - val_mae: 3722.3440\n",
      "Epoch 151/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 55714548.0000 - mae: 4747.1011 - val_loss: 25276952.0000 - val_mae: 3721.4880\n",
      "Epoch 152/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 55585052.0000 - mae: 4739.6689 - val_loss: 25262522.0000 - val_mae: 3720.6260\n",
      "Epoch 153/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 55429368.0000 - mae: 4731.5933 - val_loss: 25248554.0000 - val_mae: 3719.7966\n",
      "Epoch 154/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 55300428.0000 - mae: 4724.0259 - val_loss: 25234018.0000 - val_mae: 3718.9268\n",
      "Epoch 155/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 55167776.0000 - mae: 4716.2144 - val_loss: 25219180.0000 - val_mae: 3718.0415\n",
      "Epoch 156/500\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 55020724.0000 - mae: 4708.0132 - val_loss: 25204078.0000 - val_mae: 3717.1353\n",
      "Epoch 157/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 54880072.0000 - mae: 4699.5825 - val_loss: 25188848.0000 - val_mae: 3716.2146\n",
      "Epoch 158/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 54724056.0000 - mae: 4691.3359 - val_loss: 25173490.0000 - val_mae: 3715.2861\n",
      "Epoch 159/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 54589760.0000 - mae: 4683.1963 - val_loss: 25157638.0000 - val_mae: 3714.3279\n",
      "Epoch 160/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 54432972.0000 - mae: 4674.6406 - val_loss: 25141818.0000 - val_mae: 3713.3638\n",
      "Epoch 161/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 54275464.0000 - mae: 4665.6943 - val_loss: 25125960.0000 - val_mae: 3712.3875\n",
      "Epoch 162/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 54109452.0000 - mae: 4656.6343 - val_loss: 25110082.0000 - val_mae: 3711.4131\n",
      "Epoch 163/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 53962244.0000 - mae: 4647.9272 - val_loss: 25093740.0000 - val_mae: 3710.4055\n",
      "Epoch 164/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 53794580.0000 - mae: 4638.6436 - val_loss: 25077276.0000 - val_mae: 3709.3899\n",
      "Epoch 165/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 53613228.0000 - mae: 4628.6943 - val_loss: 25061082.0000 - val_mae: 3708.3835\n",
      "Epoch 166/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 53467200.0000 - mae: 4620.2939 - val_loss: 25044136.0000 - val_mae: 3707.3276\n",
      "Epoch 167/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 53289112.0000 - mae: 4610.4946 - val_loss: 25026812.0000 - val_mae: 3706.2478\n",
      "Epoch 168/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 53126904.0000 - mae: 4600.8037 - val_loss: 25008994.0000 - val_mae: 3705.1301\n",
      "Epoch 169/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 52967104.0000 - mae: 4591.2075 - val_loss: 24990908.0000 - val_mae: 3703.9937\n",
      "Epoch 170/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 52770004.0000 - mae: 4580.7007 - val_loss: 24973106.0000 - val_mae: 3702.8728\n",
      "Epoch 171/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 52595216.0000 - mae: 4570.5117 - val_loss: 24955064.0000 - val_mae: 3701.7319\n",
      "Epoch 172/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 52409376.0000 - mae: 4559.9819 - val_loss: 24937030.0000 - val_mae: 3700.5916\n",
      "Epoch 173/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 52228540.0000 - mae: 4549.1313 - val_loss: 24918354.0000 - val_mae: 3699.3989\n",
      "Epoch 174/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 52051228.0000 - mae: 4538.5156 - val_loss: 24899462.0000 - val_mae: 3698.1804\n",
      "Epoch 175/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 51867032.0000 - mae: 4528.1904 - val_loss: 24880268.0000 - val_mae: 3696.9329\n",
      "Epoch 176/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 51675952.0000 - mae: 4517.0913 - val_loss: 24861092.0000 - val_mae: 3695.6843\n",
      "Epoch 177/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 51483776.0000 - mae: 4505.6772 - val_loss: 24841988.0000 - val_mae: 3694.4397\n",
      "Epoch 178/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 51273292.0000 - mae: 4493.8862 - val_loss: 24823424.0000 - val_mae: 3693.2273\n",
      "Epoch 179/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 51103208.0000 - mae: 4482.8594 - val_loss: 24803870.0000 - val_mae: 3691.9456\n",
      "Epoch 180/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 50892196.0000 - mae: 4471.1895 - val_loss: 24784590.0000 - val_mae: 3690.6897\n",
      "Epoch 181/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 50683336.0000 - mae: 4459.3081 - val_loss: 24765670.0000 - val_mae: 3689.4629\n",
      "Epoch 182/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 50505088.0000 - mae: 4447.8472 - val_loss: 24745886.0000 - val_mae: 3688.1711\n",
      "Epoch 183/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 50284472.0000 - mae: 4435.3599 - val_loss: 24725994.0000 - val_mae: 3686.8657\n",
      "Epoch 184/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 50071596.0000 - mae: 4422.7026 - val_loss: 24706132.0000 - val_mae: 3685.5564\n",
      "Epoch 185/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 49875540.0000 - mae: 4410.0249 - val_loss: 24685448.0000 - val_mae: 3684.1897\n",
      "Epoch 186/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 49662392.0000 - mae: 4398.3813 - val_loss: 24665106.0000 - val_mae: 3682.8433\n",
      "Epoch 187/500\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 49442704.0000 - mae: 4385.5474 - val_loss: 24644728.0000 - val_mae: 3681.4807\n",
      "Epoch 188/500\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 49239316.0000 - mae: 4373.3794 - val_loss: 24624414.0000 - val_mae: 3680.1177\n",
      "Epoch 189/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 49019776.0000 - mae: 4361.0093 - val_loss: 24603648.0000 - val_mae: 3678.7168\n",
      "Epoch 190/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 48806372.0000 - mae: 4348.2866 - val_loss: 24582684.0000 - val_mae: 3677.3000\n",
      "Epoch 191/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 48566912.0000 - mae: 4334.1528 - val_loss: 24562352.0000 - val_mae: 3675.9209\n",
      "Epoch 192/500\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 48363308.0000 - mae: 4322.8779 - val_loss: 24541310.0000 - val_mae: 3674.4875\n",
      "Epoch 193/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 48119132.0000 - mae: 4308.4575 - val_loss: 24520738.0000 - val_mae: 3673.0833\n",
      "Epoch 194/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 47932556.0000 - mae: 4296.6616 - val_loss: 24499398.0000 - val_mae: 3671.6162\n",
      "Epoch 195/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 47680620.0000 - mae: 4283.6104 - val_loss: 24478800.0000 - val_mae: 3670.1912\n",
      "Epoch 196/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 47442908.0000 - mae: 4269.2759 - val_loss: 24458158.0000 - val_mae: 3668.7578\n",
      "Epoch 197/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 47227488.0000 - mae: 4255.6836 - val_loss: 24437202.0000 - val_mae: 3667.2971\n",
      "Epoch 198/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 46976240.0000 - mae: 4242.1758 - val_loss: 24416816.0000 - val_mae: 3665.8677\n",
      "Epoch 199/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 46769004.0000 - mae: 4228.9634 - val_loss: 24395696.0000 - val_mae: 3664.3687\n",
      "Epoch 200/500\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 46503184.0000 - mae: 4213.7407 - val_loss: 24374920.0000 - val_mae: 3662.8843\n",
      "Epoch 201/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 46296528.0000 - mae: 4201.1475 - val_loss: 24353496.0000 - val_mae: 3661.3440\n",
      "Epoch 202/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 46036764.0000 - mae: 4185.4292 - val_loss: 24332984.0000 - val_mae: 3659.8589\n",
      "Epoch 203/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 45799660.0000 - mae: 4172.0195 - val_loss: 24311888.0000 - val_mae: 3658.3291\n",
      "Epoch 204/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 45564944.0000 - mae: 4156.9087 - val_loss: 24290776.0000 - val_mae: 3656.7864\n",
      "Epoch 205/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 45337416.0000 - mae: 4141.8887 - val_loss: 24269150.0000 - val_mae: 3655.2004\n",
      "Epoch 206/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 45083192.0000 - mae: 4127.1187 - val_loss: 24248002.0000 - val_mae: 3653.6370\n",
      "Epoch 207/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 44828332.0000 - mae: 4112.2402 - val_loss: 24227288.0000 - val_mae: 3652.0894\n",
      "Epoch 208/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 44581972.0000 - mae: 4097.3696 - val_loss: 24206030.0000 - val_mae: 3650.4851\n",
      "Epoch 209/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 44345556.0000 - mae: 4080.9978 - val_loss: 24184214.0000 - val_mae: 3648.8303\n",
      "Epoch 210/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 44105860.0000 - mae: 4066.9653 - val_loss: 24162800.0000 - val_mae: 3647.1987\n",
      "Epoch 211/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 43836300.0000 - mae: 4050.6541 - val_loss: 24142078.0000 - val_mae: 3645.6123\n",
      "Epoch 212/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 43587776.0000 - mae: 4034.1741 - val_loss: 24121088.0000 - val_mae: 3643.9924\n",
      "Epoch 213/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 43334824.0000 - mae: 4018.0005 - val_loss: 24100294.0000 - val_mae: 3642.3777\n",
      "Epoch 214/500\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 43113144.0000 - mae: 4002.7354 - val_loss: 24079138.0000 - val_mae: 3640.7209\n",
      "Epoch 215/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 42812364.0000 - mae: 3986.2954 - val_loss: 24058766.0000 - val_mae: 3639.1199\n",
      "Epoch 216/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 42570884.0000 - mae: 3969.8340 - val_loss: 24038824.0000 - val_mae: 3637.5410\n",
      "Epoch 217/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 42312316.0000 - mae: 3953.1587 - val_loss: 24018682.0000 - val_mae: 3635.9331\n",
      "Epoch 218/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 42049428.0000 - mae: 3936.5164 - val_loss: 23998414.0000 - val_mae: 3634.3062\n",
      "Epoch 219/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 41808736.0000 - mae: 3920.3142 - val_loss: 23977872.0000 - val_mae: 3632.6389\n",
      "Epoch 220/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 41547180.0000 - mae: 3902.5823 - val_loss: 23957418.0000 - val_mae: 3630.9580\n",
      "Epoch 221/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 41307136.0000 - mae: 3886.8108 - val_loss: 23936994.0000 - val_mae: 3629.2710\n",
      "Epoch 222/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 41033052.0000 - mae: 3868.6765 - val_loss: 23916940.0000 - val_mae: 3627.6018\n",
      "Epoch 223/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 40789216.0000 - mae: 3852.4541 - val_loss: 23896804.0000 - val_mae: 3625.9060\n",
      "Epoch 224/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 40511860.0000 - mae: 3835.4160 - val_loss: 23877288.0000 - val_mae: 3624.2520\n",
      "Epoch 225/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 40239020.0000 - mae: 3818.3435 - val_loss: 23858196.0000 - val_mae: 3622.6147\n",
      "Epoch 226/500\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 40025296.0000 - mae: 3803.4619 - val_loss: 23838254.0000 - val_mae: 3620.8960\n",
      "Epoch 227/500\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 39719440.0000 - mae: 3785.7170 - val_loss: 23819408.0000 - val_mae: 3619.2517\n",
      "Epoch 228/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 39494892.0000 - mae: 3769.2205 - val_loss: 23800292.0000 - val_mae: 3617.5771\n",
      "Epoch 229/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 39203972.0000 - mae: 3751.1707 - val_loss: 23781458.0000 - val_mae: 3615.9104\n",
      "Epoch 230/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 38971280.0000 - mae: 3735.3022 - val_loss: 23762582.0000 - val_mae: 3614.2244\n",
      "Epoch 231/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 38693660.0000 - mae: 3717.7756 - val_loss: 23743806.0000 - val_mae: 3612.5261\n",
      "Epoch 232/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 38432936.0000 - mae: 3700.3958 - val_loss: 23724818.0000 - val_mae: 3610.7988\n",
      "Epoch 233/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 38207440.0000 - mae: 3685.1882 - val_loss: 23705170.0000 - val_mae: 3609.0078\n",
      "Epoch 234/500\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 37912300.0000 - mae: 3667.3430 - val_loss: 23686784.0000 - val_mae: 3607.3091\n",
      "Epoch 235/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 37673628.0000 - mae: 3651.0439 - val_loss: 23668844.0000 - val_mae: 3605.6333\n",
      "Epoch 236/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 37397724.0000 - mae: 3632.7144 - val_loss: 23650642.0000 - val_mae: 3603.9170\n",
      "Epoch 237/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 37136648.0000 - mae: 3615.5063 - val_loss: 23632342.0000 - val_mae: 3602.1775\n",
      "Epoch 238/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 36883984.0000 - mae: 3599.9692 - val_loss: 23614348.0000 - val_mae: 3600.4524\n",
      "Epoch 239/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 36588372.0000 - mae: 3581.7366 - val_loss: 23597100.0000 - val_mae: 3598.7793\n",
      "Epoch 240/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 36368228.0000 - mae: 3567.3396 - val_loss: 23578794.0000 - val_mae: 3597.0049\n",
      "Epoch 241/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 36063900.0000 - mae: 3549.9358 - val_loss: 23561512.0000 - val_mae: 3595.3074\n",
      "Epoch 242/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 35832456.0000 - mae: 3534.8579 - val_loss: 23543656.0000 - val_mae: 3593.5410\n",
      "Epoch 243/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 35587176.0000 - mae: 3517.9153 - val_loss: 23526142.0000 - val_mae: 3591.7791\n",
      "Epoch 244/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 35317948.0000 - mae: 3501.5022 - val_loss: 23508450.0000 - val_mae: 3589.9919\n",
      "Epoch 245/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 35055904.0000 - mae: 3483.7490 - val_loss: 23490352.0000 - val_mae: 3588.1580\n",
      "Epoch 246/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 34829428.0000 - mae: 3469.5771 - val_loss: 23472412.0000 - val_mae: 3586.3318\n",
      "Epoch 247/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 34552704.0000 - mae: 3453.2195 - val_loss: 23455668.0000 - val_mae: 3584.6016\n",
      "Epoch 248/500\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 34287128.0000 - mae: 3437.4583 - val_loss: 23439790.0000 - val_mae: 3582.9346\n",
      "Epoch 249/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 34050436.0000 - mae: 3423.9690 - val_loss: 23423706.0000 - val_mae: 3581.2292\n",
      "Epoch 250/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 33809716.0000 - mae: 3407.6270 - val_loss: 23407770.0000 - val_mae: 3579.5278\n",
      "Epoch 251/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 33530622.0000 - mae: 3392.3218 - val_loss: 23392418.0000 - val_mae: 3577.8718\n",
      "Epoch 252/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 33279648.0000 - mae: 3376.7168 - val_loss: 23376898.0000 - val_mae: 3576.1794\n",
      "Epoch 253/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 33068870.0000 - mae: 3361.5076 - val_loss: 23361274.0000 - val_mae: 3574.4607\n",
      "Epoch 254/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 32796312.0000 - mae: 3347.1086 - val_loss: 23346352.0000 - val_mae: 3572.7939\n",
      "Epoch 255/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 32566290.0000 - mae: 3331.7065 - val_loss: 23331416.0000 - val_mae: 3571.1121\n",
      "Epoch 256/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 32313086.0000 - mae: 3317.1750 - val_loss: 23317698.0000 - val_mae: 3569.5286\n",
      "Epoch 257/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 32082636.0000 - mae: 3301.3164 - val_loss: 23303124.0000 - val_mae: 3567.8523\n",
      "Epoch 258/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 31834908.0000 - mae: 3286.8315 - val_loss: 23289304.0000 - val_mae: 3566.2371\n",
      "Epoch 259/500\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 31591126.0000 - mae: 3271.6985 - val_loss: 23275420.0000 - val_mae: 3564.5989\n",
      "Epoch 260/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 31384812.0000 - mae: 3258.2317 - val_loss: 23261318.0000 - val_mae: 3562.9160\n",
      "Epoch 261/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 31132396.0000 - mae: 3242.4695 - val_loss: 23247792.0000 - val_mae: 3561.2771\n",
      "Epoch 262/500\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 30882544.0000 - mae: 3228.1169 - val_loss: 23234776.0000 - val_mae: 3559.6824\n",
      "Epoch 263/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 30674436.0000 - mae: 3216.0818 - val_loss: 23221058.0000 - val_mae: 3558.0103\n",
      "Epoch 264/500\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 30413366.0000 - mae: 3201.5369 - val_loss: 23208852.0000 - val_mae: 3556.4810\n",
      "Epoch 265/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 30190134.0000 - mae: 3189.0852 - val_loss: 23196288.0000 - val_mae: 3554.9126\n",
      "Epoch 266/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 29939450.0000 - mae: 3173.8940 - val_loss: 23183960.0000 - val_mae: 3553.3506\n",
      "Epoch 267/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 29723956.0000 - mae: 3160.9358 - val_loss: 23171560.0000 - val_mae: 3551.7627\n",
      "Epoch 268/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 29533052.0000 - mae: 3148.4666 - val_loss: 23158494.0000 - val_mae: 3550.0930\n",
      "Epoch 269/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 29266784.0000 - mae: 3134.5557 - val_loss: 23146188.0000 - val_mae: 3548.4937\n",
      "Epoch 270/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 29027940.0000 - mae: 3118.8936 - val_loss: 23134234.0000 - val_mae: 3546.9106\n",
      "Epoch 271/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 28817400.0000 - mae: 3105.6604 - val_loss: 23122204.0000 - val_mae: 3545.3047\n",
      "Epoch 272/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 28626248.0000 - mae: 3093.9028 - val_loss: 23109340.0000 - val_mae: 3543.5928\n",
      "Epoch 273/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 28350918.0000 - mae: 3080.3625 - val_loss: 23097992.0000 - val_mae: 3542.0361\n",
      "Epoch 274/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 28163064.0000 - mae: 3068.2957 - val_loss: 23086190.0000 - val_mae: 3540.4175\n",
      "Epoch 275/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 27950914.0000 - mae: 3055.2195 - val_loss: 23074902.0000 - val_mae: 3538.8616\n",
      "Epoch 276/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 27722898.0000 - mae: 3043.6843 - val_loss: 23064050.0000 - val_mae: 3537.3484\n",
      "Epoch 277/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 27508160.0000 - mae: 3029.8679 - val_loss: 23052818.0000 - val_mae: 3535.7791\n",
      "Epoch 278/500\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 27288584.0000 - mae: 3018.1182 - val_loss: 23042156.0000 - val_mae: 3534.2688\n",
      "Epoch 279/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 27095414.0000 - mae: 3005.7026 - val_loss: 23031374.0000 - val_mae: 3532.7158\n",
      "Epoch 280/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 26864992.0000 - mae: 2993.1597 - val_loss: 23020758.0000 - val_mae: 3531.1829\n",
      "Epoch 281/500\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 26632646.0000 - mae: 2980.9478 - val_loss: 23010310.0000 - val_mae: 3529.6589\n",
      "Epoch 282/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 26432756.0000 - mae: 2969.3613 - val_loss: 23000100.0000 - val_mae: 3528.1470\n",
      "Epoch 283/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 26223440.0000 - mae: 2955.9402 - val_loss: 22989696.0000 - val_mae: 3526.6099\n",
      "Epoch 284/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 26035612.0000 - mae: 2944.5947 - val_loss: 22979068.0000 - val_mae: 3525.0354\n",
      "Epoch 285/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 25862522.0000 - mae: 2932.6885 - val_loss: 22968188.0000 - val_mae: 3523.4197\n",
      "Epoch 286/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 25627104.0000 - mae: 2919.9836 - val_loss: 22957650.0000 - val_mae: 3521.8406\n",
      "Epoch 287/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 25455912.0000 - mae: 2908.6946 - val_loss: 22946524.0000 - val_mae: 3520.1714\n",
      "Epoch 288/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 25246086.0000 - mae: 2897.0576 - val_loss: 22936128.0000 - val_mae: 3518.5620\n",
      "Epoch 289/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 25067368.0000 - mae: 2885.3386 - val_loss: 22927034.0000 - val_mae: 3517.1104\n",
      "Epoch 290/500\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 24848622.0000 - mae: 2872.0581 - val_loss: 22918244.0000 - val_mae: 3515.6797\n",
      "Epoch 291/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 24656438.0000 - mae: 2859.4331 - val_loss: 22908334.0000 - val_mae: 3514.1199\n",
      "Epoch 292/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 24479362.0000 - mae: 2848.4792 - val_loss: 22899050.0000 - val_mae: 3512.6313\n",
      "Epoch 293/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 24288540.0000 - mae: 2836.7385 - val_loss: 22889988.0000 - val_mae: 3511.1594\n",
      "Epoch 294/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 24109128.0000 - mae: 2825.4658 - val_loss: 22881454.0000 - val_mae: 3509.7510\n",
      "Epoch 295/500\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 23917334.0000 - mae: 2813.4661 - val_loss: 22872646.0000 - val_mae: 3508.3162\n",
      "Epoch 296/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 23750550.0000 - mae: 2803.9583 - val_loss: 22864026.0000 - val_mae: 3506.8657\n",
      "Epoch 297/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 23539424.0000 - mae: 2790.3892 - val_loss: 22855594.0000 - val_mae: 3505.4607\n",
      "Epoch 298/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 23421038.0000 - mae: 2781.0103 - val_loss: 22846558.0000 - val_mae: 3503.9639\n",
      "Epoch 299/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 23209436.0000 - mae: 2769.3386 - val_loss: 22838860.0000 - val_mae: 3502.6250\n",
      "Epoch 300/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 23032124.0000 - mae: 2757.4058 - val_loss: 22831546.0000 - val_mae: 3501.3250\n",
      "Epoch 301/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 22887362.0000 - mae: 2746.5022 - val_loss: 22823496.0000 - val_mae: 3499.9109\n",
      "Epoch 302/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 22702516.0000 - mae: 2734.7036 - val_loss: 22816022.0000 - val_mae: 3498.5837\n",
      "Epoch 303/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 22523180.0000 - mae: 2725.5403 - val_loss: 22808580.0000 - val_mae: 3497.2700\n",
      "Epoch 304/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 22370756.0000 - mae: 2716.6667 - val_loss: 22800684.0000 - val_mae: 3495.8928\n",
      "Epoch 305/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 22198846.0000 - mae: 2707.4873 - val_loss: 22793920.0000 - val_mae: 3494.6069\n",
      "Epoch 306/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 22037714.0000 - mae: 2698.4102 - val_loss: 22786564.0000 - val_mae: 3493.2791\n",
      "Epoch 307/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 21884378.0000 - mae: 2689.0090 - val_loss: 22779066.0000 - val_mae: 3491.9148\n",
      "Epoch 308/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 21712390.0000 - mae: 2681.3293 - val_loss: 22772392.0000 - val_mae: 3490.6414\n",
      "Epoch 309/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 21556656.0000 - mae: 2672.5674 - val_loss: 22765474.0000 - val_mae: 3489.3386\n",
      "Epoch 310/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 21426084.0000 - mae: 2666.4282 - val_loss: 22757468.0000 - val_mae: 3487.8953\n",
      "Epoch 311/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 21248610.0000 - mae: 2658.6228 - val_loss: 22750338.0000 - val_mae: 3486.5933\n",
      "Epoch 312/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 21097686.0000 - mae: 2651.2312 - val_loss: 22742486.0000 - val_mae: 3485.1833\n",
      "Epoch 313/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 20966320.0000 - mae: 2645.0549 - val_loss: 22734756.0000 - val_mae: 3483.7598\n",
      "Epoch 314/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 20827434.0000 - mae: 2640.1279 - val_loss: 22727592.0000 - val_mae: 3482.4053\n",
      "Epoch 315/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 20658582.0000 - mae: 2632.9211 - val_loss: 22721066.0000 - val_mae: 3481.1362\n",
      "Epoch 316/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 20528782.0000 - mae: 2625.6733 - val_loss: 22715024.0000 - val_mae: 3479.9568\n",
      "Epoch 317/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 20378088.0000 - mae: 2620.2974 - val_loss: 22708648.0000 - val_mae: 3478.7134\n",
      "Epoch 318/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 20245504.0000 - mae: 2613.1826 - val_loss: 22702206.0000 - val_mae: 3477.4688\n",
      "Epoch 319/500\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 20116814.0000 - mae: 2607.9048 - val_loss: 22696016.0000 - val_mae: 3476.2683\n",
      "Epoch 320/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 19961344.0000 - mae: 2600.5728 - val_loss: 22690326.0000 - val_mae: 3475.1299\n",
      "Epoch 321/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 19836666.0000 - mae: 2595.1816 - val_loss: 22684962.0000 - val_mae: 3474.0181\n",
      "Epoch 322/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 19715956.0000 - mae: 2589.4858 - val_loss: 22678266.0000 - val_mae: 3472.7444\n",
      "Epoch 323/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 19587952.0000 - mae: 2583.1147 - val_loss: 22673336.0000 - val_mae: 3471.7026\n",
      "Epoch 324/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 19460746.0000 - mae: 2577.4756 - val_loss: 22668956.0000 - val_mae: 3470.7522\n",
      "Epoch 325/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 19321114.0000 - mae: 2570.8516 - val_loss: 22665048.0000 - val_mae: 3469.8755\n",
      "Epoch 326/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 19189430.0000 - mae: 2564.3440 - val_loss: 22660658.0000 - val_mae: 3468.9626\n",
      "Epoch 327/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 19083250.0000 - mae: 2558.8755 - val_loss: 22653702.0000 - val_mae: 3467.6650\n",
      "Epoch 328/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 18960248.0000 - mae: 2553.2720 - val_loss: 22647684.0000 - val_mae: 3466.4919\n",
      "Epoch 329/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 18864586.0000 - mae: 2549.0825 - val_loss: 22641950.0000 - val_mae: 3465.3804\n",
      "Epoch 330/500\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 18729414.0000 - mae: 2542.5139 - val_loss: 22638420.0000 - val_mae: 3464.5562\n",
      "Epoch 331/500\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 18603454.0000 - mae: 2536.3821 - val_loss: 22635010.0000 - val_mae: 3463.7214\n",
      "Epoch 332/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 18490880.0000 - mae: 2530.4419 - val_loss: 22630554.0000 - val_mae: 3462.7673\n",
      "Epoch 333/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 18385230.0000 - mae: 2524.9148 - val_loss: 22624786.0000 - val_mae: 3461.6116\n",
      "Epoch 334/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 18289248.0000 - mae: 2520.8457 - val_loss: 22619862.0000 - val_mae: 3460.5947\n",
      "Epoch 335/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 18174242.0000 - mae: 2514.4077 - val_loss: 22616660.0000 - val_mae: 3459.8416\n",
      "Epoch 336/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 18049558.0000 - mae: 2508.3047 - val_loss: 22614094.0000 - val_mae: 3459.1697\n",
      "Epoch 337/500\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 17960476.0000 - mae: 2503.4233 - val_loss: 22610518.0000 - val_mae: 3458.3169\n",
      "Epoch 338/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 17840804.0000 - mae: 2496.5085 - val_loss: 22608868.0000 - val_mae: 3457.7554\n",
      "Epoch 339/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 17754002.0000 - mae: 2492.3120 - val_loss: 22607470.0000 - val_mae: 3457.1709\n",
      "Epoch 340/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 17629236.0000 - mae: 2485.1194 - val_loss: 22604906.0000 - val_mae: 3456.4365\n",
      "Epoch 341/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 17546454.0000 - mae: 2481.1521 - val_loss: 22600316.0000 - val_mae: 3455.4138\n",
      "Epoch 342/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 17425008.0000 - mae: 2474.7156 - val_loss: 22596610.0000 - val_mae: 3454.5503\n",
      "Epoch 343/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 17349842.0000 - mae: 2471.2087 - val_loss: 22594180.0000 - val_mae: 3453.8154\n",
      "Epoch 344/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 17242390.0000 - mae: 2465.5908 - val_loss: 22590750.0000 - val_mae: 3452.9927\n",
      "Epoch 345/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 17135122.0000 - mae: 2460.2310 - val_loss: 22589828.0000 - val_mae: 3452.4873\n",
      "Epoch 346/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 17036094.0000 - mae: 2455.2168 - val_loss: 22587826.0000 - val_mae: 3451.8396\n",
      "Epoch 347/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 16945768.0000 - mae: 2450.6128 - val_loss: 22584570.0000 - val_mae: 3450.9875\n",
      "Epoch 348/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 16848166.0000 - mae: 2445.0510 - val_loss: 22581708.0000 - val_mae: 3450.1812\n",
      "Epoch 349/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 16760115.0000 - mae: 2440.4080 - val_loss: 22579010.0000 - val_mae: 3449.3596\n",
      "Epoch 350/500\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 16658169.0000 - mae: 2434.4209 - val_loss: 22575780.0000 - val_mae: 3448.5015\n",
      "Epoch 351/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 16590405.0000 - mae: 2430.4382 - val_loss: 22570874.0000 - val_mae: 3447.4133\n",
      "Epoch 352/500\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 16481670.0000 - mae: 2425.1428 - val_loss: 22568614.0000 - val_mae: 3446.7092\n",
      "Epoch 353/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 16396651.0000 - mae: 2420.5481 - val_loss: 22566300.0000 - val_mae: 3445.9517\n",
      "Epoch 354/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 16324310.0000 - mae: 2416.4973 - val_loss: 22564374.0000 - val_mae: 3445.2688\n",
      "Epoch 355/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 16225792.0000 - mae: 2410.5142 - val_loss: 22562052.0000 - val_mae: 3444.5374\n",
      "Epoch 356/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 16137048.0000 - mae: 2405.6729 - val_loss: 22559610.0000 - val_mae: 3443.7769\n",
      "Epoch 357/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 16047399.0000 - mae: 2400.9141 - val_loss: 22557350.0000 - val_mae: 3443.0774\n",
      "Epoch 358/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 15966620.0000 - mae: 2396.2827 - val_loss: 22555962.0000 - val_mae: 3442.4854\n",
      "Epoch 359/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 15878583.0000 - mae: 2390.7107 - val_loss: 22554808.0000 - val_mae: 3441.9253\n",
      "Epoch 360/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 15801433.0000 - mae: 2385.4736 - val_loss: 22553210.0000 - val_mae: 3441.2910\n",
      "Epoch 361/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 15712754.0000 - mae: 2380.3652 - val_loss: 22552814.0000 - val_mae: 3440.8259\n",
      "Epoch 362/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 15646613.0000 - mae: 2375.9473 - val_loss: 22550726.0000 - val_mae: 3440.0947\n",
      "Epoch 363/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 15570169.0000 - mae: 2371.5552 - val_loss: 22548952.0000 - val_mae: 3439.3748\n",
      "Epoch 364/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 15485992.0000 - mae: 2366.2124 - val_loss: 22548018.0000 - val_mae: 3438.8152\n",
      "Epoch 365/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 15404572.0000 - mae: 2361.3457 - val_loss: 22548366.0000 - val_mae: 3438.4055\n",
      "Epoch 366/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 15333461.0000 - mae: 2355.8225 - val_loss: 22547026.0000 - val_mae: 3437.7749\n",
      "Epoch 367/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 15249010.0000 - mae: 2351.1709 - val_loss: 22546304.0000 - val_mae: 3437.2676\n",
      "Epoch 368/500\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 15183104.0000 - mae: 2346.7429 - val_loss: 22543444.0000 - val_mae: 3436.4402\n",
      "Epoch 369/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 15098345.0000 - mae: 2342.9265 - val_loss: 22542708.0000 - val_mae: 3435.9019\n",
      "Epoch 370/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 15043834.0000 - mae: 2339.4756 - val_loss: 22543188.0000 - val_mae: 3435.5310\n",
      "Epoch 371/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 14957592.0000 - mae: 2334.2859 - val_loss: 22544062.0000 - val_mae: 3435.2458\n",
      "Epoch 372/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 14877119.0000 - mae: 2330.1921 - val_loss: 22542776.0000 - val_mae: 3434.7241\n",
      "Epoch 373/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 14819145.0000 - mae: 2327.5967 - val_loss: 22539950.0000 - val_mae: 3433.9365\n",
      "Epoch 374/500\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 14739326.0000 - mae: 2322.6086 - val_loss: 22538944.0000 - val_mae: 3433.3691\n",
      "Epoch 375/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 14659473.0000 - mae: 2318.3201 - val_loss: 22539182.0000 - val_mae: 3432.9697\n",
      "Epoch 376/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 14598448.0000 - mae: 2314.4426 - val_loss: 22539042.0000 - val_mae: 3432.5049\n",
      "Epoch 377/500\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 14541170.0000 - mae: 2310.6777 - val_loss: 22537634.0000 - val_mae: 3431.9331\n",
      "Epoch 378/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 14455300.0000 - mae: 2305.8013 - val_loss: 22536594.0000 - val_mae: 3431.4058\n",
      "Epoch 379/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 14394168.0000 - mae: 2302.7173 - val_loss: 22535014.0000 - val_mae: 3430.7979\n",
      "Epoch 380/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 14327782.0000 - mae: 2298.3367 - val_loss: 22533606.0000 - val_mae: 3430.2441\n",
      "Epoch 381/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 14269309.0000 - mae: 2294.6250 - val_loss: 22532524.0000 - val_mae: 3429.6987\n",
      "Epoch 382/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 14193208.0000 - mae: 2290.6846 - val_loss: 22532416.0000 - val_mae: 3429.2771\n",
      "Epoch 383/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 14137484.0000 - mae: 2286.9331 - val_loss: 22531576.0000 - val_mae: 3428.7405\n",
      "Epoch 384/500\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 14061961.0000 - mae: 2282.1704 - val_loss: 22530564.0000 - val_mae: 3428.2061\n",
      "Epoch 385/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 14014367.0000 - mae: 2278.6270 - val_loss: 22528956.0000 - val_mae: 3427.6047\n",
      "Epoch 386/500\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 13942236.0000 - mae: 2274.4465 - val_loss: 22528474.0000 - val_mae: 3427.1199\n",
      "Epoch 387/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 13872578.0000 - mae: 2270.0681 - val_loss: 22528092.0000 - val_mae: 3426.7400\n",
      "Epoch 388/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 13834404.0000 - mae: 2267.0342 - val_loss: 22523858.0000 - val_mae: 3426.0129\n",
      "Epoch 389/500\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 13756570.0000 - mae: 2261.9658 - val_loss: 22520784.0000 - val_mae: 3425.4888\n",
      "Epoch 390/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 13708533.0000 - mae: 2258.7388 - val_loss: 22517170.0000 - val_mae: 3424.8550\n",
      "Epoch 391/500\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 13648207.0000 - mae: 2253.4348 - val_loss: 22513980.0000 - val_mae: 3424.3052\n",
      "Epoch 392/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 13582328.0000 - mae: 2249.3818 - val_loss: 22509710.0000 - val_mae: 3423.6870\n",
      "Epoch 393/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 13525989.0000 - mae: 2245.5454 - val_loss: 22507110.0000 - val_mae: 3423.2146\n",
      "Epoch 394/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 13471884.0000 - mae: 2241.7744 - val_loss: 22503316.0000 - val_mae: 3422.5830\n",
      "Epoch 395/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 13426370.0000 - mae: 2238.7620 - val_loss: 22499376.0000 - val_mae: 3421.9314\n",
      "Epoch 396/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 13360391.0000 - mae: 2233.8250 - val_loss: 22497340.0000 - val_mae: 3421.4612\n",
      "Epoch 397/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 13305947.0000 - mae: 2230.4114 - val_loss: 22494652.0000 - val_mae: 3421.0115\n",
      "Epoch 398/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 13264796.0000 - mae: 2227.3567 - val_loss: 22491428.0000 - val_mae: 3420.6057\n",
      "Epoch 399/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 13201982.0000 - mae: 2222.7258 - val_loss: 22488408.0000 - val_mae: 3420.1687\n",
      "Epoch 400/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 13146773.0000 - mae: 2218.7585 - val_loss: 22486658.0000 - val_mae: 3419.9268\n",
      "Epoch 401/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 13089516.0000 - mae: 2214.2883 - val_loss: 22483080.0000 - val_mae: 3419.4736\n",
      "Epoch 402/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 13045269.0000 - mae: 2210.7341 - val_loss: 22477826.0000 - val_mae: 3418.7666\n",
      "Epoch 403/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 13004424.0000 - mae: 2208.3662 - val_loss: 22473412.0000 - val_mae: 3418.1487\n",
      "Epoch 404/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 12941083.0000 - mae: 2203.4043 - val_loss: 22470822.0000 - val_mae: 3417.8000\n",
      "Epoch 405/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 12899698.0000 - mae: 2199.7588 - val_loss: 22467732.0000 - val_mae: 3417.3872\n",
      "Epoch 406/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 12849188.0000 - mae: 2196.0227 - val_loss: 22464340.0000 - val_mae: 3416.9377\n",
      "Epoch 407/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 12798969.0000 - mae: 2192.6687 - val_loss: 22462470.0000 - val_mae: 3416.6794\n",
      "Epoch 408/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 12752610.0000 - mae: 2188.4639 - val_loss: 22459364.0000 - val_mae: 3416.2686\n",
      "Epoch 409/500\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 12700240.0000 - mae: 2184.3674 - val_loss: 22457174.0000 - val_mae: 3415.9333\n",
      "Epoch 410/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 12657735.0000 - mae: 2181.7566 - val_loss: 22454438.0000 - val_mae: 3415.4841\n",
      "Epoch 411/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 12613784.0000 - mae: 2178.1860 - val_loss: 22450952.0000 - val_mae: 3414.9768\n",
      "Epoch 412/500\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 12561733.0000 - mae: 2174.7014 - val_loss: 22449072.0000 - val_mae: 3414.7031\n",
      "Epoch 413/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 12518863.0000 - mae: 2172.2351 - val_loss: 22445312.0000 - val_mae: 3414.2415\n",
      "Epoch 414/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 12481685.0000 - mae: 2168.7693 - val_loss: 22442838.0000 - val_mae: 3413.8994\n",
      "Epoch 415/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 12430521.0000 - mae: 2165.4448 - val_loss: 22439744.0000 - val_mae: 3413.4375\n",
      "Epoch 416/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 12403049.0000 - mae: 2163.5867 - val_loss: 22437682.0000 - val_mae: 3413.0496\n",
      "Epoch 417/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 12343456.0000 - mae: 2159.2241 - val_loss: 22435782.0000 - val_mae: 3412.7017\n",
      "Epoch 418/500\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 12303509.0000 - mae: 2156.9971 - val_loss: 22433928.0000 - val_mae: 3412.3701\n",
      "Epoch 419/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 12265563.0000 - mae: 2153.7815 - val_loss: 22432670.0000 - val_mae: 3412.1331\n",
      "Epoch 420/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 12217560.0000 - mae: 2149.9722 - val_loss: 22430956.0000 - val_mae: 3411.8342\n",
      "Epoch 421/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 12182550.0000 - mae: 2147.6782 - val_loss: 22428692.0000 - val_mae: 3411.3733\n",
      "Epoch 422/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 12135780.0000 - mae: 2144.1184 - val_loss: 22425906.0000 - val_mae: 3410.8799\n",
      "Epoch 423/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 12109353.0000 - mae: 2142.2876 - val_loss: 22423166.0000 - val_mae: 3410.3740\n",
      "Epoch 424/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 12059266.0000 - mae: 2138.6704 - val_loss: 22422004.0000 - val_mae: 3410.1094\n",
      "Epoch 425/500\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 12013548.0000 - mae: 2135.3735 - val_loss: 22421520.0000 - val_mae: 3409.9277\n",
      "Epoch 426/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 11990713.0000 - mae: 2133.7678 - val_loss: 22419994.0000 - val_mae: 3409.5657\n",
      "Epoch 427/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 11947455.0000 - mae: 2129.8552 - val_loss: 22419726.0000 - val_mae: 3409.3428\n",
      "Epoch 428/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 11908737.0000 - mae: 2126.7439 - val_loss: 22419480.0000 - val_mae: 3409.0803\n",
      "Epoch 429/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 11865346.0000 - mae: 2122.9504 - val_loss: 22417792.0000 - val_mae: 3408.7419\n",
      "Epoch 430/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 11836516.0000 - mae: 2121.1582 - val_loss: 22415788.0000 - val_mae: 3408.3411\n",
      "Epoch 431/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 11790599.0000 - mae: 2117.8267 - val_loss: 22414080.0000 - val_mae: 3408.0100\n",
      "Epoch 432/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 11754541.0000 - mae: 2114.7729 - val_loss: 22412274.0000 - val_mae: 3407.6372\n",
      "Epoch 433/500\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 11718963.0000 - mae: 2111.9736 - val_loss: 22411132.0000 - val_mae: 3407.3616\n",
      "Epoch 434/500\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 11689531.0000 - mae: 2109.6375 - val_loss: 22410100.0000 - val_mae: 3407.1335\n",
      "Epoch 435/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 11652864.0000 - mae: 2106.4377 - val_loss: 22409814.0000 - val_mae: 3406.9807\n",
      "Epoch 436/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 11616813.0000 - mae: 2103.7085 - val_loss: 22410344.0000 - val_mae: 3406.9143\n",
      "Epoch 437/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 11589283.0000 - mae: 2101.0168 - val_loss: 22409036.0000 - val_mae: 3406.5330\n",
      "Epoch 438/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 11548378.0000 - mae: 2097.8433 - val_loss: 22405872.0000 - val_mae: 3405.9319\n",
      "Epoch 439/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 11511939.0000 - mae: 2095.5193 - val_loss: 22404476.0000 - val_mae: 3405.5696\n",
      "Epoch 440/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 11478136.0000 - mae: 2092.8650 - val_loss: 22404440.0000 - val_mae: 3405.3750\n",
      "Epoch 441/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 11448478.0000 - mae: 2089.9639 - val_loss: 22404754.0000 - val_mae: 3405.2310\n",
      "Epoch 442/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 11413752.0000 - mae: 2087.0645 - val_loss: 22403112.0000 - val_mae: 3404.7942\n",
      "Epoch 443/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 11385725.0000 - mae: 2084.8398 - val_loss: 22401500.0000 - val_mae: 3404.4763\n",
      "Epoch 444/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 11358918.0000 - mae: 2082.9324 - val_loss: 22399966.0000 - val_mae: 3404.0688\n",
      "Epoch 445/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 11316349.0000 - mae: 2079.9346 - val_loss: 22400292.0000 - val_mae: 3403.8860\n",
      "Epoch 446/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 11291895.0000 - mae: 2077.4141 - val_loss: 22401904.0000 - val_mae: 3403.8264\n",
      "Epoch 447/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 11257897.0000 - mae: 2073.8474 - val_loss: 22403074.0000 - val_mae: 3403.7424\n",
      "Epoch 448/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 11234863.0000 - mae: 2071.9648 - val_loss: 22400468.0000 - val_mae: 3403.2217\n",
      "Epoch 449/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 11196404.0000 - mae: 2069.6602 - val_loss: 22399760.0000 - val_mae: 3402.9768\n",
      "Epoch 450/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 11170520.0000 - mae: 2067.6079 - val_loss: 22399932.0000 - val_mae: 3402.8572\n",
      "Epoch 451/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 11137716.0000 - mae: 2064.7102 - val_loss: 22401278.0000 - val_mae: 3402.7898\n",
      "Epoch 452/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 11108639.0000 - mae: 2062.0093 - val_loss: 22401868.0000 - val_mae: 3402.5869\n",
      "Epoch 453/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 11081437.0000 - mae: 2059.6746 - val_loss: 22401700.0000 - val_mae: 3402.3181\n",
      "Epoch 454/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 11059795.0000 - mae: 2057.5801 - val_loss: 22401496.0000 - val_mae: 3402.1140\n",
      "Epoch 455/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 11028194.0000 - mae: 2055.5447 - val_loss: 22400556.0000 - val_mae: 3401.8306\n",
      "Epoch 456/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 10998771.0000 - mae: 2053.4797 - val_loss: 22400106.0000 - val_mae: 3401.5803\n",
      "Epoch 457/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 10975248.0000 - mae: 2051.1011 - val_loss: 22401210.0000 - val_mae: 3401.5466\n",
      "Epoch 458/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 10950515.0000 - mae: 2048.5186 - val_loss: 22404284.0000 - val_mae: 3401.7429\n",
      "Epoch 459/500\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 10916837.0000 - mae: 2045.0658 - val_loss: 22405794.0000 - val_mae: 3401.7866\n",
      "Epoch 460/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 10891213.0000 - mae: 2042.4711 - val_loss: 22405186.0000 - val_mae: 3401.4827\n",
      "Epoch 461/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 10873050.0000 - mae: 2041.4896 - val_loss: 22403572.0000 - val_mae: 3401.1101\n",
      "Epoch 462/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 10845121.0000 - mae: 2039.3367 - val_loss: 22405060.0000 - val_mae: 3401.1062\n",
      "Epoch 463/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 10817790.0000 - mae: 2036.6632 - val_loss: 22406342.0000 - val_mae: 3401.1001\n",
      "Epoch 464/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 10791017.0000 - mae: 2034.0935 - val_loss: 22408670.0000 - val_mae: 3401.2144\n",
      "Epoch 465/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 10769197.0000 - mae: 2031.3250 - val_loss: 22411492.0000 - val_mae: 3401.4058\n",
      "Epoch 466/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 10746108.0000 - mae: 2029.0863 - val_loss: 22410194.0000 - val_mae: 3401.1182\n",
      "Epoch 467/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 10718198.0000 - mae: 2026.5155 - val_loss: 22410628.0000 - val_mae: 3401.0854\n",
      "Epoch 468/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 10700142.0000 - mae: 2025.0359 - val_loss: 22410386.0000 - val_mae: 3400.8237\n",
      "Epoch 469/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 10665915.0000 - mae: 2022.2334 - val_loss: 22413508.0000 - val_mae: 3401.0000\n",
      "Epoch 470/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 10646583.0000 - mae: 2019.7108 - val_loss: 22416258.0000 - val_mae: 3401.1387\n",
      "Epoch 471/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 10621083.0000 - mae: 2017.1598 - val_loss: 22419462.0000 - val_mae: 3401.3218\n",
      "Epoch 472/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 10604018.0000 - mae: 2015.0155 - val_loss: 22422504.0000 - val_mae: 3401.5569\n",
      "Epoch 473/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 10580297.0000 - mae: 2012.6334 - val_loss: 22424048.0000 - val_mae: 3401.6257\n",
      "Epoch 474/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 10555517.0000 - mae: 2010.5093 - val_loss: 22426036.0000 - val_mae: 3401.7056\n",
      "Epoch 475/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 10531846.0000 - mae: 2008.0875 - val_loss: 22424596.0000 - val_mae: 3401.4221\n",
      "Epoch 476/500\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 10514454.0000 - mae: 2006.5869 - val_loss: 22422818.0000 - val_mae: 3401.1174\n",
      "Epoch 477/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 10486726.0000 - mae: 2004.9244 - val_loss: 22424094.0000 - val_mae: 3401.0889\n",
      "Epoch 478/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 10468649.0000 - mae: 2002.9229 - val_loss: 22425924.0000 - val_mae: 3401.1665\n",
      "Epoch 479/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 10450313.0000 - mae: 2000.9498 - val_loss: 22426864.0000 - val_mae: 3401.1504\n",
      "Epoch 480/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 10432296.0000 - mae: 1998.7733 - val_loss: 22430322.0000 - val_mae: 3401.5317\n",
      "Epoch 481/500\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 10402836.0000 - mae: 1996.1901 - val_loss: 22430552.0000 - val_mae: 3401.4712\n",
      "Epoch 482/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 10380408.0000 - mae: 1994.5149 - val_loss: 22429722.0000 - val_mae: 3401.2847\n",
      "Epoch 483/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 10362505.0000 - mae: 1993.0013 - val_loss: 22429462.0000 - val_mae: 3401.1606\n",
      "Epoch 484/500\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 10340102.0000 - mae: 1991.4652 - val_loss: 22429252.0000 - val_mae: 3401.0415\n",
      "Epoch 485/500\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 10322776.0000 - mae: 1989.6361 - val_loss: 22429162.0000 - val_mae: 3400.9143\n",
      "Epoch 486/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 10301676.0000 - mae: 1987.7992 - val_loss: 22430138.0000 - val_mae: 3400.9512\n",
      "Epoch 487/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 10284118.0000 - mae: 1986.0802 - val_loss: 22431098.0000 - val_mae: 3400.9041\n",
      "Epoch 488/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 10263797.0000 - mae: 1984.3169 - val_loss: 22431774.0000 - val_mae: 3400.9133\n",
      "Epoch 489/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 10243134.0000 - mae: 1982.1272 - val_loss: 22432936.0000 - val_mae: 3400.9377\n",
      "Epoch 490/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 10221633.0000 - mae: 1980.0341 - val_loss: 22433844.0000 - val_mae: 3400.9485\n",
      "Epoch 491/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 10210460.0000 - mae: 1978.5289 - val_loss: 22434004.0000 - val_mae: 3400.8135\n",
      "Epoch 492/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 10190166.0000 - mae: 1977.3422 - val_loss: 22434228.0000 - val_mae: 3400.6719\n",
      "Epoch 493/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 10166005.0000 - mae: 1975.5707 - val_loss: 22435298.0000 - val_mae: 3400.7205\n",
      "Epoch 494/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 10148616.0000 - mae: 1973.7002 - val_loss: 22436302.0000 - val_mae: 3400.7559\n",
      "Epoch 495/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 10132563.0000 - mae: 1972.0688 - val_loss: 22436644.0000 - val_mae: 3400.7339\n",
      "Epoch 496/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 10118066.0000 - mae: 1971.8027 - val_loss: 22436540.0000 - val_mae: 3400.6223\n",
      "Epoch 497/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 10098365.0000 - mae: 1970.0813 - val_loss: 22437644.0000 - val_mae: 3400.6958\n",
      "Epoch 498/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 10077948.0000 - mae: 1968.3989 - val_loss: 22438528.0000 - val_mae: 3400.7727\n",
      "Epoch 499/500\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 10059676.0000 - mae: 1966.6820 - val_loss: 22440126.0000 - val_mae: 3400.9780\n",
      "Epoch 500/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 10050768.0000 - mae: 1966.0305 - val_loss: 22438294.0000 - val_mae: 3400.8572\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n"
     ]
    }
   ],
   "source": [
    "# scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train_nn = scaler.fit_transform(X_train_nn)\n",
    "X_test_nn = scaler.transform(X_test_nn)\n",
    "\n",
    "# fit the model on the X_train and y_train\n",
    "model = Sequential()\n",
    "# model.add(Dense(256, activation=\"relu\", input_dim=X_train_nn.shape[1]))  # First layer\n",
    "# model.add(Dense(128, activation=\"relu\"))  # First layer\n",
    "# model.add(Dense(64, activation=\"relu\"))  # First layer\n",
    "model.add(Dense(32, activation=\"relu\", input_dim=X_train_nn.shape[1]))  # Fourth layer\n",
    "model.add(Dense(16, activation=\"relu\"))  # New additional layer\n",
    "model.add(Dense(8, activation=\"relu\"))  # New additional layer\n",
    "model.add(Dense(1, activation = 'linear')) \n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"mean_squared_error\", metrics=[\"mae\"])\n",
    "\n",
    "\n",
    "model.fit(X_train_nn, y_train_nn, epochs=500, batch_size=64, validation_split=0.2)\n",
    "\n",
    "# # get the predictions\n",
    "# boolean_columns = [col for col in X_test_nn.columns if X_test_nn[col].dtype == bool]\n",
    "# for col in boolean_columns:\n",
    "#     X_test_nn[col] = X_test_nn[col].astype(int)\n",
    "\n",
    "y_pred = model.predict(X_test_nn).flatten()\n",
    "\n",
    "\n",
    "# # get the predictions\n",
    "# boolean_columns = [col for col in X_test_nn.columns if X_test_nn[col].dtype == bool]\n",
    "# for col in boolean_columns:\n",
    "#     X_test_nn[col] = X_test_nn[col].astype(int)\n",
    "\n",
    "y_hat = model.predict(X_test_nn).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11324.204  ,  5330.855  ,  7167.6265 ,  7393.0444 ,  3149.7878 ,\n",
       "        8194.65   , 23398.646  ,  1748.6278 ,    82.85541,  2298.103  ,\n",
       "        7988.1724 , 12099.55   ,  1951.3784 ,   305.72742, 12493.538  ,\n",
       "        2007.7446 ], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_data_diff[\"gdp_pred\"] = y_hat\n",
    "poland_pred = pred_data_diff[[\"year\", \"region\", \"gdp_pred\", \"real_gdp\"]]\n",
    "poland_pred.to_csv(\"poland_predictions_new_2.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
