{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import rasterio\n",
    "from rasterio.mask import mask\n",
    "from rasterio.transform import from_origin\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd \n",
    "import pandas as pd\n",
    "from dnb_annual import *\n",
    "from variables import years, composites, region_map, region_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jakub\\anaconda\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\jakub\\anaconda\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Resizing, Dropout, BatchNormalization, Activation, Add, GlobalAveragePooling2D, Input, Reshape, Conv2DTranspose, Cropping2D\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this script is used only once to generate the regional images for each year\n",
    "# country_polygons = gpd.read_file(\"geoBoundaries-UKR-ADM1.geojson\")\n",
    "# years = [2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023]\n",
    "# for year in years:\n",
    "#     dnb = dnb_annual(year, composites, country_polygons)\n",
    "#     dnb.load_all_data()\n",
    "#     dnb.filter_data()\n",
    "#     dnb.save_rasters()\n",
    "#     dnb.load_rasters()\n",
    "#     dnb.build_regional_images()\n",
    "#     dnb.add_padding()\n",
    "#     dnb.save_regional_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this script is used to clean gdp data\n",
    "\n",
    "# Inflation data\n",
    "# inflation = pd.read_excel(\"data/isc_reg.xls\", skiprows=2, header=1)\n",
    "# inflation = inflation.drop(columns=inflation.columns[0])\n",
    "# inflation = inflation.rename(columns={inflation.columns[-1]: \"region\"})\n",
    "# inflation = inflation[~inflation[\"region\"].isin([\"Ukraine\", \"oblasts\"])]\n",
    "# inflation = inflation.dropna()\n",
    "# inflation[\"region\"] = inflation[\"region\"].map(region_map)\n",
    "# inflation.columns = inflation.columns.astype(str)\n",
    "# inflation = inflation.melt(id_vars=\"region\", var_name=\"year\", value_name=\"inflation\")\n",
    "# inflation.to_csv(\"data/inflation.csv\", index=False)\n",
    "\n",
    "# GDP data\n",
    "# gdp = pd.read_excel(\"data/ukr_reg_gdp.xls\", skiprows=3, header=1)\n",
    "# gdp = gdp.drop(columns=gdp.columns[0])\n",
    "# gdp = gdp.iloc[:, np.r_[18:36, -1]]\n",
    "# gdp = gdp.rename(columns={gdp.columns[-1]: \"region\"})\n",
    "# gdp = gdp[~gdp[\"region\"].isin([\"Ukrane\", \"oblasts\"])]\n",
    "# gdp = gdp.dropna()\n",
    "# gdp[\"region\"] = gdp[\"region\"].map(region_map)\n",
    "# gdp[\"region\"] = gdp[\"region\"].fillna(\"Sevastopol\")\n",
    "# gdp.columns = gdp.columns.astype(str)\n",
    "# gdp = gdp.rename(columns={gdp.columns[i]: gdp.columns[i][:4] for i in range(18)})\n",
    "# gdp = gdp.melt(id_vars=\"region\", var_name=\"year\", value_name=\"real_gdp_change\")\n",
    "\n",
    "# # include only years from 2012 inclusive, exclude Sevastopol and the Autonomous Republic of Crimea\n",
    "# gdp = gdp[gdp[\"year\"].astype(int) >= 2012]\n",
    "# gdp = gdp[~gdp[\"region\"].isin([\"Sevastopol\", \"Autonomous Republic of Crimea\"])]\n",
    "\n",
    "# # set the value for the starting year to 100 (2012), NaN for the rest\n",
    "# gdp.loc[gdp[\"year\"] == \"2012\", \"real_gdp\"] = 100\n",
    "# gdp = gdp.sort_values(by=[\"region\", \"year\"])\n",
    "# gdp[\"real_gdp_change\"] = gdp[\"real_gdp_change\"] / 100\n",
    "\n",
    "# # reste the index\n",
    "# gdp = gdp.reset_index(drop=True)\n",
    "\n",
    "# # # calculate the real gdp\n",
    "# for i in range(1, gdp.shape[0]):\n",
    "\n",
    "#     # skip if the year is 2012\n",
    "#     if gdp.loc[gdp.index[i], \"year\"] == \"2012\":\n",
    "#         continue\n",
    "#     else:\n",
    "#         gdp.loc[gdp.index[i], \"real_gdp\"] = gdp.loc[gdp.index[i-1], \"real_gdp\"] * (gdp.loc[gdp.index[i], \"real_gdp_change\"])\n",
    "\n",
    "# # delete the real_gdp_change column\n",
    "# gdp = gdp.drop(columns=\"real_gdp_change\")\n",
    "\n",
    "# # get the nominal gdp\n",
    "# gdp_nominal = pd.read_excel(\"data/ukr_reg_gdp.xls\", skiprows=3, header=1)\n",
    "# gdp_nominal = gdp_nominal.iloc[:, np.r_[9, -1]]\n",
    "# gdp_nominal.columns = [\"gdp_nominal\", \"region\"]\n",
    "# gdp_nominal = gdp_nominal[~gdp_nominal[\"region\"].isin([\"Ukrane\", \"oblasts\"])]\n",
    "# gdp_nominal = gdp_nominal.dropna()\n",
    "# gdp_nominal[\"region\"] = gdp_nominal[\"region\"].map(region_map)\n",
    "# gdp_nominal[\"region\"] = gdp_nominal[\"region\"].fillna(\"Sevastopol\")\n",
    "\n",
    "# # merge nominal gdp to real gdp by region\n",
    "# gdp = gdp.merge(gdp_nominal, on=\"region\")\n",
    "\n",
    "# # multiple the real gdp by the nominal gdp\n",
    "# gdp[\"real_gdp\"] = gdp[\"real_gdp\"] * gdp[\"gdp_nominal\"]\n",
    "\n",
    "# # drop the nominal gdp column\n",
    "# gdp = gdp.drop(columns=\"gdp_nominal\")\n",
    "\n",
    "# # for the region column, change all spaces to _\n",
    "# gdp[\"region\"] = gdp[\"region\"].str.replace(\" \", \"_\")\n",
    "\n",
    "# # save the data\n",
    "# gdp.to_csv(\"data/clean_ukr_gdp.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare gdp data, Ukraine\n",
    "gdp_ukr = pd.read_excel(\"data/ukr_reg_gdp.xls\", skiprows=3, header=1)\n",
    "gdp_ukr = gdp_ukr.drop(columns=gdp_ukr.columns[0])\n",
    "gdp_ukr = gdp_ukr.iloc[:, np.r_[18:36, -1]]\n",
    "gdp_ukr = gdp_ukr.rename(columns={gdp_ukr.columns[-1]: \"region\"})\n",
    "gdp_ukr = gdp_ukr[~gdp_ukr[\"region\"].isin([\"Ukrane\", \"oblasts\"])]\n",
    "gdp_ukr = gdp_ukr.dropna()\n",
    "gdp_ukr[\"region\"] = gdp_ukr[\"region\"].map(region_map)\n",
    "gdp_ukr[\"region\"] = gdp_ukr[\"region\"].fillna(\"Sevastopol\")\n",
    "gdp_ukr.columns = gdp_ukr.columns.astype(str)\n",
    "gdp_ukr = gdp_ukr.rename(columns={gdp_ukr.columns[i]: gdp_ukr.columns[i][:4] for i in range(18)})\n",
    "gdp_ukr = gdp_ukr[~gdp_ukr[\"region\"].isin([\"Sevastopol\", \"Autonomous Republic of Crimea\"])]\n",
    "\n",
    "# gdp_ukr_nominal = pd.read_excel(\"data/ukr_reg_gdp.xls\", skiprows=3, header=1)\n",
    "# gdp_ukr = gdp_ukr.drop(columns=gdp_ukr.columns[0])\n",
    "# gdp_ukr = gdp_ukr.iloc[:, np.r_[18:36, -1]]\n",
    "\n",
    "# select years from 2012 inclusive and the region column\n",
    "gdp_ukr = gdp_ukr[[gdp_ukr.columns[i] for i in range(18) if int(gdp_ukr.columns[i]) >= 2012] + [\"region\"]]\n",
    "\n",
    "# Poland\n",
    "gdp_pol = pd.read_excel(\"data/pol_reg_gdp.xlsx\", header=1, sheet_name=1)\n",
    "gdp_pol = gdp_pol.drop(columns=gdp_pol.columns[0])\n",
    "gdp_pol = gdp_pol.drop(0)\n",
    "gdp_pol = gdp_pol.rename(columns={gdp_pol.columns[0]: \"region\"})\n",
    "# gdp_pol = gdp_pol.drop(columns=\"2022\")\n",
    "\n",
    "def clean_gdp_data(data, country):\n",
    "\n",
    "    # set the 2012 first column to 100\n",
    "    data[\"2012\"] = 100\n",
    "\n",
    "    # divide columns from 2013 to 2021 by 100\n",
    "    for year in range(2013, 2022):\n",
    "        data[str(year)] = data[str(year)] / 100\n",
    "\n",
    "    # calculate the real gdp\n",
    "    for i in range(2013, 2022):\n",
    "        data[str(i)] = data[str(i)] * data[str(i-1)]\n",
    "\n",
    "    # add a column 2022 and 2023 with NaN\n",
    "    if country == \"ukr\":\n",
    "        data[\"2022\"] = np.nan\n",
    "        data[\"2023\"] = np.nan\n",
    "\n",
    "    # format to long\n",
    "    data = data.melt(id_vars=\"region\", var_name=\"year\", value_name=\"real_gdp\")\n",
    "    data[\"region\"] = data[\"region\"].str.replace(\" \", \"_\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "gdp_pol = clean_gdp_data(gdp_pol, country = \"pol\")\n",
    "gdp_ukr = clean_gdp_data(gdp_ukr, country = \"ukr\")\n",
    "\n",
    "# read nominal gdp data for Ukraine\n",
    "gdp_ukr_nominal = pd.read_excel(\"data/ukr_reg_gdp.xls\", skiprows=3, header=1)\n",
    "gdp_ukr_nominal = gdp_ukr_nominal.drop(columns=gdp_ukr_nominal.columns[0])\n",
    "gdp_ukr_nominal = gdp_ukr_nominal.iloc[:, np.r_[8, -1]]\n",
    "\n",
    "gdp_ukr_nominal = gdp_ukr_nominal.rename(columns={gdp_ukr_nominal.columns[-1]: \"region\"})\n",
    "gdp_ukr_nominal = gdp_ukr_nominal[~gdp_ukr_nominal[\"region\"].isin([\"Ukrane\", \"oblasts\"])]\n",
    "gdp_ukr_nominal = gdp_ukr_nominal.dropna()\n",
    "gdp_ukr_nominal[\"region\"] = gdp_ukr_nominal[\"region\"].map(region_map)\n",
    "gdp_ukr_nominal[\"region\"] = gdp_ukr_nominal[\"region\"].fillna(\"Sevastopol\")\n",
    "gdp_ukr_nominal.columns = gdp_ukr_nominal.columns.astype(str)\n",
    "gdp_ukr_nominal = gdp_ukr_nominal[~gdp_ukr_nominal[\"region\"].isin([\"Sevastopol\", \"Autonomous Republic of Crimea\"])]\n",
    "gdp_ukr_nominal[\"region\"] = gdp_ukr_nominal[\"region\"].str.replace(\" \", \"_\")\n",
    "\n",
    "# merge nominal gdp to real gdp by region\n",
    "gdp_ukr = gdp_ukr.merge(gdp_ukr_nominal, on=\"region\")\n",
    "\n",
    "# multiple the real gdp by the nominal gdp\n",
    "gdp_ukr[\"real_gdp\"] = gdp_ukr[\"real_gdp\"] * gdp_ukr[\"2012\"]\n",
    "\n",
    "# drop the nominal gdp column\n",
    "gdp_ukr = gdp_ukr.drop(columns=\"2012\")\n",
    "\n",
    "# save the data\n",
    "gdp_pol.to_csv(\"data/clean_pol_gdp.csv\", index=False)\n",
    "gdp_ukr.to_csv(\"data/clean_ukr_gdp.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>region</th>\n",
       "      <th>year</th>\n",
       "      <th>real_gdp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Vinnytsia_Oblast</td>\n",
       "      <td>2012</td>\n",
       "      <td>3302400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Volyn_Oblast</td>\n",
       "      <td>2012</td>\n",
       "      <td>2000500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dnipropetrovsk_Oblast</td>\n",
       "      <td>2012</td>\n",
       "      <td>14797000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Donetsk_Oblast</td>\n",
       "      <td>2012</td>\n",
       "      <td>17077500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Zhytomyr_Oblast</td>\n",
       "      <td>2012</td>\n",
       "      <td>2484900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>Khmelnytskyi_Oblast</td>\n",
       "      <td>2023</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>Cherkasy_Oblast</td>\n",
       "      <td>2023</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>Chernivtsi_Oblast</td>\n",
       "      <td>2023</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>Chernihiv_Oblast</td>\n",
       "      <td>2023</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>Kyiv</td>\n",
       "      <td>2023</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    region  year    real_gdp\n",
       "0         Vinnytsia_Oblast  2012   3302400.0\n",
       "1             Volyn_Oblast  2012   2000500.0\n",
       "2    Dnipropetrovsk_Oblast  2012  14797000.0\n",
       "3           Donetsk_Oblast  2012  17077500.0\n",
       "4          Zhytomyr_Oblast  2012   2484900.0\n",
       "..                     ...   ...         ...\n",
       "295    Khmelnytskyi_Oblast  2023         NaN\n",
       "296        Cherkasy_Oblast  2023         NaN\n",
       "297      Chernivtsi_Oblast  2023         NaN\n",
       "298       Chernihiv_Oblast  2023         NaN\n",
       "299                   Kyiv  2023         NaN\n",
       "\n",
       "[300 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdp_ukr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2012</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33024.0</td>\n",
       "      <td>Vinnytsia_Oblast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20005.0</td>\n",
       "      <td>Volyn_Oblast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>147970.0</td>\n",
       "      <td>Dnipropetrovsk_Oblast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>170775.0</td>\n",
       "      <td>Donetsk_Oblast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>24849.0</td>\n",
       "      <td>Zhytomyr_Oblast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>21404.0</td>\n",
       "      <td>Zakarpattia_Oblast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>54828.0</td>\n",
       "      <td>Zaporizhia_Oblast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>32286.0</td>\n",
       "      <td>Ivano-Frankivsk_Oblast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>69663.0</td>\n",
       "      <td>Kyiv_Oblast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>22056.0</td>\n",
       "      <td>Kirovohrad_Oblast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>58767.0</td>\n",
       "      <td>Luhansk_Oblast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>61962.0</td>\n",
       "      <td>Lviv_Oblast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>29205.0</td>\n",
       "      <td>Mykolaiv_Oblast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>64743.0</td>\n",
       "      <td>Odessa_Oblast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>56580.0</td>\n",
       "      <td>Poltava_Oblast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>21795.0</td>\n",
       "      <td>Rivne_Oblast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>24933.0</td>\n",
       "      <td>Sumy_Oblast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>17957.0</td>\n",
       "      <td>Ternopil_Oblast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>82223.0</td>\n",
       "      <td>Kharkiv_Oblast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>19357.0</td>\n",
       "      <td>Kherson_Oblast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>26237.0</td>\n",
       "      <td>Khmelnytskyi_Oblast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>31265.0</td>\n",
       "      <td>Cherkasy_Oblast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>13166.0</td>\n",
       "      <td>Chernivtsi_Oblast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>23934.0</td>\n",
       "      <td>Chernihiv_Oblast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>275685.0</td>\n",
       "      <td>Kyiv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        2012                  region\n",
       "3    33024.0        Vinnytsia_Oblast\n",
       "4    20005.0            Volyn_Oblast\n",
       "5   147970.0   Dnipropetrovsk_Oblast\n",
       "6   170775.0          Donetsk_Oblast\n",
       "7    24849.0         Zhytomyr_Oblast\n",
       "8    21404.0      Zakarpattia_Oblast\n",
       "9    54828.0       Zaporizhia_Oblast\n",
       "10   32286.0  Ivano-Frankivsk_Oblast\n",
       "11   69663.0             Kyiv_Oblast\n",
       "12   22056.0       Kirovohrad_Oblast\n",
       "13   58767.0          Luhansk_Oblast\n",
       "14   61962.0             Lviv_Oblast\n",
       "15   29205.0         Mykolaiv_Oblast\n",
       "16   64743.0           Odessa_Oblast\n",
       "17   56580.0          Poltava_Oblast\n",
       "18   21795.0            Rivne_Oblast\n",
       "19   24933.0             Sumy_Oblast\n",
       "20   17957.0         Ternopil_Oblast\n",
       "21   82223.0          Kharkiv_Oblast\n",
       "22   19357.0          Kherson_Oblast\n",
       "23   26237.0     Khmelnytskyi_Oblast\n",
       "24   31265.0         Cherkasy_Oblast\n",
       "25   13166.0       Chernivtsi_Oblast\n",
       "26   23934.0        Chernihiv_Oblast\n",
       "27  275685.0                    Kyiv"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "gdp_ukr_nominal = pd.read_excel(\"data/ukr_reg_gdp.xls\", skiprows=3, header=1)\n",
    "gdp_ukr_nominal = gdp_ukr_nominal.drop(columns=gdp_ukr_nominal.columns[0])\n",
    "gdp_ukr_nominal = gdp_ukr_nominal.iloc[:, np.r_[8, -1]]\n",
    "\n",
    "gdp_ukr_nominal = gdp_ukr_nominal.rename(columns={gdp_ukr_nominal.columns[-1]: \"region\"})\n",
    "gdp_ukr_nominal = gdp_ukr_nominal[~gdp_ukr_nominal[\"region\"].isin([\"Ukrane\", \"oblasts\"])]\n",
    "gdp_ukr_nominal = gdp_ukr_nominal.dropna()\n",
    "gdp_ukr_nominal[\"region\"] = gdp_ukr_nominal[\"region\"].map(region_map)\n",
    "gdp_ukr_nominal[\"region\"] = gdp_ukr_nominal[\"region\"].fillna(\"Sevastopol\")\n",
    "gdp_ukr_nominal.columns = gdp_ukr_nominal.columns.astype(str)\n",
    "gdp_ukr_nominal = gdp_ukr_nominal[~gdp_ukr_nominal[\"region\"].isin([\"Sevastopol\", \"Autonomous Republic of Crimea\"])]\n",
    "gdp_ukr_nominal[\"region\"] = gdp_ukr_nominal[\"region\"].str.replace(\" \", \"_\")\n",
    "gdp_ukr_nominal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Loading the MNIST dataset\n",
    "# from keras.datasets import mnist\n",
    "# (train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load clean gdp data\n",
    "gdp = pd.read_csv(\"data/clean_ukr_gdp.csv\")\n",
    "\n",
    "# delete observations with year > 2021\n",
    "gdp = gdp[gdp[\"year\"].astype(int) <= 2021]\n",
    "\n",
    "# Initialise a three dimensional array to store the images with the shape (number of images, height, width, channels)\n",
    "X = np.zeros((len(gdp), 765, 1076, 2))\n",
    "y = np.zeros(len(gdp))\n",
    "\n",
    "# load the snow covered and snow free images, add them together and append to the list\n",
    "for i in range(len(gdp)):\n",
    "\n",
    "    # get year, region, and gdp\n",
    "    year = gdp[\"year\"][i]\n",
    "    region = gdp[\"region\"][i]\n",
    "    gdp_value = gdp[\"real_gdp\"][i]\n",
    "\n",
    "    # get the file name\n",
    "    file_name = f\"{year}_{region}.h5\"\n",
    "\n",
    "    # load the image\n",
    "    file_path = f\"data/annual_region_images/{file_name}\"\n",
    "    \n",
    "    with h5py.File(file_path, 'r') as annual_region:\n",
    "        # nearnad_snow_cov = annual_region[\"NearNadir_Composite_Snow_Covered\"][:]\n",
    "        nearnad_snow_free = annual_region[\"NearNadir_Composite_Snow_Free\"][:]\n",
    "        # offnad_snow_cov = annual_region[\"OffNadir_Composite_Snow_Covered\"][:]\n",
    "        offnad_snow_free = annual_region[\"OffNadir_Composite_Snow_Free\"][:]\n",
    "        # allangle_snow_cov = annual_region[\"AllAngle_Composite_Snow_Covered\"][:]\n",
    "        # allangle_snow_free = annual_region[\"AllAngle_Composite_Snow_Free\"][:]\n",
    "\n",
    "        # add the two images together\n",
    "        # combined = snow_covered + snow_free\n",
    "\n",
    "    # add the gdp value to y\n",
    "    y[i] = gdp_value\n",
    "\n",
    "    # append both images as two channels to to X\n",
    "    # X[i, :, :, 0] = allangle_snow_cov\n",
    "    # X[i, :, :, 1] = allangle_snow_free\n",
    "    # X[i, :, :, 2] = offnad_snow_cov\n",
    "    X[i, :, :, 0] = offnad_snow_free\n",
    "    # X[i, :, :, 4] = nearnad_snow_cov\n",
    "    X[i, :, :, 1] = nearnad_snow_free\n",
    "# print(X.shape)\n",
    "# print(y.shape)\n",
    "\n",
    "# Normalise the images\n",
    "maximum = X.max()\n",
    "X = X / maximum\n",
    "\n",
    "# normalise GDP values\n",
    "y_max = y.max()\n",
    "y = y / y_max\n",
    "\n",
    "# print(y.mean())\n",
    "# print(maximum)\n",
    "\n",
    "# change the format to a float16\n",
    "X = X.astype(np.float16)\n",
    "y = y.astype(np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select 80% of the data for training, choose randomly\n",
    "# X is the images, y is the gdp\n",
    "train_size = int(0.8 * len(gdp))\n",
    "test_size = len(gdp) - train_size\n",
    "\n",
    "# select randomly train_size numbers from 0 to len(gdp)\n",
    "train_indices = np.random.choice(len(gdp), train_size, replace=False)\n",
    "test_indices = np.setdiff1d(np.arange(len(gdp)), train_indices)\n",
    "\n",
    "# get the train data\n",
    "X_train = X[train_indices]\n",
    "y_train = y[train_indices]\n",
    "\n",
    "# get the test data\n",
    "X_test = X[test_indices]\n",
    "y_test = y[test_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\jakub\\anaconda\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\jakub\\anaconda\\lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "# Resizing the images\n",
    "model.add(Resizing(300, 440, input_shape=(765, 1076, 2)))\n",
    "# Start with Convolutional layers\n",
    "model.add(Conv2D(16, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))  # Additional Conv layer\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))  # Additional Conv layer\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "# Flatten the results to feed into a dense layer\n",
    "model.add(Flatten())\n",
    "# Add dense layers (hidden layers)\n",
    "model.add(Dense(128, activation='relu'))  # Upscaled dense layer\n",
    "model.add(Dense(64, activation='relu'))   # Additional dense layer\n",
    "# Output layer\n",
    "model.add(Dense(1))\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(), loss='mean_squared_error', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " resizing (Resizing)         (None, 300, 440, 2)       0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 298, 438, 16)      304       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 149, 219, 16)      0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 147, 217, 32)      4640      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 73, 108, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 71, 106, 64)       18496     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 35, 53, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 33, 51, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 16, 25, 128)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 51200)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               6553728   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6659345 (25.40 MB)\n",
      "Trainable params: 6659345 (25.40 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\jakub\\anaconda\\lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\jakub\\anaconda\\lib\\site-packages\\keras\\src\\layers\\normalization\\batch_normalization.py:979: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 765, 1076, 2)]       0         []                            \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)             (None, 383, 538, 64)         6336      ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization (Batch  (None, 383, 538, 64)         256       ['conv2d[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation (Activation)     (None, 383, 538, 64)         0         ['batch_normalization[0][0]'] \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2  (None, 192, 269, 64)         0         ['activation[0][0]']          \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)           (None, 192, 269, 32)         18464     ['max_pooling2d[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_1 (Bat  (None, 192, 269, 32)         128       ['conv2d_1[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_1 (Activation)   (None, 192, 269, 32)         0         ['batch_normalization_1[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)           (None, 192, 269, 32)         9248      ['activation_1[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)           (None, 192, 269, 32)         2080      ['max_pooling2d[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_2 (Bat  (None, 192, 269, 32)         128       ['conv2d_2[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " add (Add)                   (None, 192, 269, 32)         0         ['conv2d_3[0][0]',            \n",
      "                                                                     'batch_normalization_2[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " activation_2 (Activation)   (None, 192, 269, 32)         0         ['add[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)           (None, 192, 269, 32)         9248      ['activation_2[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_3 (Bat  (None, 192, 269, 32)         128       ['conv2d_4[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_3 (Activation)   (None, 192, 269, 32)         0         ['batch_normalization_3[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)           (None, 192, 269, 32)         9248      ['activation_3[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_4 (Bat  (None, 192, 269, 32)         128       ['conv2d_5[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " add_1 (Add)                 (None, 192, 269, 32)         0         ['activation_2[0][0]',        \n",
      "                                                                     'batch_normalization_4[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " activation_4 (Activation)   (None, 192, 269, 32)         0         ['add_1[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)           (None, 96, 135, 64)          18496     ['activation_4[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_5 (Bat  (None, 96, 135, 64)          256       ['conv2d_6[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_5 (Activation)   (None, 96, 135, 64)          0         ['batch_normalization_5[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)           (None, 96, 135, 64)          36928     ['activation_5[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)           (None, 96, 135, 64)          2112      ['activation_4[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_6 (Bat  (None, 96, 135, 64)          256       ['conv2d_7[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " add_2 (Add)                 (None, 96, 135, 64)          0         ['conv2d_8[0][0]',            \n",
      "                                                                     'batch_normalization_6[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " activation_6 (Activation)   (None, 96, 135, 64)          0         ['add_2[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)           (None, 96, 135, 64)          36928     ['activation_6[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_7 (Bat  (None, 96, 135, 64)          256       ['conv2d_9[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_7 (Activation)   (None, 96, 135, 64)          0         ['batch_normalization_7[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)          (None, 96, 135, 64)          36928     ['activation_7[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_8 (Bat  (None, 96, 135, 64)          256       ['conv2d_10[0][0]']           \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " add_3 (Add)                 (None, 96, 135, 64)          0         ['activation_6[0][0]',        \n",
      "                                                                     'batch_normalization_8[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " activation_8 (Activation)   (None, 96, 135, 64)          0         ['add_3[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)          (None, 48, 68, 128)          73856     ['activation_8[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_9 (Bat  (None, 48, 68, 128)          512       ['conv2d_11[0][0]']           \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_9 (Activation)   (None, 48, 68, 128)          0         ['batch_normalization_9[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)          (None, 48, 68, 128)          147584    ['activation_9[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)          (None, 48, 68, 128)          8320      ['activation_8[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_10 (Ba  (None, 48, 68, 128)          512       ['conv2d_12[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " add_4 (Add)                 (None, 48, 68, 128)          0         ['conv2d_13[0][0]',           \n",
      "                                                                     'batch_normalization_10[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_10 (Activation)  (None, 48, 68, 128)          0         ['add_4[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)          (None, 48, 68, 128)          147584    ['activation_10[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_11 (Ba  (None, 48, 68, 128)          512       ['conv2d_14[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_11 (Activation)  (None, 48, 68, 128)          0         ['batch_normalization_11[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)          (None, 48, 68, 128)          147584    ['activation_11[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_12 (Ba  (None, 48, 68, 128)          512       ['conv2d_15[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " add_5 (Add)                 (None, 48, 68, 128)          0         ['activation_10[0][0]',       \n",
      "                                                                     'batch_normalization_12[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_12 (Activation)  (None, 48, 68, 128)          0         ['add_5[0][0]']               \n",
      "                                                                                                  \n",
      " global_average_pooling2d (  (None, 128)                  0         ['activation_12[0][0]']       \n",
      " GlobalAveragePooling2D)                                                                          \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 1)                    129       ['global_average_pooling2d[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 714913 (2.73 MB)\n",
      "Trainable params: 712993 (2.72 MB)\n",
      "Non-trainable params: 1920 (7.50 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def basic_block(x, filters, stride=1):\n",
    "    y = Conv2D(filters, (3, 3), strides=stride, padding='same')(x)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation('relu')(y)\n",
    "\n",
    "    y = Conv2D(filters, (3, 3), padding='same')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "\n",
    "    if stride != 1 or x.shape[-1] != filters:\n",
    "        x = Conv2D(filters, (1, 1), strides=stride, padding='same')(x)\n",
    "    y = Add()([x, y])\n",
    "    y = Activation('relu')(y)\n",
    "    return y\n",
    "\n",
    "def ResNet18(input_shape=(765, 1076, 2)):\n",
    "    input = Input(shape=input_shape)\n",
    "\n",
    "    x = Resizing(300, 440, input_shape=(765, 1076, 2))\n",
    "\n",
    "    x = Conv2D(64, (7, 7), strides=(2, 2), padding='same')(input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
    "\n",
    "    x = basic_block(x, 32, stride=1)\n",
    "    x = basic_block(x, 32, stride=1)\n",
    "    x = basic_block(x, 64, stride=2)\n",
    "    x = basic_block(x, 64, stride=1)\n",
    "    x = basic_block(x, 128, stride=2)\n",
    "    x = basic_block(x, 128, stride=1)\n",
    "    # x = basic_block(x, 512, stride=2)\n",
    "    # x = basic_block(x, 512, stride=1)\n",
    "\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(1, activation='linear')(x)\n",
    "\n",
    "    model = Model(inputs=input, outputs=x)\n",
    "    return model\n",
    "\n",
    "# Example usage\n",
    "model = ResNet18()\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=Adam(), loss='mean_squared_error', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\jakub\\anaconda\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\jakub\\anaconda\\lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "# Resizing the images\n",
    "model.add(Resizing(300, 440, input_shape=(765, 1076, 2)))\n",
    "\n",
    "# Start with Convolutional layers\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))  # Increased number of filters\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))  # Increased number of filters\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))  # Increased number of filters\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(Conv2D(256, (3, 3), activation='relu'))  # Increased number of filters\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "# Flatten the results to feed into a dense layer\n",
    "model.add(Flatten())\n",
    "\n",
    "# Add dense layers (hidden layers)\n",
    "model.add(Dense(256, activation='relu'))  # Increased the number of neurons\n",
    "model.add(Dense(128, activation='relu'))  # Increased the number of neurons\n",
    "model.add(Dense(64, activation='relu'))   # Kept as is for detailed feature extraction\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(), loss='mean_squared_error', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 765, 1076, 2)\n",
      "0.02544\n"
     ]
    }
   ],
   "source": [
    "# check the size of X_train and y_train\n",
    "print(X_train.shape)\n",
    "print(X_train[1, :, :, 0].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "WARNING:tensorflow:From c:\\Users\\jakub\\anaconda\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\jakub\\anaconda\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "3/3 [==============================] - 16s 4s/step - loss: 0.0640 - mae: 0.1743 - val_loss: 0.0293 - val_mae: 0.0902\n",
      "Epoch 2/20\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.0439 - mae: 0.1071 - val_loss: 0.0346 - val_mae: 0.1490\n",
      "Epoch 3/20\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.0414 - mae: 0.1337 - val_loss: 0.0277 - val_mae: 0.0993\n",
      "Epoch 4/20\n",
      "3/3 [==============================] - 8s 2s/step - loss: 0.0399 - mae: 0.1153 - val_loss: 0.0269 - val_mae: 0.1088\n",
      "Epoch 5/20\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.0377 - mae: 0.1098 - val_loss: 0.0249 - val_mae: 0.1064\n",
      "Epoch 6/20\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.0350 - mae: 0.1026 - val_loss: 0.0373 - val_mae: 0.1708\n",
      "Epoch 7/20\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.0334 - mae: 0.1171 - val_loss: 0.0206 - val_mae: 0.0602\n",
      "Epoch 8/20\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.0328 - mae: 0.0889 - val_loss: 0.0168 - val_mae: 0.0758\n",
      "Epoch 9/20\n",
      "3/3 [==============================] - 8s 2s/step - loss: 0.0277 - mae: 0.1023 - val_loss: 0.0155 - val_mae: 0.0865\n",
      "Epoch 10/20\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.0296 - mae: 0.0962 - val_loss: 0.0368 - val_mae: 0.1799\n",
      "Epoch 11/20\n",
      "3/3 [==============================] - 8s 2s/step - loss: 0.0252 - mae: 0.1210 - val_loss: 0.0086 - val_mae: 0.0436\n",
      "Epoch 12/20\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.0119 - mae: 0.0561 - val_loss: 0.0097 - val_mae: 0.0427\n",
      "Epoch 13/20\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.0093 - mae: 0.0572 - val_loss: 0.0068 - val_mae: 0.0483\n",
      "Epoch 14/20\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.0083 - mae: 0.0588 - val_loss: 0.0043 - val_mae: 0.0558\n",
      "Epoch 15/20\n",
      "3/3 [==============================] - 8s 2s/step - loss: 0.0044 - mae: 0.0547 - val_loss: 0.0036 - val_mae: 0.0467\n",
      "Epoch 16/20\n",
      "3/3 [==============================] - 9s 2s/step - loss: 0.0034 - mae: 0.0429 - val_loss: 0.0025 - val_mae: 0.0283\n",
      "Epoch 17/20\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.0032 - mae: 0.0352 - val_loss: 0.0019 - val_mae: 0.0262\n",
      "Epoch 18/20\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.0024 - mae: 0.0342 - val_loss: 0.0020 - val_mae: 0.0311\n",
      "Epoch 19/20\n",
      "3/3 [==============================] - 8s 2s/step - loss: 0.0024 - mae: 0.0337 - val_loss: 0.0019 - val_mae: 0.0266\n",
      "Epoch 20/20\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.0023 - mae: 0.0297 - val_loss: 0.0016 - val_mae: 0.0261\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2082d7c3dc0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=20, batch_size=64, validation_split=0.2)  # Assuming you have a validation split of 20%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 185ms/step - loss: 0.0018 - mae: 0.0279\n",
      "Test MAE: 0.02792929857969284\n",
      "Test Loss: 0.001751961070112884\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Evaluate your model on the testing data\n",
    "test_loss, test_mae = model.evaluate(X_test, y_test)\n",
    "print('Test MAE:', test_mae) # mean absolute error\n",
    "print('Test Loss:', test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 110ms/step\n",
      "24.462252\n"
     ]
    }
   ],
   "source": [
    "y_hat = model.predict(X_test).flatten()\n",
    "\n",
    "# print the mean absolute percentage error\n",
    "print(np.mean(100*np.abs((y_test - y_hat) / y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 124ms/step\n",
      "0.048462953\n",
      "10.229971\n"
     ]
    }
   ],
   "source": [
    "# get the predictions from X_test\n",
    "y_hat = model.predict(X_test).flatten()\n",
    "\n",
    "# convert the predictions back to the original scale\n",
    "# y_hat = y_hat * y_std + y_mean\n",
    "# y_test = y_test * y_std + y_mean\n",
    "\n",
    "# compute the mae\n",
    "mae = np.mean(np.abs(y_test - y_hat))\n",
    "print(mae)\n",
    "\n",
    "# compute the mean percentage error\n",
    "percentage_error = np.mean(100*np.abs((y_test - y_hat) / y_test))\n",
    "print(percentage_error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "gdp = pd.read_csv(\"data/clean_ukr_gdp.csv\")\n",
    "\n",
    "\n",
    "# Initialise a three dimensional array to store the images with the shape (number of images, height, width, channels)\n",
    "X = np.zeros((len(gdp), 765, 1076))\n",
    "\n",
    "# load the snow covered and snow free images, add them together and append to the list\n",
    "for i in range(len(gdp)):\n",
    "\n",
    "    # get year, region, and gdp\n",
    "    year = gdp[\"year\"][i]\n",
    "    region = gdp[\"region\"][i]\n",
    "    gdp_value = gdp[\"real_gdp\"][i]\n",
    "\n",
    "    # get the file name\n",
    "    file_name = f\"{year}_{region}_hq.h5\"\n",
    "\n",
    "    # load the image\n",
    "    file_path = f\"data/annual_region_images/{file_name}\"\n",
    "    \n",
    "    with h5py.File(file_path, 'r') as annual_region:\n",
    "        # nearnad_snow_cov = annual_region[\"NearNadir_Composite_Snow_Covered\"][:]\n",
    "        # nearnad_snow_free = annual_region[\"NearNadir_Composite_Snow_Free\"][:]\n",
    "        # offnad_snow_cov = annual_region[\"OffNadir_Composite_Snow_Covered\"][:]\n",
    "        offnad_snow_free = annual_region[\"OffNadir_Composite_Snow_Free\"][:]\n",
    "        # allangle_snow_cov = annual_region[\"AllAngle_Composite_Snow_Covered\"][:]\n",
    "        # allangle_snow_free = annual_region[\"AllAngle_Composite_Snow_Free\"][:]\n",
    "\n",
    "        # add the two images together\n",
    "        # combined = snow_covered + snow_free\n",
    "\n",
    "    # append both images as two channels to to X\n",
    "    # X[i, :, :]= allangle_snow_cov\n",
    "    # X[i, :, :] = allangle_snow_free\n",
    "    # X[i, :, :] = offnad_snow_cov\n",
    "    X[i, :, :] = offnad_snow_free\n",
    "    # X[i, :, :] = nearnad_snow_cov\n",
    "    # X[i, :, :] = nearnad_snow_free\n",
    "\n",
    "X = X.astype(np.float32)\n",
    "\n",
    "# take the log\n",
    "X = np.log(X + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jakub\\anaconda\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\jakub\\anaconda\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "(300, 765, 1076)\n",
      "(300, 382, 538, 1)\n"
     ]
    }
   ],
   "source": [
    "# resize images \n",
    "import tensorflow as tf\n",
    "print(X.shape)\n",
    "\n",
    "h, w = 382, 538\n",
    "if X.ndim == 3:  # If images have shape (300, 765, 1076)\n",
    "    X = np.expand_dims(X, axis=-1)\n",
    "images_resized = tf.image.resize(X, (h, w)).numpy()\n",
    "\n",
    "print(images_resized.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\jakub\\anaconda\\lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\jakub\\anaconda\\lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\jakub\\anaconda\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 382, 538, 1)]     0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 382, 538, 16)      160       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 191, 269, 16)      0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 191, 269, 32)      4640      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 96, 135, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 414720)            0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                4147210   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 407360)            4480960   \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 95, 134, 32)       0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 95, 134, 32)       9248      \n",
      "                                                                 \n",
      " up_sampling2d (UpSampling2  (None, 190, 268, 32)      0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 190, 268, 16)      4624      \n",
      "                                                                 \n",
      " up_sampling2d_1 (UpSamplin  (None, 380, 536, 16)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " zero_padding2d (ZeroPaddin  (None, 382, 538, 16)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 382, 538, 1)       145       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8646987 (32.99 MB)\n",
      "Trainable params: 8646987 (32.99 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Dimensions\n",
    "input_shape = (h, w, 1)\n",
    "\n",
    "# Length of the compressed vector\n",
    "k = 10\n",
    "\n",
    "# Resize the data\n",
    "images_resized = tf.image.resize(X, (h, w)).numpy()\n",
    "\n",
    "# normalize the data\n",
    "# images_resized = images_resized / images_resized.max()\n",
    "\n",
    "# Define the autoencoder\n",
    "input_img = layers.Input(shape=input_shape)\n",
    "\n",
    "# Encoder\n",
    "x = layers.Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\n",
    "x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "# Flatten and add the bottleneck layer\n",
    "x = layers.Flatten()(x)\n",
    "encoded = layers.Dense(k, activation='relu')(x)\n",
    "\n",
    "# Decoder\n",
    "x = layers.Dense((h // 4) * (w// 4) * 32, activation='relu')(encoded)\n",
    "x = layers.Reshape((h // 4, w // 4, 32))(x)\n",
    "x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "x = layers.UpSampling2D((2, 2))(x)\n",
    "x = layers.Conv2D(16, (3, 3), activation='relu', padding='same')(x)\n",
    "x = layers.UpSampling2D((2, 2))(x)\n",
    "x = layers.ZeroPadding2D(padding=((1, 1), (1, 1)))(x)\n",
    "\n",
    "decoded = layers.Conv2D(1, (3, 3), activation='linear', padding='same')(x)\n",
    "# Create the autoencoder model\n",
    "autoencoder = models.Model(input_img, decoded)\n",
    "\n",
    "# Compile the model\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Print the model summary\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "WARNING:tensorflow:From c:\\Users\\jakub\\anaconda\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "10/10 [==============================] - 29s 2s/step - loss: 0.1712\n",
      "Epoch 2/20\n",
      "10/10 [==============================] - 19s 2s/step - loss: 0.1492\n",
      "Epoch 3/20\n",
      "10/10 [==============================] - 26s 3s/step - loss: 0.1378\n",
      "Epoch 4/20\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.1248\n",
      "Epoch 5/20\n",
      "10/10 [==============================] - 24s 2s/step - loss: 0.1127\n",
      "Epoch 6/20\n",
      "10/10 [==============================] - 26s 2s/step - loss: 0.1010\n",
      "Epoch 7/20\n",
      "10/10 [==============================] - 21s 2s/step - loss: 0.0895\n",
      "Epoch 8/20\n",
      "10/10 [==============================] - 19s 2s/step - loss: 0.0803\n",
      "Epoch 9/20\n",
      "10/10 [==============================] - 20s 2s/step - loss: 0.0730\n",
      "Epoch 10/20\n",
      "10/10 [==============================] - 21s 2s/step - loss: 0.0673\n",
      "Epoch 11/20\n",
      "10/10 [==============================] - 21s 2s/step - loss: 0.0638\n",
      "Epoch 12/20\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.0602\n",
      "Epoch 13/20\n",
      "10/10 [==============================] - 21s 2s/step - loss: 0.0571\n",
      "Epoch 14/20\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.0547\n",
      "Epoch 15/20\n",
      "10/10 [==============================] - 21s 2s/step - loss: 0.0532\n",
      "Epoch 16/20\n",
      "10/10 [==============================] - 21s 2s/step - loss: 0.0538\n",
      "Epoch 17/20\n",
      "10/10 [==============================] - 21s 2s/step - loss: 0.0517\n",
      "Epoch 18/20\n",
      "10/10 [==============================] - 23s 2s/step - loss: 0.0492\n",
      "Epoch 19/20\n",
      "10/10 [==============================] - 28s 3s/step - loss: 0.0475\n",
      "Epoch 20/20\n",
      "10/10 [==============================] - 19s 2s/step - loss: 0.0461\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1592d464c40>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming you have training data loaded in `train_images`\n",
    "autoencoder.fit(images_resized, images_resized, epochs=20, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jakub\\anaconda\\lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "autoencoder.save('simple_autoencoder.h5')\n",
    "encoder = models.Model(input_img, encoded)\n",
    "encoder.save('encoder.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 3s 165ms/step\n"
     ]
    }
   ],
   "source": [
    "encoded_imgs = encoder.predict(images_resized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# turn the encoded images to a pandas dataframe\n",
    "encoded_imgs = pd.DataFrame(encoded_imgs)\n",
    "gdp = pd.read_csv(\"data/clean_ukr_gdp.csv\")\n",
    "\n",
    "\n",
    "# change column names to encoded_1, encoded_2, ...\n",
    "encoded_imgs.columns = [f\"encoded_{i}\" for i in range(encoded_imgs.shape[1])]\n",
    "\n",
    "# # add encoded images to the gdp data as new columns\n",
    "gdp = pd.concat([gdp, encoded_imgs], axis=1)\n",
    "\n",
    "data = pd.get_dummies(gdp, columns=[\"region\"])\n",
    "\n",
    "# training data contains years until 2021\n",
    "train_data = data[data[\"year\"] <= 2021]\n",
    "test_data = data[data[\"year\"] >= 2022]\n",
    "\n",
    "X = train_data.drop(columns=[\"year\", \"real_gdp\"])\n",
    "y = train_data[\"real_gdp\"]\n",
    "\n",
    "# take randomly 80% of the data for training\n",
    "train_size = int(0.8 * len(train_data))\n",
    "test_size = len(train_data) - train_size\n",
    "\n",
    "train_indices = np.random.choice(len(train_data), train_size, replace=False)\n",
    "test_indices = np.setdiff1d(np.arange(len(train_data)), train_indices)\n",
    "\n",
    "X_train = X.iloc[train_indices]\n",
    "y_train = y.iloc[train_indices]\n",
    "X_test = X.iloc[test_indices]\n",
    "y_test = y.iloc[test_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "436327.0290139677\n",
      "11.918372167146778\n"
     ]
    }
   ],
   "source": [
    "# fit a XGBoost model\n",
    "import xgboost as xgb\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(objective=\"reg:squarederror\", random_state=0)\n",
    "\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# get the predictions\n",
    "y_hat = xgb_model.predict(X_test)\n",
    "\n",
    "# compute the mae\n",
    "mae = np.mean(np.abs(y_test - y_hat))\n",
    "\n",
    "# compute the mean percentage error\n",
    "percentage_error = np.mean(100*np.abs((y_test - y_hat) / y_test))\n",
    "\n",
    "print(mae)\n",
    "print(percentage_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/4118865579.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  results[\"gdp_prediction\"] = y_hat\n"
     ]
    }
   ],
   "source": [
    "# predict on full data\n",
    "best_model = xgb.XGBRegressor(objective=\"reg:squarederror\", random_state=0)\n",
    "\n",
    "best_model.fit(X, y)\n",
    "\n",
    "results = test_data\n",
    "test_data = test_data.drop(columns=[\"year\", \"real_gdp\"])\n",
    "y_hat = best_model.predict(test_data)\n",
    "\n",
    "results[\"gdp_prediction\"] = y_hat\n",
    "\n",
    "# results = results[[\"region\", \"gdp_prediction\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>real_gdp</th>\n",
       "      <th>encoded_0</th>\n",
       "      <th>encoded_1</th>\n",
       "      <th>encoded_2</th>\n",
       "      <th>encoded_3</th>\n",
       "      <th>encoded_4</th>\n",
       "      <th>encoded_5</th>\n",
       "      <th>encoded_6</th>\n",
       "      <th>encoded_7</th>\n",
       "      <th>...</th>\n",
       "      <th>region_Poltava_Oblast</th>\n",
       "      <th>region_Rivne_Oblast</th>\n",
       "      <th>region_Sumy_Oblast</th>\n",
       "      <th>region_Ternopil_Oblast</th>\n",
       "      <th>region_Vinnytsia_Oblast</th>\n",
       "      <th>region_Volyn_Oblast</th>\n",
       "      <th>region_Zakarpattia_Oblast</th>\n",
       "      <th>region_Zaporizhia_Oblast</th>\n",
       "      <th>region_Zhytomyr_Oblast</th>\n",
       "      <th>gdp_prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>2022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>32.356968</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>43.147476</td>\n",
       "      <td>6.614171</td>\n",
       "      <td>57.424915</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2.901053e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>2022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>65.775826</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42.208607</td>\n",
       "      <td>28.832651</td>\n",
       "      <td>20.495667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>19.882025</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2.009610e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>2022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>28.939548</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.387489</td>\n",
       "      <td>51.399616</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.370453e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>2022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>85.156311</td>\n",
       "      <td>2.947619</td>\n",
       "      <td>62.387512</td>\n",
       "      <td>32.447998</td>\n",
       "      <td>23.472427</td>\n",
       "      <td>31.117155</td>\n",
       "      <td>4.903654</td>\n",
       "      <td>44.713188</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3.709054e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>2022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.884701</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>38.491554</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.141434</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.568389</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>2.948006e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>2022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>132.615051</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.576831</td>\n",
       "      <td>13.221070</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.538671</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2.111304e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>2022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.066391</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>69.055626</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2.851961e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>2022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24.315319</td>\n",
       "      <td>12.990799</td>\n",
       "      <td>23.690477</td>\n",
       "      <td>40.577160</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.253977</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3.480044e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>2022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24.124853</td>\n",
       "      <td>11.860563</td>\n",
       "      <td>3.499140</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>83.198761</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42.292961</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2.524573e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>2022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>49.317535</td>\n",
       "      <td>23.669285</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2.799378e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>2022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>64.932922</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>62.514519</td>\n",
       "      <td>85.151123</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.974605e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>2022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.377668</td>\n",
       "      <td>105.309410</td>\n",
       "      <td>51.549778</td>\n",
       "      <td>2.520474</td>\n",
       "      <td>39.702934</td>\n",
       "      <td>20.234741</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>99.735855</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>5.917208e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>2022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.117034</td>\n",
       "      <td>9.557559</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3.109010e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>2022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>32.279461</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>5.157749e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>2022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.620899</td>\n",
       "      <td>29.988794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3.881390e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>2022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>47.804504</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.081977</td>\n",
       "      <td>24.307182</td>\n",
       "      <td>2.423903</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.453278</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2.231310e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>2022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>23.274954</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27.327993</td>\n",
       "      <td>1.941042</td>\n",
       "      <td>4.701285</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098328</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2.496220e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>2022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.404328</td>\n",
       "      <td>34.771671</td>\n",
       "      <td>36.263599</td>\n",
       "      <td>26.087931</td>\n",
       "      <td>17.159975</td>\n",
       "      <td>43.585247</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2.251190e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>2022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.531701</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.862323</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4.148731e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>2022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>86.954224</td>\n",
       "      <td>84.024147</td>\n",
       "      <td>50.510502</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.939266e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>2022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.881707</td>\n",
       "      <td>1.879484</td>\n",
       "      <td>8.300534</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>63.572739</td>\n",
       "      <td>70.749725</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2.624105e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>2022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>35.141968</td>\n",
       "      <td>26.455742</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.144335</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2.961603e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>2022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.895641</td>\n",
       "      <td>49.103500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.424743</td>\n",
       "      <td>49.028187</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.219572</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.645569e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>2022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.357098</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.728305</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2.571784e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>2022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>58.731270</td>\n",
       "      <td>46.541512</td>\n",
       "      <td>51.322849</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.283657</td>\n",
       "      <td>17.720144</td>\n",
       "      <td>12.576164</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2.572198e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>2023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>75.949532</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>131.039780</td>\n",
       "      <td>64.321556</td>\n",
       "      <td>54.991585</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>94.787010</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4.900883e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>2023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>64.977066</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>81.399017</td>\n",
       "      <td>52.826775</td>\n",
       "      <td>17.857889</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>71.891693</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>6.023840e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>2023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>51.968273</td>\n",
       "      <td>20.797262</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>88.438080</td>\n",
       "      <td>15.208241</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.374459e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>2023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>79.791435</td>\n",
       "      <td>8.541647</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>95.033958</td>\n",
       "      <td>68.514725</td>\n",
       "      <td>33.335163</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>102.231369</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>6.869778e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>2023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>35.114967</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.980209</td>\n",
       "      <td>31.953747</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>79.424248</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>5.935744e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>2023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>166.415970</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42.227905</td>\n",
       "      <td>39.235859</td>\n",
       "      <td>3.090797</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>76.413551</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>6.494650e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>2023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21.286146</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.071556</td>\n",
       "      <td>116.726830</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>58.657703</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2.723546e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>2023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.700587</td>\n",
       "      <td>18.709120</td>\n",
       "      <td>68.790398</td>\n",
       "      <td>64.153214</td>\n",
       "      <td>59.984715</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>108.449814</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3.440170e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>2023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.468575</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>86.899544</td>\n",
       "      <td>23.521624</td>\n",
       "      <td>73.919250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>113.595375</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>6.475463e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>2023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>85.617317</td>\n",
       "      <td>110.655190</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.273093</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2.490796e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>2023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.193028</td>\n",
       "      <td>79.083710</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>123.386757</td>\n",
       "      <td>132.203903</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>82.014114</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4.168402e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>2023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>104.092178</td>\n",
       "      <td>53.096714</td>\n",
       "      <td>66.645973</td>\n",
       "      <td>43.919735</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>164.768494</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>6.925840e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>2023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>64.378624</td>\n",
       "      <td>53.028915</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>28.328146</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3.098534e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>2023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>118.833672</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>35.174244</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>5.554954e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>2023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>89.932915</td>\n",
       "      <td>79.645287</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>43.839787</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4.597882e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>2023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>58.596031</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>75.023285</td>\n",
       "      <td>38.211773</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>81.722946</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>6.112166e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>2023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>52.441467</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>56.185223</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>44.379845</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2.347504e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>2023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.049595</td>\n",
       "      <td>79.127190</td>\n",
       "      <td>40.940548</td>\n",
       "      <td>89.539642</td>\n",
       "      <td>75.145302</td>\n",
       "      <td>77.351135</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>86.608978</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>5.879978e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>2023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>35.120060</td>\n",
       "      <td>44.417534</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>58.568123</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4.573593e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>2023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>91.466980</td>\n",
       "      <td>95.484573</td>\n",
       "      <td>107.734558</td>\n",
       "      <td>0.114152</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.519308</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.982434e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>2023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.501080</td>\n",
       "      <td>16.011730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>34.552292</td>\n",
       "      <td>129.523178</td>\n",
       "      <td>79.853905</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>75.113213</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>6.201698e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>2023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.381772</td>\n",
       "      <td>47.776096</td>\n",
       "      <td>107.813789</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>38.612297</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3.188360e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>2023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>79.026489</td>\n",
       "      <td>82.651367</td>\n",
       "      <td>1.452701</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>54.113617</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>59.449387</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2.449324e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>2023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>23.723021</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>63.326340</td>\n",
       "      <td>6.445199</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>47.565456</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2.358227e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>2023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>58.493618</td>\n",
       "      <td>46.457417</td>\n",
       "      <td>51.225571</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>39.787750</td>\n",
       "      <td>16.869678</td>\n",
       "      <td>11.558626</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2.572198e+07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     year  real_gdp  encoded_0   encoded_1  encoded_2   encoded_3   encoded_4  \\\n",
       "250  2022       NaN   0.000000   32.356968   0.000000   43.147476    6.614171   \n",
       "251  2022       NaN   0.000000   65.775826   0.000000   42.208607   28.832651   \n",
       "252  2022       NaN   0.000000    0.000000  28.939548    0.000000    0.000000   \n",
       "253  2022       NaN  85.156311    2.947619  62.387512   32.447998   23.472427   \n",
       "254  2022       NaN   0.000000    6.884701   0.000000   38.491554    0.000000   \n",
       "255  2022       NaN   0.000000  132.615051   0.000000    5.576831   13.221070   \n",
       "256  2022       NaN   0.000000   16.066391   0.000000    0.000000   69.055626   \n",
       "257  2022       NaN   0.000000    0.000000  24.315319   12.990799   23.690477   \n",
       "258  2022       NaN   0.000000   24.124853  11.860563    3.499140    0.000000   \n",
       "259  2022       NaN   0.000000    0.000000  49.317535   23.669285    0.000000   \n",
       "260  2022       NaN   0.000000   64.932922   0.000000   62.514519   85.151123   \n",
       "261  2022       NaN  14.377668  105.309410  51.549778    2.520474   39.702934   \n",
       "262  2022       NaN   0.000000    0.000000   0.000000   12.117034    9.557559   \n",
       "263  2022       NaN   0.000000    0.000000   0.000000    0.000000    0.000000   \n",
       "264  2022       NaN   0.000000    0.000000   0.000000   40.620899   29.988794   \n",
       "265  2022       NaN   0.000000   47.804504   0.000000    5.081977   24.307182   \n",
       "266  2022       NaN   0.000000   23.274954   0.000000   27.327993    1.941042   \n",
       "267  2022       NaN   1.404328   34.771671  36.263599   26.087931   17.159975   \n",
       "268  2022       NaN   0.000000    0.000000   1.531701    0.000000    7.862323   \n",
       "269  2022       NaN   0.000000   86.954224  84.024147   50.510502    0.000000   \n",
       "270  2022       NaN   1.881707    1.879484   8.300534    0.000000   63.572739   \n",
       "271  2022       NaN   0.000000    0.000000  35.141968   26.455742    0.000000   \n",
       "272  2022       NaN   0.000000   50.895641  49.103500    0.000000    3.424743   \n",
       "273  2022       NaN   0.000000    1.357098   0.000000    6.728305    0.000000   \n",
       "274  2022       NaN  58.731270   46.541512  51.322849    0.000000    0.000000   \n",
       "275  2023       NaN   0.000000   75.949532   0.000000  131.039780   64.321556   \n",
       "276  2023       NaN   0.000000   64.977066   0.000000   81.399017   52.826775   \n",
       "277  2023       NaN   0.000000    0.000000  51.968273   20.797262    0.000000   \n",
       "278  2023       NaN  79.791435    8.541647   0.000000   95.033958   68.514725   \n",
       "279  2023       NaN   0.000000   35.114967   0.000000  100.980209   31.953747   \n",
       "280  2023       NaN   0.000000  166.415970   0.000000   42.227905   39.235859   \n",
       "281  2023       NaN   0.000000   21.286146   0.000000    9.071556  116.726830   \n",
       "282  2023       NaN   0.000000   14.700587  18.709120   68.790398   64.153214   \n",
       "283  2023       NaN   0.000000    5.468575   0.000000   86.899544   23.521624   \n",
       "284  2023       NaN   0.000000    0.000000  85.617317  110.655190    0.000000   \n",
       "285  2023       NaN   4.193028   79.083710   0.000000  123.386757  132.203903   \n",
       "286  2023       NaN   0.000000  104.092178  53.096714   66.645973   43.919735   \n",
       "287  2023       NaN   0.000000    0.000000   0.000000   64.378624   53.028915   \n",
       "288  2023       NaN   0.000000    0.000000   0.000000  118.833672    0.000000   \n",
       "289  2023       NaN   0.000000    0.000000   0.000000   89.932915   79.645287   \n",
       "290  2023       NaN   0.000000   58.596031   0.000000   75.023285   38.211773   \n",
       "291  2023       NaN   0.000000   52.441467   0.000000   56.185223    0.000000   \n",
       "292  2023       NaN  30.049595   79.127190  40.940548   89.539642   75.145302   \n",
       "293  2023       NaN   0.000000    0.000000   0.000000   35.120060   44.417534   \n",
       "294  2023       NaN   0.000000   91.466980  95.484573  107.734558    0.114152   \n",
       "295  2023       NaN  32.501080   16.011730   0.000000   34.552292  129.523178   \n",
       "296  2023       NaN   0.000000   15.381772  47.776096  107.813789    0.000000   \n",
       "297  2023       NaN   0.000000   79.026489  82.651367    1.452701    0.000000   \n",
       "298  2023       NaN   0.000000   23.723021   0.000000   63.326340    6.445199   \n",
       "299  2023       NaN  58.493618   46.457417  51.225571    0.000000    0.000000   \n",
       "\n",
       "     encoded_5  encoded_6   encoded_7  ...  region_Poltava_Oblast  \\\n",
       "250  57.424915   0.000000    0.000000  ...                  False   \n",
       "251  20.495667   0.000000   19.882025  ...                  False   \n",
       "252  33.387489  51.399616    0.000000  ...                  False   \n",
       "253  31.117155   4.903654   44.713188  ...                  False   \n",
       "254   7.141434   0.000000    0.568389  ...                  False   \n",
       "255   0.000000   0.000000   13.538671  ...                  False   \n",
       "256   0.000000   0.000000    0.000000  ...                  False   \n",
       "257  40.577160   0.000000   30.253977  ...                  False   \n",
       "258  83.198761   0.000000   42.292961  ...                  False   \n",
       "259   0.000000   0.000000    0.000000  ...                  False   \n",
       "260   0.000000   0.000000    0.000000  ...                  False   \n",
       "261  20.234741   0.000000   99.735855  ...                  False   \n",
       "262   0.000000   0.000000    0.000000  ...                  False   \n",
       "263  32.279461   0.000000    0.000000  ...                  False   \n",
       "264   0.000000   0.000000    0.000000  ...                   True   \n",
       "265   2.423903   0.000000    3.453278  ...                  False   \n",
       "266   4.701285   0.000000    0.098328  ...                  False   \n",
       "267  43.585247   0.000000    0.000000  ...                  False   \n",
       "268   0.000000   0.000000    0.000000  ...                  False   \n",
       "269   0.000000   0.000000    0.000000  ...                  False   \n",
       "270  70.749725   0.000000    0.000000  ...                  False   \n",
       "271   0.000000   0.000000    1.144335  ...                  False   \n",
       "272  49.028187   0.000000   14.219572  ...                  False   \n",
       "273   0.000000   0.000000    0.000000  ...                  False   \n",
       "274  40.283657  17.720144   12.576164  ...                  False   \n",
       "275  54.991585   0.000000   94.787010  ...                  False   \n",
       "276  17.857889   0.000000   71.891693  ...                  False   \n",
       "277   0.000000  88.438080   15.208241  ...                  False   \n",
       "278  33.335163   0.000000  102.231369  ...                  False   \n",
       "279   0.000000   0.000000   79.424248  ...                  False   \n",
       "280   3.090797   0.000000   76.413551  ...                  False   \n",
       "281   0.000000   0.000000   58.657703  ...                  False   \n",
       "282  59.984715   0.000000  108.449814  ...                  False   \n",
       "283  73.919250   0.000000  113.595375  ...                  False   \n",
       "284   0.000000   0.000000    5.273093  ...                  False   \n",
       "285   0.000000   0.000000   82.014114  ...                  False   \n",
       "286   0.000000   0.000000  164.768494  ...                  False   \n",
       "287   0.000000   0.000000   28.328146  ...                  False   \n",
       "288   0.000000   0.000000   35.174244  ...                  False   \n",
       "289   0.000000   0.000000   43.839787  ...                   True   \n",
       "290   0.000000   0.000000   81.722946  ...                  False   \n",
       "291   0.000000   0.000000   44.379845  ...                  False   \n",
       "292  77.351135   0.000000   86.608978  ...                  False   \n",
       "293   0.000000   0.000000   58.568123  ...                  False   \n",
       "294   0.000000   0.000000   16.519308  ...                  False   \n",
       "295  79.853905   0.000000   75.113213  ...                  False   \n",
       "296   0.000000   0.000000   38.612297  ...                  False   \n",
       "297  54.113617   0.000000   59.449387  ...                  False   \n",
       "298   0.000000   0.000000   47.565456  ...                  False   \n",
       "299  39.787750  16.869678   11.558626  ...                  False   \n",
       "\n",
       "     region_Rivne_Oblast  region_Sumy_Oblast  region_Ternopil_Oblast  \\\n",
       "250                False               False                   False   \n",
       "251                False               False                   False   \n",
       "252                False               False                   False   \n",
       "253                False               False                   False   \n",
       "254                False               False                   False   \n",
       "255                False               False                   False   \n",
       "256                False               False                   False   \n",
       "257                False               False                   False   \n",
       "258                False               False                   False   \n",
       "259                False               False                   False   \n",
       "260                False               False                   False   \n",
       "261                False               False                   False   \n",
       "262                False               False                   False   \n",
       "263                False               False                   False   \n",
       "264                False               False                   False   \n",
       "265                 True               False                   False   \n",
       "266                False                True                   False   \n",
       "267                False               False                    True   \n",
       "268                False               False                   False   \n",
       "269                False               False                   False   \n",
       "270                False               False                   False   \n",
       "271                False               False                   False   \n",
       "272                False               False                   False   \n",
       "273                False               False                   False   \n",
       "274                False               False                   False   \n",
       "275                False               False                   False   \n",
       "276                False               False                   False   \n",
       "277                False               False                   False   \n",
       "278                False               False                   False   \n",
       "279                False               False                   False   \n",
       "280                False               False                   False   \n",
       "281                False               False                   False   \n",
       "282                False               False                   False   \n",
       "283                False               False                   False   \n",
       "284                False               False                   False   \n",
       "285                False               False                   False   \n",
       "286                False               False                   False   \n",
       "287                False               False                   False   \n",
       "288                False               False                   False   \n",
       "289                False               False                   False   \n",
       "290                 True               False                   False   \n",
       "291                False                True                   False   \n",
       "292                False               False                    True   \n",
       "293                False               False                   False   \n",
       "294                False               False                   False   \n",
       "295                False               False                   False   \n",
       "296                False               False                   False   \n",
       "297                False               False                   False   \n",
       "298                False               False                   False   \n",
       "299                False               False                   False   \n",
       "\n",
       "     region_Vinnytsia_Oblast  region_Volyn_Oblast  region_Zakarpattia_Oblast  \\\n",
       "250                     True                False                      False   \n",
       "251                    False                 True                      False   \n",
       "252                    False                False                      False   \n",
       "253                    False                False                      False   \n",
       "254                    False                False                      False   \n",
       "255                    False                False                       True   \n",
       "256                    False                False                      False   \n",
       "257                    False                False                      False   \n",
       "258                    False                False                      False   \n",
       "259                    False                False                      False   \n",
       "260                    False                False                      False   \n",
       "261                    False                False                      False   \n",
       "262                    False                False                      False   \n",
       "263                    False                False                      False   \n",
       "264                    False                False                      False   \n",
       "265                    False                False                      False   \n",
       "266                    False                False                      False   \n",
       "267                    False                False                      False   \n",
       "268                    False                False                      False   \n",
       "269                    False                False                      False   \n",
       "270                    False                False                      False   \n",
       "271                    False                False                      False   \n",
       "272                    False                False                      False   \n",
       "273                    False                False                      False   \n",
       "274                    False                False                      False   \n",
       "275                     True                False                      False   \n",
       "276                    False                 True                      False   \n",
       "277                    False                False                      False   \n",
       "278                    False                False                      False   \n",
       "279                    False                False                      False   \n",
       "280                    False                False                       True   \n",
       "281                    False                False                      False   \n",
       "282                    False                False                      False   \n",
       "283                    False                False                      False   \n",
       "284                    False                False                      False   \n",
       "285                    False                False                      False   \n",
       "286                    False                False                      False   \n",
       "287                    False                False                      False   \n",
       "288                    False                False                      False   \n",
       "289                    False                False                      False   \n",
       "290                    False                False                      False   \n",
       "291                    False                False                      False   \n",
       "292                    False                False                      False   \n",
       "293                    False                False                      False   \n",
       "294                    False                False                      False   \n",
       "295                    False                False                      False   \n",
       "296                    False                False                      False   \n",
       "297                    False                False                      False   \n",
       "298                    False                False                      False   \n",
       "299                    False                False                      False   \n",
       "\n",
       "     region_Zaporizhia_Oblast  region_Zhytomyr_Oblast  gdp_prediction  \n",
       "250                     False                   False    2.901053e+06  \n",
       "251                     False                   False    2.009610e+06  \n",
       "252                     False                   False    1.370453e+07  \n",
       "253                     False                   False    3.709054e+06  \n",
       "254                     False                    True    2.948006e+06  \n",
       "255                     False                   False    2.111304e+06  \n",
       "256                      True                   False    2.851961e+06  \n",
       "257                     False                   False    3.480044e+06  \n",
       "258                     False                   False    2.524573e+06  \n",
       "259                     False                   False    2.799378e+06  \n",
       "260                     False                   False    1.974605e+06  \n",
       "261                     False                   False    5.917208e+06  \n",
       "262                     False                   False    3.109010e+06  \n",
       "263                     False                   False    5.157749e+06  \n",
       "264                     False                   False    3.881390e+06  \n",
       "265                     False                   False    2.231310e+06  \n",
       "266                     False                   False    2.496220e+06  \n",
       "267                     False                   False    2.251190e+06  \n",
       "268                     False                   False    4.148731e+06  \n",
       "269                     False                   False    1.939266e+06  \n",
       "270                     False                   False    2.624105e+06  \n",
       "271                     False                   False    2.961603e+06  \n",
       "272                     False                   False    1.645569e+06  \n",
       "273                     False                   False    2.571784e+06  \n",
       "274                     False                   False    2.572198e+07  \n",
       "275                     False                   False    4.900883e+06  \n",
       "276                     False                   False    6.023840e+06  \n",
       "277                     False                   False    1.374459e+07  \n",
       "278                     False                   False    6.869778e+06  \n",
       "279                     False                    True    5.935744e+06  \n",
       "280                     False                   False    6.494650e+06  \n",
       "281                      True                   False    2.723546e+06  \n",
       "282                     False                   False    3.440170e+06  \n",
       "283                     False                   False    6.475463e+06  \n",
       "284                     False                   False    2.490796e+06  \n",
       "285                     False                   False    4.168402e+06  \n",
       "286                     False                   False    6.925840e+06  \n",
       "287                     False                   False    3.098534e+06  \n",
       "288                     False                   False    5.554954e+06  \n",
       "289                     False                   False    4.597882e+06  \n",
       "290                     False                   False    6.112166e+06  \n",
       "291                     False                   False    2.347504e+06  \n",
       "292                     False                   False    5.879978e+06  \n",
       "293                     False                   False    4.573593e+06  \n",
       "294                     False                   False    1.982434e+06  \n",
       "295                     False                   False    6.201698e+06  \n",
       "296                     False                   False    3.188360e+06  \n",
       "297                     False                   False    2.449324e+06  \n",
       "298                     False                   False    2.358227e+06  \n",
       "299                     False                   False    2.572198e+07  \n",
       "\n",
       "[50 rows x 38 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "665518.0110806451\n",
      "11.308365807514534\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf_model = RandomForestRegressor(n_estimators=1000, random_state = 0)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# get the predictions\n",
    "y_hat = rf_model.predict(X_test)\n",
    "\n",
    "# compute the mae\n",
    "mae = np.mean(np.abs(y_test - y_hat))\n",
    "\n",
    "# compute the mean percentage error\n",
    "percentage_error = np.mean(100*np.abs((y_test - y_hat) / y_test))\n",
    "\n",
    "print(mae)\n",
    "print(percentage_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "620590.9893821041\n",
      "13.228268150766247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/1003370021.py:5: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  model.fit(X_train, y_train)\n",
      "c:\\Users\\jakub\\anaconda\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:530: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\jakub\\anaconda\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 54890190761860.125, tolerance: 713662316497.8124\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "model = Lasso(alpha=0)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# get the predictions\n",
    "y_hat = model.predict(X_test)\n",
    "\n",
    "# compute the mae\n",
    "mae = np.mean(np.abs(y_test - y_hat))\n",
    "\n",
    "# compute the mean percentage error\n",
    "percentage_error = np.mean(100*np.abs((y_test - y_hat) / y_test))\n",
    "\n",
    "print(mae)\n",
    "print(percentage_error)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single network per region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 [==============================] - 0s 410ms/step - loss: 2899.6953 - mae: 52.8233 - val_loss: 4068.0486 - val_mae: 48.5410\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 454.8785 - mae: 19.2141 - val_loss: 3358.1562 - val_mae: 50.0887\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 297ms/step - loss: 193.1741 - mae: 11.0878 - val_loss: 3299.6956 - val_mae: 54.9634\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 902.5318 - mae: 28.7994 - val_loss: 3386.0059 - val_mae: 57.1687\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 1449.6063 - mae: 37.1084 - val_loss: 3447.0120 - val_mae: 57.8721\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 1587.5557 - mae: 38.9249 - val_loss: 3490.9270 - val_mae: 57.7127\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 1381.5608 - mae: 36.1877 - val_loss: 3574.1543 - val_mae: 56.8205\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 948.7422 - mae: 29.6191 - val_loss: 3789.1223 - val_mae: 55.2009\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 457.6523 - mae: 19.6678 - val_loss: 4228.9092 - val_mae: 52.8605\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 120.1036 - mae: 9.0455 - val_loss: 4884.5513 - val_mae: 50.0088\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 111.8463 - mae: 8.6825 - val_loss: 5643.6533 - val_mae: 57.9683\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 303ms/step - loss: 377.6425 - mae: 17.4811 - val_loss: 6181.6509 - val_mae: 63.1236\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 311ms/step - loss: 617.8540 - mae: 23.3610 - val_loss: 6256.3940 - val_mae: 63.2272\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 587.6190 - mae: 22.7228 - val_loss: 5932.1465 - val_mae: 59.1377\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 353.1226 - mae: 16.8274 - val_loss: 5456.8604 - val_mae: 52.8683\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 139.8477 - mae: 9.6796 - val_loss: 5030.3628 - val_mae: 53.6654\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 276ms/step - loss: 68.0973 - mae: 7.0351 - val_loss: 4731.1694 - val_mae: 55.3013\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 116.2849 - mae: 8.7719 - val_loss: 4560.6069 - val_mae: 56.4500\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 207.9006 - mae: 11.8367 - val_loss: 4496.1113 - val_mae: 57.1619\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 278.3305 - mae: 14.5121 - val_loss: 4519.1172 - val_mae: 57.5134\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 296.1777 - mae: 15.1211 - val_loss: 4622.1167 - val_mae: 57.5734\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 260.2924 - mae: 13.8987 - val_loss: 4805.9414 - val_mae: 57.4021\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 190.6164 - mae: 11.1406 - val_loss: 5071.3467 - val_mae: 57.0566\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 345ms/step - loss: 117.8690 - mae: 8.6603 - val_loss: 5408.1011 - val_mae: 56.6043\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 276ms/step - loss: 71.7280 - mae: 6.9731 - val_loss: 5784.3066 - val_mae: 56.1278\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 276ms/step - loss: 68.0035 - mae: 7.0774 - val_loss: 6143.1357 - val_mae: 55.7220\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 279ms/step - loss: 99.6595 - mae: 8.4249 - val_loss: 6415.0220 - val_mae: 57.7689\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 139.9164 - mae: 10.0317 - val_loss: 6544.6558 - val_mae: 58.9051\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 159.6760 - mae: 10.8287 - val_loss: 6516.7759 - val_mae: 58.4838\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 281ms/step - loss: 147.2876 - mae: 10.3602 - val_loss: 6361.0376 - val_mae: 56.7796\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 309ms/step - loss: 113.6757 - mae: 8.9551 - val_loss: 6133.8652 - val_mae: 56.4662\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 292ms/step - loss: 80.2823 - mae: 7.5494 - val_loss: 5893.0874 - val_mae: 56.9342\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 63.0353 - mae: 6.7683 - val_loss: 5681.5278 - val_mae: 57.3536\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 356ms/step - loss: 64.9177 - mae: 6.5990 - val_loss: 5523.2705 - val_mae: 57.6863\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 78.5688 - mae: 7.1689 - val_loss: 5427.7383 - val_mae: 57.9124\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 371ms/step - loss: 93.1807 - mae: 7.5965 - val_loss: 5395.7378 - val_mae: 58.0297\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 100.4050 - mae: 7.8896 - val_loss: 5423.5742 - val_mae: 58.0446\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 281ms/step - loss: 97.2176 - mae: 7.7559 - val_loss: 5504.8306 - val_mae: 57.9714\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 346ms/step - loss: 85.9311 - mae: 7.3385 - val_loss: 5629.9604 - val_mae: 57.8302\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 72.1830 - mae: 6.9302 - val_loss: 5785.0454 - val_mae: 57.6468\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 280ms/step - loss: 61.9669 - mae: 6.4374 - val_loss: 5951.1499 - val_mae: 57.4504\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 281ms/step - loss: 58.8277 - mae: 6.4198 - val_loss: 6105.5591 - val_mae: 57.2711\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 276ms/step - loss: 62.3784 - mae: 6.8206 - val_loss: 6225.5991 - val_mae: 57.1367\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 68.8758 - mae: 7.1146 - val_loss: 6294.0376 - val_mae: 57.0656\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 292ms/step - loss: 73.5960 - mae: 7.2636 - val_loss: 6303.8140 - val_mae: 57.0650\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 73.5961 - mae: 7.2553 - val_loss: 6259.7017 - val_mae: 57.1286\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 349ms/step - loss: 69.0973 - mae: 7.1039 - val_loss: 6176.0576 - val_mae: 57.2397\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 62.7694 - mae: 6.8449 - val_loss: 6072.0156 - val_mae: 57.3755\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 57.7518 - mae: 6.5264 - val_loss: 5966.4927 - val_mae: 57.5122\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 303ms/step - loss: 55.8367 - mae: 6.1989 - val_loss: 5874.7910 - val_mae: 57.6294\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 56.8566 - mae: 6.1768 - val_loss: 5807.2109 - val_mae: 57.7132\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 59.2490 - mae: 6.3326 - val_loss: 5769.1479 - val_mae: 57.7549\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 61.1424 - mae: 6.4414 - val_loss: 5761.8350 - val_mae: 57.7523\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 61.3182 - mae: 6.4507 - val_loss: 5783.1299 - val_mae: 57.7079\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 59.6475 - mae: 6.3670 - val_loss: 5828.0884 - val_mae: 57.6291\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 56.9334 - mae: 6.2073 - val_loss: 5889.4233 - val_mae: 57.5263\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 54.3673 - mae: 6.0356 - val_loss: 5958.0664 - val_mae: 57.4125\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 52.8748 - mae: 6.0046 - val_loss: 6024.1167 - val_mae: 57.3011\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 52.7084 - mae: 6.1522 - val_loss: 6078.2231 - val_mae: 57.2046\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 53.4108 - mae: 6.2833 - val_loss: 6113.1035 - val_mae: 57.1323\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 54.1583 - mae: 6.3599 - val_loss: 6124.9150 - val_mae: 57.0893\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 266ms/step - loss: 54.2546 - mae: 6.3730 - val_loss: 6113.8369 - val_mae: 57.0753\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 53.4817 - mae: 6.3236 - val_loss: 6083.7349 - val_mae: 57.0858\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 52.1287 - mae: 6.2220 - val_loss: 6041.0566 - val_mae: 57.1125\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 50.7435 - mae: 6.0852 - val_loss: 5993.3223 - val_mae: 57.1457\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 49.7885 - mae: 5.9337 - val_loss: 5947.7129 - val_mae: 57.1757\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 49.4138 - mae: 5.7871 - val_loss: 5910.0596 - val_mae: 57.1941\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 49.4457 - mae: 5.7633 - val_loss: 5884.2817 - val_mae: 57.1952\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 49.5415 - mae: 5.7462 - val_loss: 5872.2358 - val_mae: 57.1761\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 49.3974 - mae: 5.7571 - val_loss: 5873.7964 - val_mae: 57.1366\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 48.8918 - mae: 5.7247 - val_loss: 5887.1499 - val_mae: 57.0792\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 48.1123 - mae: 5.6614 - val_loss: 5909.0854 - val_mae: 57.0085\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 47.2722 - mae: 5.6315 - val_loss: 5935.5166 - val_mae: 56.9304\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 312ms/step - loss: 46.5824 - mae: 5.6341 - val_loss: 5961.9731 - val_mae: 56.8514\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 307ms/step - loss: 46.1381 - mae: 5.6923 - val_loss: 5984.2788 - val_mae: 56.7778\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 389ms/step - loss: 45.8901 - mae: 5.7380 - val_loss: 5999.1377 - val_mae: 56.7142\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 305ms/step - loss: 45.6984 - mae: 5.7623 - val_loss: 6004.6084 - val_mae: 56.6635\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 45.4181 - mae: 5.7601 - val_loss: 6000.3433 - val_mae: 56.6265\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 44.9777 - mae: 5.7303 - val_loss: 5987.5142 - val_mae: 56.6015\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 415ms/step - loss: 44.4047 - mae: 5.6763 - val_loss: 5968.4712 - val_mae: 56.5854\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 43.7884 - mae: 5.6043 - val_loss: 5946.2461 - val_mae: 56.5737\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 293ms/step - loss: 43.2304 - mae: 5.5230 - val_loss: 5923.9795 - val_mae: 56.5616\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 308ms/step - loss: 42.7767 - mae: 5.4412 - val_loss: 5904.4409 - val_mae: 56.5447\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 42.4078 - mae: 5.3667 - val_loss: 5889.6758 - val_mae: 56.5199\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 42.0639 - mae: 5.3056 - val_loss: 5880.8228 - val_mae: 56.4850\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 41.6811 - mae: 5.2612 - val_loss: 5878.0654 - val_mae: 56.4398\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 41.2276 - mae: 5.2341 - val_loss: 5880.7148 - val_mae: 56.3853\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 40.7126 - mae: 5.2221 - val_loss: 5887.3760 - val_mae: 56.3238\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 40.1767 - mae: 5.2214 - val_loss: 5896.2104 - val_mae: 56.2583\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 266ms/step - loss: 39.6597 - mae: 5.2261 - val_loss: 5905.2065 - val_mae: 56.1917\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 318ms/step - loss: 39.1823 - mae: 5.2303 - val_loss: 5912.5083 - val_mae: 56.1272\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 309ms/step - loss: 38.7392 - mae: 5.2290 - val_loss: 5916.6514 - val_mae: 56.0672\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 38.3039 - mae: 5.2181 - val_loss: 5916.7778 - val_mae: 56.0131\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 37.8484 - mae: 5.1950 - val_loss: 5912.7197 - val_mae: 55.9675\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 340ms/step - loss: 37.3607 - mae: 5.1597 - val_loss: 5904.9355 - val_mae: 55.9279\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 328ms/step - loss: 36.8549 - mae: 5.1138 - val_loss: 5894.4258 - val_mae: 55.8935\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 36.3487 - mae: 5.0601 - val_loss: 5882.4990 - val_mae: 55.8615\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 35.8529 - mae: 5.0021 - val_loss: 5870.5376 - val_mae: 55.8290\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 35.3822 - mae: 4.9445 - val_loss: 5859.7949 - val_mae: 55.7941\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 264ms/step - loss: 34.9270 - mae: 4.9052 - val_loss: 5851.2017 - val_mae: 55.7548\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "Predicted GDP for Vinnytsia_Oblast in 2021: [121.38515]\n",
      "Actual GDP for Vinnytsia_Oblast in 2021: 131.375\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - 0s 399ms/step - loss: 196.5434 - mae: 12.5793 - val_loss: 5747.0479 - val_mae: 59.3014\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 146.8246 - mae: 10.5023 - val_loss: 5434.9800 - val_mae: 56.3433\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 83.5303 - mae: 7.8152 - val_loss: 5090.4961 - val_mae: 52.8651\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 39.9099 - mae: 5.5290 - val_loss: 4776.9873 - val_mae: 49.4642\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 29.3479 - mae: 4.4129 - val_loss: 4529.7896 - val_mae: 48.5753\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 293ms/step - loss: 45.0924 - mae: 5.1759 - val_loss: 4361.7710 - val_mae: 48.8013\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 326ms/step - loss: 70.7647 - mae: 6.9496 - val_loss: 4273.0898 - val_mae: 48.9426\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 90.8216 - mae: 8.1786 - val_loss: 4258.7783 - val_mae: 49.0024\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 337ms/step - loss: 96.2122 - mae: 8.5415 - val_loss: 4312.4053 - val_mae: 48.9881\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 470ms/step - loss: 85.8404 - mae: 7.9372 - val_loss: 4426.5630 - val_mae: 48.9109\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 310ms/step - loss: 65.0408 - mae: 6.6477 - val_loss: 4591.2681 - val_mae: 48.7869\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 42.5610 - mae: 5.1133 - val_loss: 4791.5786 - val_mae: 49.2563\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 26.8439 - mae: 3.8835 - val_loss: 5006.1016 - val_mae: 51.5365\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 334ms/step - loss: 22.5588 - mae: 3.7378 - val_loss: 5207.9224 - val_mae: 53.5844\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 387ms/step - loss: 28.7195 - mae: 4.7788 - val_loss: 5368.9126 - val_mae: 55.1524\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 388ms/step - loss: 39.4689 - mae: 5.5761 - val_loss: 5466.6460 - val_mae: 56.0665\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 47.4808 - mae: 6.0021 - val_loss: 5490.9116 - val_mae: 56.2628\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 464ms/step - loss: 48.0513 - mae: 6.0606 - val_loss: 5446.4639 - val_mae: 55.7958\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 330ms/step - loss: 41.2961 - mae: 5.6338 - val_loss: 5350.5112 - val_mae: 54.8196\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 371ms/step - loss: 31.2147 - mae: 4.9761 - val_loss: 5226.6221 - val_mae: 53.5451\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 388ms/step - loss: 22.7132 - mae: 4.1542 - val_loss: 5098.2480 - val_mae: 52.1932\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 425ms/step - loss: 18.8446 - mae: 3.2944 - val_loss: 4984.4126 - val_mae: 50.9603\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 466ms/step - loss: 19.7983 - mae: 3.3494 - val_loss: 4898.0439 - val_mae: 49.9960\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 475ms/step - loss: 23.5591 - mae: 3.9121 - val_loss: 4846.2349 - val_mae: 49.3952\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 359ms/step - loss: 27.3656 - mae: 4.2734 - val_loss: 4831.3174 - val_mae: 49.1977\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 29.0926 - mae: 4.4121 - val_loss: 4851.9614 - val_mae: 49.3934\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 376ms/step - loss: 28.0015 - mae: 4.3340 - val_loss: 4903.8462 - val_mae: 49.9302\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 352ms/step - loss: 24.7593 - mae: 4.0680 - val_loss: 4980.0859 - val_mae: 50.7230\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 400ms/step - loss: 20.8855 - mae: 3.6606 - val_loss: 5071.4746 - val_mae: 51.6626\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 400ms/step - loss: 17.9610 - mae: 3.3108 - val_loss: 5167.0962 - val_mae: 52.6288\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 16.9124 - mae: 3.2652 - val_loss: 5255.4390 - val_mae: 53.5041\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 410ms/step - loss: 17.6735 - mae: 3.4529 - val_loss: 5326.0156 - val_mae: 54.1891\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 400ms/step - loss: 19.3509 - mae: 3.6365 - val_loss: 5371.1641 - val_mae: 54.6159\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 330ms/step - loss: 20.7716 - mae: 3.8246 - val_loss: 5387.4287 - val_mae: 54.7571\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 327ms/step - loss: 21.1087 - mae: 3.8551 - val_loss: 5376.0459 - val_mae: 54.6274\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 356ms/step - loss: 20.2344 - mae: 3.7366 - val_loss: 5342.3599 - val_mae: 54.2777\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 343ms/step - loss: 18.6577 - mae: 3.5848 - val_loss: 5294.3486 - val_mae: 53.7837\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 357ms/step - loss: 17.1430 - mae: 3.4418 - val_loss: 5240.9458 - val_mae: 53.2313\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 376ms/step - loss: 16.2826 - mae: 3.3540 - val_loss: 5190.4678 - val_mae: 52.7037\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 317ms/step - loss: 16.2432 - mae: 3.3705 - val_loss: 5149.5640 - val_mae: 52.2703\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 326ms/step - loss: 16.7799 - mae: 3.3841 - val_loss: 5122.7012 - val_mae: 51.9797\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 327ms/step - loss: 17.4338 - mae: 3.3956 - val_loss: 5112.0669 - val_mae: 51.8570\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 337ms/step - loss: 17.7930 - mae: 3.4354 - val_loss: 5117.7065 - val_mae: 51.9041\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 411ms/step - loss: 17.6683 - mae: 3.4275 - val_loss: 5137.8042 - val_mae: 52.1014\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 328ms/step - loss: 17.1346 - mae: 3.3959 - val_loss: 5169.0254 - val_mae: 52.4129\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 345ms/step - loss: 16.4470 - mae: 3.3892 - val_loss: 5206.9556 - val_mae: 52.7913\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 361ms/step - loss: 15.8899 - mae: 3.3803 - val_loss: 5246.5972 - val_mae: 53.1848\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 344ms/step - loss: 15.6401 - mae: 3.3703 - val_loss: 5283.0371 - val_mae: 53.5435\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 384ms/step - loss: 15.7018 - mae: 3.3604 - val_loss: 5312.0469 - val_mae: 53.8259\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 15.9316 - mae: 3.3513 - val_loss: 5330.6812 - val_mae: 54.0042\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 358ms/step - loss: 16.1335 - mae: 3.3436 - val_loss: 5337.6465 - val_mae: 54.0665\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 489ms/step - loss: 16.1630 - mae: 3.3378 - val_loss: 5333.3931 - val_mae: 54.0178\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 1s 731ms/step - loss: 15.9882 - mae: 3.3336 - val_loss: 5319.8701 - val_mae: 53.8770\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 1s 595ms/step - loss: 15.6839 - mae: 3.3308 - val_loss: 5300.1162 - val_mae: 53.6739\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 498ms/step - loss: 15.3758 - mae: 3.3288 - val_loss: 5277.6426 - val_mae: 53.4428\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 438ms/step - loss: 15.1687 - mae: 3.3270 - val_loss: 5255.9038 - val_mae: 53.2184\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 415ms/step - loss: 15.1001 - mae: 3.3249 - val_loss: 5237.8291 - val_mae: 53.0303\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 400ms/step - loss: 15.1365 - mae: 3.3219 - val_loss: 5225.5234 - val_mae: 52.9003\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 353ms/step - loss: 15.1996 - mae: 3.3176 - val_loss: 5220.1104 - val_mae: 52.8403\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 374ms/step - loss: 15.2175 - mae: 3.3117 - val_loss: 5221.7388 - val_mae: 52.8520\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 354ms/step - loss: 15.1537 - mae: 3.3042 - val_loss: 5229.6641 - val_mae: 52.9277\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 395ms/step - loss: 15.0181 - mae: 3.2952 - val_loss: 5242.4160 - val_mae: 53.0524\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 370ms/step - loss: 14.8543 - mae: 3.2851 - val_loss: 5258.0479 - val_mae: 53.2060\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 346ms/step - loss: 14.7121 - mae: 3.2743 - val_loss: 5274.4028 - val_mae: 53.3664\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 328ms/step - loss: 14.6225 - mae: 3.2634 - val_loss: 5289.0410 - val_mae: 53.5096\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 340ms/step - loss: 14.5861 - mae: 3.2528 - val_loss: 5299.0205 - val_mae: 53.6072\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 322ms/step - loss: 14.5705 - mae: 3.2428 - val_loss: 5302.2622 - val_mae: 53.6396\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 440ms/step - loss: 14.5346 - mae: 3.2337 - val_loss: 5298.5498 - val_mae: 53.6049\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 312ms/step - loss: 14.4632 - mae: 3.2254 - val_loss: 5289.4165 - val_mae: 53.5177\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 14.3749 - mae: 3.2180 - val_loss: 5277.5767 - val_mae: 53.4039\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 331ms/step - loss: 14.2995 - mae: 3.2110 - val_loss: 5266.1504 - val_mae: 53.2936\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 327ms/step - loss: 14.2507 - mae: 3.2042 - val_loss: 5257.8428 - val_mae: 53.2128\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 303ms/step - loss: 14.2177 - mae: 3.1973 - val_loss: 5254.3892 - val_mae: 53.1786\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 320ms/step - loss: 14.1788 - mae: 3.1900 - val_loss: 5256.2017 - val_mae: 53.1951\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 313ms/step - loss: 14.1219 - mae: 3.1825 - val_loss: 5262.4268 - val_mae: 53.2541\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 309ms/step - loss: 14.0523 - mae: 3.1746 - val_loss: 5271.2695 - val_mae: 53.3380\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 326ms/step - loss: 13.9866 - mae: 3.1665 - val_loss: 5280.4902 - val_mae: 53.4253\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 348ms/step - loss: 13.9348 - mae: 3.1586 - val_loss: 5287.9712 - val_mae: 53.4957\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 303ms/step - loss: 13.8932 - mae: 3.1510 - val_loss: 5292.2334 - val_mae: 53.5350\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 307ms/step - loss: 13.8507 - mae: 3.1439 - val_loss: 5292.7231 - val_mae: 53.5379\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 311ms/step - loss: 13.7987 - mae: 3.1374 - val_loss: 5289.8848 - val_mae: 53.5087\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 345ms/step - loss: 13.7391 - mae: 3.1313 - val_loss: 5284.9487 - val_mae: 53.4591\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 324ms/step - loss: 13.6802 - mae: 3.1256 - val_loss: 5279.5366 - val_mae: 53.4047\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 310ms/step - loss: 13.6279 - mae: 3.1202 - val_loss: 5275.2476 - val_mae: 53.3610\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 340ms/step - loss: 13.5819 - mae: 3.1148 - val_loss: 5273.2349 - val_mae: 53.3392\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 430ms/step - loss: 13.5366 - mae: 3.1091 - val_loss: 5274.0142 - val_mae: 53.3442\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 450ms/step - loss: 13.4868 - mae: 3.1034 - val_loss: 5277.3643 - val_mae: 53.3740\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 13.4325 - mae: 3.0974 - val_loss: 5282.5005 - val_mae: 53.4208\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 346ms/step - loss: 13.3778 - mae: 3.0914 - val_loss: 5288.2598 - val_mae: 53.4736\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 325ms/step - loss: 13.3262 - mae: 3.0853 - val_loss: 5293.4531 - val_mae: 53.5208\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 343ms/step - loss: 13.2785 - mae: 3.0794 - val_loss: 5297.1499 - val_mae: 53.5536\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 332ms/step - loss: 13.2316 - mae: 3.0737 - val_loss: 5298.8833 - val_mae: 53.5673\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 336ms/step - loss: 13.1825 - mae: 3.0684 - val_loss: 5298.7192 - val_mae: 53.5628\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 348ms/step - loss: 13.1313 - mae: 3.0633 - val_loss: 5297.1978 - val_mae: 53.5454\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 350ms/step - loss: 13.0794 - mae: 3.0584 - val_loss: 5295.1230 - val_mae: 53.5226\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 13.0289 - mae: 3.0536 - val_loss: 5293.3506 - val_mae: 53.5028\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 344ms/step - loss: 12.9804 - mae: 3.0488 - val_loss: 5292.5981 - val_mae: 53.4928\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 311ms/step - loss: 12.9325 - mae: 3.0437 - val_loss: 5293.2490 - val_mae: 53.4963\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 317ms/step - loss: 12.8838 - mae: 3.0385 - val_loss: 5295.2910 - val_mae: 53.5131\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 332ms/step - loss: 12.8334 - mae: 3.0329 - val_loss: 5298.3975 - val_mae: 53.5402\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "Predicted GDP for Volyn_Oblast in 2021: [107.682755]\n",
      "Actual GDP for Volyn_Oblast in 2021: 95.75\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - 0s 430ms/step - loss: 6686.4980 - mae: 78.9293 - val_loss: 5295.7573 - val_mae: 68.3569\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 312ms/step - loss: 515.2283 - mae: 18.4450 - val_loss: 3477.9417 - val_mae: 44.4849\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 296ms/step - loss: 532.2343 - mae: 21.3367 - val_loss: 3231.5991 - val_mae: 50.8892\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 305ms/step - loss: 1400.5046 - mae: 36.7436 - val_loss: 3110.4658 - val_mae: 51.6438\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 310ms/step - loss: 1675.9694 - mae: 40.4281 - val_loss: 3037.9119 - val_mae: 50.9647\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 304ms/step - loss: 1571.3306 - mae: 39.1990 - val_loss: 3046.3967 - val_mae: 49.7988\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 312ms/step - loss: 1283.3066 - mae: 35.3215 - val_loss: 3215.9749 - val_mae: 48.6902\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 303ms/step - loss: 907.5311 - mae: 29.4654 - val_loss: 3565.8054 - val_mae: 47.3693\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 326ms/step - loss: 518.2346 - mae: 21.7230 - val_loss: 4158.9565 - val_mae: 45.6978\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 322ms/step - loss: 202.6268 - mae: 12.1099 - val_loss: 5046.2681 - val_mae: 55.8914\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 304ms/step - loss: 71.1699 - mae: 7.4498 - val_loss: 6141.4707 - val_mae: 66.1274\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 311ms/step - loss: 181.2915 - mae: 11.3870 - val_loss: 7042.9585 - val_mae: 73.0866\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 303ms/step - loss: 388.8538 - mae: 17.2628 - val_loss: 7431.0386 - val_mae: 75.4493\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 308ms/step - loss: 462.3289 - mae: 19.4256 - val_loss: 7276.6875 - val_mae: 73.6591\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 363.6764 - mae: 17.0643 - val_loss: 6784.7915 - val_mae: 69.2137\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 303ms/step - loss: 203.5269 - mae: 12.0499 - val_loss: 6191.6040 - val_mae: 63.7742\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 304ms/step - loss: 86.9311 - mae: 8.1935 - val_loss: 5654.8535 - val_mae: 58.5419\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 340ms/step - loss: 44.5882 - mae: 6.2183 - val_loss: 5241.5957 - val_mae: 54.2370\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 321ms/step - loss: 57.2413 - mae: 5.4375 - val_loss: 4954.1816 - val_mae: 50.9876\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 325ms/step - loss: 94.1832 - mae: 7.6396 - val_loss: 4787.1562 - val_mae: 48.9448\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 392ms/step - loss: 128.5474 - mae: 9.6204 - val_loss: 4727.8604 - val_mae: 49.2488\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 322ms/step - loss: 145.6196 - mae: 10.5423 - val_loss: 4764.1079 - val_mae: 49.4605\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 328ms/step - loss: 141.0594 - mae: 10.3762 - val_loss: 4885.9507 - val_mae: 49.6061\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 313ms/step - loss: 118.5136 - mae: 9.2700 - val_loss: 5083.8584 - val_mae: 51.1159\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 336ms/step - loss: 86.6842 - mae: 7.5343 - val_loss: 5345.2114 - val_mae: 53.5372\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 323ms/step - loss: 56.1752 - mae: 5.8415 - val_loss: 5649.8052 - val_mae: 56.2568\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 370ms/step - loss: 36.0318 - mae: 4.6820 - val_loss: 5970.0620 - val_mae: 58.9909\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 30.8570 - mae: 4.7875 - val_loss: 6270.3672 - val_mae: 61.4404\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 417ms/step - loss: 39.1894 - mae: 5.5396 - val_loss: 6511.9829 - val_mae: 63.3279\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 425ms/step - loss: 54.0914 - mae: 6.5210 - val_loss: 6662.4668 - val_mae: 64.4490\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 360ms/step - loss: 66.5697 - mae: 7.1042 - val_loss: 6705.9907 - val_mae: 64.7458\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 316ms/step - loss: 70.1226 - mae: 7.2446 - val_loss: 6645.5469 - val_mae: 64.2334\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 312ms/step - loss: 63.5741 - mae: 6.9613 - val_loss: 6502.9482 - val_mae: 63.0676\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 326ms/step - loss: 50.7448 - mae: 6.3385 - val_loss: 6310.2881 - val_mae: 61.4759\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 312ms/step - loss: 37.7909 - mae: 5.5037 - val_loss: 6100.5728 - val_mae: 59.7015\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 313ms/step - loss: 29.2289 - mae: 4.5754 - val_loss: 5901.5879 - val_mae: 57.9696\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 319ms/step - loss: 26.8019 - mae: 4.1319 - val_loss: 5732.9961 - val_mae: 56.4585\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 315ms/step - loss: 29.3442 - mae: 4.1903 - val_loss: 5606.3042 - val_mae: 55.2894\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 317ms/step - loss: 34.1935 - mae: 4.6380 - val_loss: 5526.5913 - val_mae: 54.5301\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 317ms/step - loss: 38.5249 - mae: 4.9883 - val_loss: 5494.2378 - val_mae: 54.2007\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 331ms/step - loss: 40.3647 - mae: 5.1197 - val_loss: 5506.1997 - val_mae: 54.2802\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 314ms/step - loss: 38.9789 - mae: 5.0245 - val_loss: 5556.5645 - val_mae: 54.7201\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 331ms/step - loss: 34.7897 - mae: 4.6858 - val_loss: 5638.0586 - val_mae: 55.6283\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 303ms/step - loss: 29.8161 - mae: 4.1745 - val_loss: 5737.5410 - val_mae: 56.7365\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 297ms/step - loss: 26.6983 - mae: 4.0087 - val_loss: 5839.5029 - val_mae: 57.8447\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 330ms/step - loss: 26.7096 - mae: 4.1615 - val_loss: 5926.1816 - val_mae: 58.6886\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 326ms/step - loss: 28.5981 - mae: 4.5990 - val_loss: 5984.5508 - val_mae: 59.1280\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 296ms/step - loss: 29.8298 - mae: 4.8089 - val_loss: 6010.8149 - val_mae: 59.1812\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 293ms/step - loss: 29.3115 - mae: 4.7516 - val_loss: 6008.5474 - val_mae: 58.9279\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 293ms/step - loss: 27.4822 - mae: 4.5002 - val_loss: 5985.8735 - val_mae: 58.4738\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 293ms/step - loss: 25.3798 - mae: 4.1700 - val_loss: 5953.7378 - val_mae: 58.0032\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 292ms/step - loss: 23.8585 - mae: 3.9422 - val_loss: 5922.7202 - val_mae: 57.7140\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 293ms/step - loss: 23.3241 - mae: 3.8200 - val_loss: 5895.4385 - val_mae: 57.4568\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 23.3933 - mae: 3.7478 - val_loss: 5874.9663 - val_mae: 57.2588\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 23.5237 - mae: 3.6869 - val_loss: 5863.2959 - val_mae: 57.1382\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 318ms/step - loss: 23.6122 - mae: 3.6611 - val_loss: 5861.2744 - val_mae: 57.1029\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 315ms/step - loss: 23.5844 - mae: 3.6497 - val_loss: 5868.6733 - val_mae: 57.1508\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 303ms/step - loss: 23.3772 - mae: 3.6350 - val_loss: 5884.2925 - val_mae: 57.2712\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 23.0501 - mae: 3.6211 - val_loss: 5906.2031 - val_mae: 57.4468\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 293ms/step - loss: 22.6895 - mae: 3.6448 - val_loss: 5932.0005 - val_mae: 57.6562\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 310ms/step - loss: 22.3577 - mae: 3.6768 - val_loss: 5959.0796 - val_mae: 57.8764\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 307ms/step - loss: 22.0933 - mae: 3.7106 - val_loss: 5984.9185 - val_mae: 58.0855\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 307ms/step - loss: 21.9001 - mae: 3.7402 - val_loss: 6007.3467 - val_mae: 58.2651\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 21.7491 - mae: 3.7600 - val_loss: 6024.7808 - val_mae: 58.4016\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 348ms/step - loss: 21.6033 - mae: 3.7665 - val_loss: 6036.3535 - val_mae: 58.4878\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 454ms/step - loss: 21.4278 - mae: 3.7579 - val_loss: 6041.9785 - val_mae: 58.5231\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 1s 511ms/step - loss: 21.2857 - mae: 3.7404 - val_loss: 6042.2124 - val_mae: 58.5125\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 394ms/step - loss: 21.1373 - mae: 3.7154 - val_loss: 6037.9780 - val_mae: 58.4636\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 318ms/step - loss: 20.9190 - mae: 3.6787 - val_loss: 6030.5229 - val_mae: 58.3872\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 336ms/step - loss: 20.6474 - mae: 3.6331 - val_loss: 6021.2388 - val_mae: 58.2949\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 433ms/step - loss: 20.3460 - mae: 3.5827 - val_loss: 6011.4702 - val_mae: 58.1985\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 328ms/step - loss: 20.1405 - mae: 3.5384 - val_loss: 6002.3823 - val_mae: 58.1078\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 335ms/step - loss: 19.9647 - mae: 3.4957 - val_loss: 5994.9619 - val_mae: 58.0318\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 329ms/step - loss: 19.7955 - mae: 3.4551 - val_loss: 5989.9902 - val_mae: 57.9770\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 338ms/step - loss: 19.6322 - mae: 3.4192 - val_loss: 5987.9443 - val_mae: 57.9477\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 330ms/step - loss: 19.4633 - mae: 3.3891 - val_loss: 5988.9834 - val_mae: 57.9453\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 347ms/step - loss: 19.2825 - mae: 3.3654 - val_loss: 5992.9541 - val_mae: 57.9685\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 334ms/step - loss: 19.0882 - mae: 3.3477 - val_loss: 5999.4482 - val_mae: 58.0138\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 330ms/step - loss: 18.8845 - mae: 3.3349 - val_loss: 6007.8389 - val_mae: 58.0756\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 333ms/step - loss: 18.6738 - mae: 3.3256 - val_loss: 6017.3994 - val_mae: 58.1477\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 328ms/step - loss: 18.4604 - mae: 3.3179 - val_loss: 6027.3779 - val_mae: 58.2235\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 18.2466 - mae: 3.3097 - val_loss: 6037.0742 - val_mae: 58.2968\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 306ms/step - loss: 18.0856 - mae: 3.3043 - val_loss: 6045.8784 - val_mae: 58.3626\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 311ms/step - loss: 17.8849 - mae: 3.2992 - val_loss: 6053.0840 - val_mae: 58.4148\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 349ms/step - loss: 17.7052 - mae: 3.2976 - val_loss: 6057.9370 - val_mae: 58.4468\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 321ms/step - loss: 17.5480 - mae: 3.2905 - val_loss: 6060.0869 - val_mae: 58.4552\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 314ms/step - loss: 17.3719 - mae: 3.2735 - val_loss: 6059.6562 - val_mae: 58.4412\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 316ms/step - loss: 17.1706 - mae: 3.2465 - val_loss: 6057.2075 - val_mae: 58.4100\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 333ms/step - loss: 16.9540 - mae: 3.2142 - val_loss: 6053.5278 - val_mae: 58.3684\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 321ms/step - loss: 16.7373 - mae: 3.1748 - val_loss: 6049.6064 - val_mae: 58.3245\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 411ms/step - loss: 16.5648 - mae: 3.1414 - val_loss: 6046.1572 - val_mae: 58.2845\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 412ms/step - loss: 16.3702 - mae: 3.1097 - val_loss: 6043.6538 - val_mae: 58.2532\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 361ms/step - loss: 16.1682 - mae: 3.0828 - val_loss: 6042.2388 - val_mae: 58.2321\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 334ms/step - loss: 15.9863 - mae: 3.0559 - val_loss: 6042.0327 - val_mae: 58.2223\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 354ms/step - loss: 15.7922 - mae: 3.0265 - val_loss: 6043.1426 - val_mae: 58.2245\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 15.5868 - mae: 2.9955 - val_loss: 6045.6328 - val_mae: 58.2393\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 353ms/step - loss: 15.4196 - mae: 2.9720 - val_loss: 6049.4067 - val_mae: 58.2659\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 337ms/step - loss: 15.2173 - mae: 2.9548 - val_loss: 6054.0107 - val_mae: 58.3003\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 347ms/step - loss: 15.0006 - mae: 2.9420 - val_loss: 6058.7383 - val_mae: 58.3361\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 356ms/step - loss: 14.8128 - mae: 2.9294 - val_loss: 6063.0229 - val_mae: 58.3684\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "Predicted GDP for Dnipropetrovsk_Oblast in 2021: [91.01775]\n",
      "Actual GDP for Dnipropetrovsk_Oblast in 2021: 86.8125\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - 1s 505ms/step - loss: 2395.7456 - mae: 42.4717 - val_loss: 7267.9404 - val_mae: 82.0003\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 326ms/step - loss: 2029.2078 - mae: 39.5681 - val_loss: 6098.3374 - val_mae: 74.5115\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 332ms/step - loss: 1540.4193 - mae: 35.8990 - val_loss: 4894.6523 - val_mae: 65.9420\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 308ms/step - loss: 1120.5330 - mae: 31.7101 - val_loss: 3852.4204 - val_mae: 57.5183\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 310ms/step - loss: 846.9097 - mae: 27.5814 - val_loss: 3034.8328 - val_mae: 49.9568\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 307ms/step - loss: 715.3555 - mae: 24.8223 - val_loss: 2433.1379 - val_mae: 43.5738\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 306ms/step - loss: 686.4862 - mae: 23.2285 - val_loss: 2007.0417 - val_mae: 38.4461\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 716.2228 - mae: 21.9515 - val_loss: 1703.4159 - val_mae: 34.3716\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 306ms/step - loss: 770.2289 - mae: 20.9423 - val_loss: 1495.1511 - val_mae: 31.2611\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 829.8036 - mae: 20.1542 - val_loss: 1352.4596 - val_mae: 28.9290\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 309ms/step - loss: 884.5188 - mae: 19.5463 - val_loss: 1257.0045 - val_mae: 27.2592\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 304ms/step - loss: 928.2014 - mae: 19.1037 - val_loss: 1196.4486 - val_mae: 26.1446\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 958.7847 - mae: 18.8005 - val_loss: 1163.6263 - val_mae: 25.5166\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 317ms/step - loss: 975.1832 - mae: 18.6812 - val_loss: 1153.6858 - val_mae: 25.3174\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 977.8821 - mae: 18.6649 - val_loss: 1163.6686 - val_mae: 25.4989\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 305ms/step - loss: 967.8537 - mae: 18.6166 - val_loss: 1191.8544 - val_mae: 26.0218\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 315ms/step - loss: 946.4517 - mae: 18.6507 - val_loss: 1237.4226 - val_mae: 26.8536\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 409ms/step - loss: 915.8298 - mae: 18.8147 - val_loss: 1300.5247 - val_mae: 27.9651\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 309ms/step - loss: 878.2257 - mae: 19.0415 - val_loss: 1380.9010 - val_mae: 29.3265\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 307ms/step - loss: 836.1116 - mae: 19.3274 - val_loss: 1478.5166 - val_mae: 30.9026\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 305ms/step - loss: 792.2108 - mae: 19.6608 - val_loss: 1592.1500 - val_mae: 32.6513\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 749.0021 - mae: 20.0302 - val_loss: 1721.5156 - val_mae: 34.5432\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 708.8527 - mae: 20.4303 - val_loss: 1870.7395 - val_mae: 36.5921\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 673.8020 - mae: 20.8485 - val_loss: 2032.4382 - val_mae: 38.6942\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 312ms/step - loss: 645.4933 - mae: 21.2731 - val_loss: 2202.0625 - val_mae: 40.7878\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 625.0812 - mae: 21.6901 - val_loss: 2373.3840 - val_mae: 42.8047\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 299ms/step - loss: 612.2994 - mae: 22.0748 - val_loss: 2539.7483 - val_mae: 44.6790\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 307ms/step - loss: 606.1857 - mae: 22.4092 - val_loss: 2693.3215 - val_mae: 46.3454\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 605.1301 - mae: 22.6798 - val_loss: 2826.2734 - val_mae: 47.7467\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 304ms/step - loss: 606.8771 - mae: 22.8712 - val_loss: 2931.3770 - val_mae: 48.8304\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 298ms/step - loss: 608.6074 - mae: 22.9689 - val_loss: 3003.1377 - val_mae: 49.5615\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 311ms/step - loss: 607.9614 - mae: 22.9671 - val_loss: 3038.8271 - val_mae: 49.9295\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 305ms/step - loss: 603.3274 - mae: 22.8676 - val_loss: 3038.2153 - val_mae: 49.9388\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 305ms/step - loss: 594.0381 - mae: 22.6799 - val_loss: 3004.1582 - val_mae: 49.6202\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 304ms/step - loss: 579.6967 - mae: 22.4039 - val_loss: 2941.2122 - val_mae: 49.0111\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 560.6716 - mae: 22.0420 - val_loss: 2855.3835 - val_mae: 48.1603\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 306ms/step - loss: 537.9648 - mae: 21.6041 - val_loss: 2753.4695 - val_mae: 47.1262\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 308ms/step - loss: 513.0003 - mae: 21.1055 - val_loss: 2641.9956 - val_mae: 45.9662\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 304ms/step - loss: 486.8808 - mae: 20.5573 - val_loss: 2526.8303 - val_mae: 44.7360\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 307ms/step - loss: 460.2701 - mae: 19.9663 - val_loss: 2412.6482 - val_mae: 43.4828\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 433.5408 - mae: 19.3390 - val_loss: 2303.0693 - val_mae: 42.2484\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 298ms/step - loss: 406.5574 - mae: 18.6758 - val_loss: 2201.3247 - val_mae: 41.0873\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 371ms/step - loss: 378.9995 - mae: 17.9758 - val_loss: 2107.9048 - val_mae: 40.0013\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 391ms/step - loss: 350.2301 - mae: 17.2341 - val_loss: 2023.0874 - val_mae: 38.9977\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 338ms/step - loss: 319.7599 - mae: 16.4388 - val_loss: 1946.5790 - val_mae: 38.0785\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 304ms/step - loss: 287.3638 - mae: 15.5786 - val_loss: 1877.5239 - val_mae: 37.2373\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 312ms/step - loss: 252.9579 - mae: 14.6314 - val_loss: 1814.8990 - val_mae: 36.4661\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 217.2467 - mae: 13.5932 - val_loss: 1757.2174 - val_mae: 35.7479\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 181.6920 - mae: 12.4505 - val_loss: 1702.6832 - val_mae: 35.0573\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 148.0141 - mae: 11.1804 - val_loss: 1648.9978 - val_mae: 34.3477\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 118.3061 - mae: 9.7832 - val_loss: 1594.4259 - val_mae: 33.5858\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 94.4865 - mae: 8.2752 - val_loss: 1537.7628 - val_mae: 32.7484\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 77.4326 - mae: 6.9436 - val_loss: 1477.5474 - val_mae: 31.7774\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 299ms/step - loss: 65.7214 - mae: 6.1711 - val_loss: 1413.7363 - val_mae: 30.6379\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 56.3901 - mae: 5.6753 - val_loss: 1348.3185 - val_mae: 29.3436\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 46.5553 - mae: 5.5125 - val_loss: 1284.9185 - val_mae: 27.9528\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 279ms/step - loss: 35.6697 - mae: 4.8604 - val_loss: 1226.7789 - val_mae: 26.5479\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 26.0061 - mae: 3.8975 - val_loss: 1175.7454 - val_mae: 25.2043\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 20.3914 - mae: 3.1444 - val_loss: 1133.0021 - val_mae: 23.9971\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 19.7552 - mae: 3.0725 - val_loss: 1098.6168 - val_mae: 23.8861\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 293ms/step - loss: 22.4835 - mae: 3.4857 - val_loss: 1071.9194 - val_mae: 24.0798\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 26.0799 - mae: 4.0518 - val_loss: 1051.8988 - val_mae: 24.1798\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 28.3347 - mae: 4.3143 - val_loss: 1037.5426 - val_mae: 24.1961\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 328ms/step - loss: 28.2474 - mae: 4.2785 - val_loss: 1027.9175 - val_mae: 24.1433\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 26.1155 - mae: 3.9852 - val_loss: 1022.1715 - val_mae: 24.0383\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 23.0774 - mae: 3.7140 - val_loss: 1019.4656 - val_mae: 23.8939\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 281ms/step - loss: 20.2996 - mae: 3.4686 - val_loss: 1019.0734 - val_mae: 23.7361\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 18.3267 - mae: 3.4760 - val_loss: 1020.2501 - val_mae: 23.5773\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 16.9138 - mae: 3.4730 - val_loss: 1022.3696 - val_mae: 23.4290\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 15.3750 - mae: 3.3614 - val_loss: 1024.9907 - val_mae: 23.2988\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 280ms/step - loss: 13.2953 - mae: 3.1429 - val_loss: 1027.8800 - val_mae: 23.1884\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 10.8401 - mae: 2.8450 - val_loss: 1030.9984 - val_mae: 23.0951\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 8.4237 - mae: 2.4971 - val_loss: 1034.4595 - val_mae: 23.0139\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 6.4869 - mae: 2.1310 - val_loss: 1038.4506 - val_mae: 22.9458\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 276ms/step - loss: 5.2455 - mae: 1.8815 - val_loss: 1043.1359 - val_mae: 22.8825\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 4.5436 - mae: 1.7662 - val_loss: 1048.6256 - val_mae: 22.9750\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 280ms/step - loss: 4.1102 - mae: 1.6408 - val_loss: 1054.9380 - val_mae: 23.1710\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 3.7044 - mae: 1.6026 - val_loss: 1061.9780 - val_mae: 23.3802\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 3.2020 - mae: 1.5306 - val_loss: 1069.5438 - val_mae: 23.5988\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 2.6349 - mae: 1.3938 - val_loss: 1077.3445 - val_mae: 23.8198\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 2.1262 - mae: 1.2498 - val_loss: 1085.0236 - val_mae: 24.0332\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 276ms/step - loss: 1.7938 - mae: 1.1452 - val_loss: 1092.2026 - val_mae: 24.2284\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 279ms/step - loss: 1.6849 - mae: 1.0438 - val_loss: 1098.5013 - val_mae: 24.3961\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 280ms/step - loss: 1.7467 - mae: 1.0459 - val_loss: 1103.6813 - val_mae: 24.5300\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 285ms/step - loss: 1.8710 - mae: 1.0783 - val_loss: 1107.5846 - val_mae: 24.6266\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 292ms/step - loss: 1.9438 - mae: 1.0765 - val_loss: 1110.1798 - val_mae: 24.6860\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 297ms/step - loss: 1.9098 - mae: 1.0531 - val_loss: 1111.5620 - val_mae: 24.7116\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 334ms/step - loss: 1.7879 - mae: 0.9894 - val_loss: 1111.9227 - val_mae: 24.7091\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 1.6437 - mae: 0.8974 - val_loss: 1111.5138 - val_mae: 24.6864\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 1.5365 - mae: 0.8661 - val_loss: 1110.5992 - val_mae: 24.6508\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 1.4899 - mae: 0.8536 - val_loss: 1109.4196 - val_mae: 24.6096\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 281ms/step - loss: 1.4818 - mae: 0.8988 - val_loss: 1108.1685 - val_mae: 24.5697\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 330ms/step - loss: 1.4692 - mae: 0.9250 - val_loss: 1106.9628 - val_mae: 24.5334\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 434ms/step - loss: 1.4191 - mae: 0.9191 - val_loss: 1105.8488 - val_mae: 24.5016\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 358ms/step - loss: 1.3195 - mae: 0.8859 - val_loss: 1104.8124 - val_mae: 24.4734\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 361ms/step - loss: 1.1828 - mae: 0.8235 - val_loss: 1103.7898 - val_mae: 24.4468\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 345ms/step - loss: 1.0427 - mae: 0.7406 - val_loss: 1102.6920 - val_mae: 24.4188\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 354ms/step - loss: 0.9289 - mae: 0.6488 - val_loss: 1101.4261 - val_mae: 24.3861\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 372ms/step - loss: 0.8523 - mae: 0.6383 - val_loss: 1099.9177 - val_mae: 24.3462\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 487ms/step - loss: 0.8029 - mae: 0.6613 - val_loss: 1098.1267 - val_mae: 24.2974\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "Predicted GDP for Donetsk_Oblast in 2021: [41.050877]\n",
      "Actual GDP for Donetsk_Oblast in 2021: 37.46875\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - 1s 713ms/step - loss: 4840.9438 - mae: 69.2014 - val_loss: 3822.8650 - val_mae: 60.6086\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 441ms/step - loss: 4565.4722 - mae: 67.2347 - val_loss: 3749.6895 - val_mae: 60.5593\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 414ms/step - loss: 4077.4272 - mae: 63.5739 - val_loss: 3674.8245 - val_mae: 60.4738\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 356ms/step - loss: 3396.9663 - mae: 58.0650 - val_loss: 3643.6763 - val_mae: 60.3084\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 378ms/step - loss: 2547.0986 - mae: 50.3219 - val_loss: 3731.1362 - val_mae: 59.9788\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 1s 541ms/step - loss: 1594.1853 - mae: 39.8204 - val_loss: 4043.3508 - val_mae: 59.3710\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 448ms/step - loss: 703.0817 - mae: 26.0938 - val_loss: 4727.4795 - val_mae: 58.2801\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 162.7443 - mae: 11.4063 - val_loss: 5922.1963 - val_mae: 56.8305\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 416ms/step - loss: 363.2369 - mae: 13.3616 - val_loss: 7326.9946 - val_mae: 64.6156\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 395ms/step - loss: 974.2075 - mae: 25.6775 - val_loss: 8230.0039 - val_mae: 70.8553\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 416ms/step - loss: 1145.5356 - mae: 30.0461 - val_loss: 8410.0391 - val_mae: 71.2216\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 410ms/step - loss: 839.2744 - mae: 26.6868 - val_loss: 8055.5093 - val_mae: 67.7047\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 399ms/step - loss: 428.9373 - mae: 19.4577 - val_loss: 7454.7368 - val_mae: 62.2133\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 410ms/step - loss: 149.7155 - mae: 11.3196 - val_loss: 6838.6919 - val_mae: 60.5108\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 1s 522ms/step - loss: 26.7973 - mae: 4.6057 - val_loss: 6325.2134 - val_mae: 60.8647\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 404ms/step - loss: 25.2378 - mae: 4.0153 - val_loss: 5953.9751 - val_mae: 61.0781\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 369ms/step - loss: 81.5048 - mae: 7.8258 - val_loss: 5724.6509 - val_mae: 61.1920\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 359ms/step - loss: 142.6910 - mae: 10.9272 - val_loss: 5624.1743 - val_mae: 61.2701\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 333ms/step - loss: 180.1525 - mae: 12.4521 - val_loss: 5637.9419 - val_mae: 61.3381\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 329ms/step - loss: 184.6463 - mae: 12.5856 - val_loss: 5756.2910 - val_mae: 61.4086\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 322ms/step - loss: 158.7376 - mae: 11.4890 - val_loss: 5972.7437 - val_mae: 61.4847\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 406ms/step - loss: 114.2755 - mae: 9.3572 - val_loss: 6280.1035 - val_mae: 61.5692\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 436ms/step - loss: 67.2736 - mae: 6.5188 - val_loss: 6664.5386 - val_mae: 61.6604\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 373ms/step - loss: 33.9445 - mae: 5.0865 - val_loss: 7098.1289 - val_mae: 61.7482\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 354ms/step - loss: 25.4289 - mae: 4.7644 - val_loss: 7516.9043 - val_mae: 61.8162\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 363ms/step - loss: 41.9791 - mae: 5.0068 - val_loss: 7816.0161 - val_mae: 63.1723\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 370ms/step - loss: 67.1764 - mae: 6.6372 - val_loss: 7951.7031 - val_mae: 64.2358\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 387ms/step - loss: 82.4137 - mae: 7.7358 - val_loss: 7933.1934 - val_mae: 64.1089\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 343ms/step - loss: 80.7592 - mae: 7.6482 - val_loss: 7793.6079 - val_mae: 63.0390\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 331ms/step - loss: 65.9074 - mae: 6.6122 - val_loss: 7578.7446 - val_mae: 61.7700\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 359ms/step - loss: 46.7468 - mae: 5.3050 - val_loss: 7334.3081 - val_mae: 61.7354\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 345ms/step - loss: 31.3429 - mae: 4.5840 - val_loss: 7096.4155 - val_mae: 61.7021\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 362ms/step - loss: 23.5607 - mae: 4.5417 - val_loss: 6888.8374 - val_mae: 61.6717\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 347ms/step - loss: 23.0410 - mae: 4.5419 - val_loss: 6724.6807 - val_mae: 61.6446\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 333ms/step - loss: 26.8796 - mae: 4.5773 - val_loss: 6606.1289 - val_mae: 61.6192\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 425ms/step - loss: 31.7039 - mae: 4.7835 - val_loss: 6534.3447 - val_mae: 61.5940\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 451ms/step - loss: 34.7797 - mae: 4.8482 - val_loss: 6508.1128 - val_mae: 61.5684\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 378ms/step - loss: 34.5992 - mae: 4.7682 - val_loss: 6523.0674 - val_mae: 61.5418\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 349ms/step - loss: 31.1577 - mae: 4.5509 - val_loss: 6572.2412 - val_mae: 61.5129\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 321ms/step - loss: 25.6839 - mae: 4.2112 - val_loss: 6646.3789 - val_mae: 61.4793\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 321ms/step - loss: 20.0231 - mae: 3.7723 - val_loss: 6734.3066 - val_mae: 61.4409\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 324ms/step - loss: 15.9452 - mae: 3.5053 - val_loss: 6823.4907 - val_mae: 61.3975\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 322ms/step - loss: 14.4854 - mae: 3.4821 - val_loss: 6901.2695 - val_mae: 61.3505\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 306ms/step - loss: 15.5130 - mae: 3.6749 - val_loss: 6956.4497 - val_mae: 61.3039\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 318ms/step - loss: 17.8823 - mae: 3.8103 - val_loss: 6966.3003 - val_mae: 61.2660\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 326ms/step - loss: 19.1167 - mae: 3.8460 - val_loss: 6923.1216 - val_mae: 61.2321\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 371ms/step - loss: 17.8962 - mae: 3.7308 - val_loss: 6823.7427 - val_mae: 61.2048\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 344ms/step - loss: 14.7280 - mae: 3.4880 - val_loss: 6670.9199 - val_mae: 61.1702\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 333ms/step - loss: 12.4520 - mae: 3.1222 - val_loss: 6500.7495 - val_mae: 61.1254\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 336ms/step - loss: 14.2838 - mae: 2.7024 - val_loss: 6452.7271 - val_mae: 61.0870\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 316ms/step - loss: 15.0203 - mae: 2.6560 - val_loss: 6492.6060 - val_mae: 61.0544\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 334ms/step - loss: 13.1224 - mae: 2.6530 - val_loss: 6568.1680 - val_mae: 61.0283\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 318ms/step - loss: 11.6572 - mae: 2.8170 - val_loss: 6638.7842 - val_mae: 61.0064\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 307ms/step - loss: 11.7958 - mae: 2.9689 - val_loss: 6679.7300 - val_mae: 60.9875\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 308ms/step - loss: 12.4833 - mae: 3.0478 - val_loss: 6684.2437 - val_mae: 60.9706\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 308ms/step - loss: 12.5522 - mae: 3.0594 - val_loss: 6656.1313 - val_mae: 60.9540\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 308ms/step - loss: 11.8083 - mae: 2.9493 - val_loss: 6604.0186 - val_mae: 60.9353\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 310ms/step - loss: 10.8111 - mae: 2.8010 - val_loss: 6540.4287 - val_mae: 60.9135\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 308ms/step - loss: 10.2497 - mae: 2.6232 - val_loss: 6481.7129 - val_mae: 60.8890\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 315ms/step - loss: 10.3123 - mae: 2.4568 - val_loss: 6444.6948 - val_mae: 60.8642\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 10.4848 - mae: 2.3420 - val_loss: 6438.4072 - val_mae: 60.8418\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 308ms/step - loss: 10.1857 - mae: 2.3020 - val_loss: 6459.2295 - val_mae: 60.8219\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 314ms/step - loss: 9.4710 - mae: 2.3284 - val_loss: 6494.5479 - val_mae: 60.8023\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 297ms/step - loss: 8.8353 - mae: 2.3902 - val_loss: 6529.2881 - val_mae: 60.7796\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 306ms/step - loss: 8.5701 - mae: 2.4503 - val_loss: 6550.8032 - val_mae: 60.7509\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 313ms/step - loss: 8.5203 - mae: 2.4769 - val_loss: 6551.2944 - val_mae: 60.7145\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 8.3485 - mae: 2.4778 - val_loss: 6528.3223 - val_mae: 60.6697\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 7.9960 - mae: 2.4007 - val_loss: 6477.0190 - val_mae: 60.6159\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 296ms/step - loss: 7.2767 - mae: 2.2232 - val_loss: 6406.8594 - val_mae: 60.5539\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 308ms/step - loss: 6.8048 - mae: 2.0176 - val_loss: 6343.5615 - val_mae: 60.4865\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 306ms/step - loss: 6.7553 - mae: 1.9015 - val_loss: 6303.6582 - val_mae: 60.4174\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 303ms/step - loss: 6.5635 - mae: 1.8939 - val_loss: 6293.2686 - val_mae: 60.3488\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 309ms/step - loss: 5.9354 - mae: 1.7680 - val_loss: 6304.3999 - val_mae: 60.2788\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 305ms/step - loss: 5.4883 - mae: 1.6777 - val_loss: 6307.6724 - val_mae: 60.2036\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 5.3058 - mae: 1.8058 - val_loss: 6289.1519 - val_mae: 60.1214\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 318ms/step - loss: 5.0689 - mae: 1.8331 - val_loss: 6243.7773 - val_mae: 60.0336\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 325ms/step - loss: 4.5767 - mae: 1.6955 - val_loss: 6177.4531 - val_mae: 59.9423\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 486ms/step - loss: 4.0188 - mae: 1.4780 - val_loss: 6105.7739 - val_mae: 59.8514\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 460ms/step - loss: 3.8832 - mae: 1.3710 - val_loss: 6066.3462 - val_mae: 59.7679\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 423ms/step - loss: 3.7367 - mae: 1.3537 - val_loss: 6063.6533 - val_mae: 59.6952\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 320ms/step - loss: 3.2298 - mae: 1.3508 - val_loss: 6081.2397 - val_mae: 59.6303\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 316ms/step - loss: 2.9438 - mae: 1.3610 - val_loss: 6079.7129 - val_mae: 59.5686\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 315ms/step - loss: 2.8322 - mae: 1.3468 - val_loss: 6049.5347 - val_mae: 59.5088\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 323ms/step - loss: 2.4372 - mae: 1.2655 - val_loss: 6000.5483 - val_mae: 59.4550\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 319ms/step - loss: 2.0469 - mae: 1.1454 - val_loss: 5956.2495 - val_mae: 59.4071\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 348ms/step - loss: 2.0056 - mae: 1.0940 - val_loss: 5953.4834 - val_mae: 59.3674\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 361ms/step - loss: 1.6935 - mae: 1.0360 - val_loss: 5983.8145 - val_mae: 59.3336\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 347ms/step - loss: 1.3990 - mae: 0.9554 - val_loss: 6007.1699 - val_mae: 59.3004\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 320ms/step - loss: 1.3514 - mae: 0.9509 - val_loss: 6002.1914 - val_mae: 59.2637\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 322ms/step - loss: 1.2077 - mae: 0.9003 - val_loss: 5968.8154 - val_mae: 59.2228\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 309ms/step - loss: 0.9355 - mae: 0.7764 - val_loss: 5927.0659 - val_mae: 59.1805\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 329ms/step - loss: 0.8293 - mae: 0.7862 - val_loss: 5902.5229 - val_mae: 59.1415\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 318ms/step - loss: 0.7756 - mae: 0.7536 - val_loss: 5905.2686 - val_mae: 59.1076\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 307ms/step - loss: 0.5709 - mae: 0.6604 - val_loss: 5921.6934 - val_mae: 59.0771\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 295ms/step - loss: 0.4415 - mae: 0.5741 - val_loss: 5927.5435 - val_mae: 59.0474\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 372ms/step - loss: 0.4328 - mae: 0.5672 - val_loss: 5909.3843 - val_mae: 59.0179\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 1s 544ms/step - loss: 0.3316 - mae: 0.5154 - val_loss: 5876.2881 - val_mae: 58.9916\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 0.2558 - mae: 0.4482 - val_loss: 5855.2441 - val_mae: 58.9727\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 342ms/step - loss: 0.2742 - mae: 0.4456 - val_loss: 5860.5786 - val_mae: 58.9636\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 327ms/step - loss: 0.2057 - mae: 0.4000 - val_loss: 5882.7314 - val_mae: 58.9625\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "Predicted GDP for Zhytomyr_Oblast in 2021: [104.06828]\n",
      "Actual GDP for Zhytomyr_Oblast in 2021: 121.0\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - 1s 666ms/step - loss: 58.2580 - mae: 7.0250 - val_loss: 5587.7051 - val_mae: 56.8323\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 360ms/step - loss: 29.7557 - mae: 4.5703 - val_loss: 5163.5293 - val_mae: 53.0425\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 340ms/step - loss: 9.7000 - mae: 2.4599 - val_loss: 4755.2998 - val_mae: 49.1230\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 344ms/step - loss: 16.7458 - mae: 3.4581 - val_loss: 4530.5386 - val_mae: 48.3510\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 365ms/step - loss: 34.0645 - mae: 5.1074 - val_loss: 4535.9736 - val_mae: 48.3669\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 359ms/step - loss: 33.8514 - mae: 5.0914 - val_loss: 4701.3496 - val_mae: 48.5515\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 315ms/step - loss: 20.3834 - mae: 3.8966 - val_loss: 4953.9512 - val_mae: 51.0300\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 328ms/step - loss: 9.8024 - mae: 2.8602 - val_loss: 5218.6035 - val_mae: 53.5065\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 336ms/step - loss: 10.4178 - mae: 2.4420 - val_loss: 5418.8550 - val_mae: 55.3067\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 314ms/step - loss: 17.8619 - mae: 3.2465 - val_loss: 5504.9741 - val_mae: 56.0600\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 341ms/step - loss: 22.6533 - mae: 3.7820 - val_loss: 5470.2920 - val_mae: 55.7490\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 323ms/step - loss: 20.3843 - mae: 3.5464 - val_loss: 5342.0791 - val_mae: 54.6015\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 339ms/step - loss: 13.9370 - mae: 2.8321 - val_loss: 5165.2637 - val_mae: 52.9823\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 324ms/step - loss: 9.0983 - mae: 2.4233 - val_loss: 4989.3989 - val_mae: 51.3234\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 328ms/step - loss: 9.2471 - mae: 2.7808 - val_loss: 4857.9946 - val_mae: 50.0489\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 320ms/step - loss: 12.8621 - mae: 3.0837 - val_loss: 4798.3140 - val_mae: 49.4587\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 323ms/step - loss: 15.5697 - mae: 3.3193 - val_loss: 4815.7573 - val_mae: 49.6302\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 309ms/step - loss: 14.7157 - mae: 3.2358 - val_loss: 4896.7710 - val_mae: 50.4240\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 315ms/step - loss: 11.4514 - mae: 2.9829 - val_loss: 5015.6167 - val_mae: 51.5674\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 314ms/step - loss: 8.7824 - mae: 2.7047 - val_loss: 5140.7847 - val_mae: 52.7456\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 321ms/step - loss: 8.5482 - mae: 2.4180 - val_loss: 5241.9023 - val_mae: 53.6794\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 333ms/step - loss: 10.1717 - mae: 2.3887 - val_loss: 5297.1572 - val_mae: 54.1839\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 321ms/step - loss: 11.6991 - mae: 2.5750 - val_loss: 5298.3145 - val_mae: 54.1962\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 1s 591ms/step - loss: 11.7037 - mae: 2.5772 - val_loss: 5251.1265 - val_mae: 53.7692\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 349ms/step - loss: 10.3026 - mae: 2.3781 - val_loss: 5171.9004 - val_mae: 53.0436\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 343ms/step - loss: 8.7276 - mae: 2.3737 - val_loss: 5082.2202 - val_mae: 52.2105\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 321ms/step - loss: 8.1314 - mae: 2.5179 - val_loss: 5003.5181 - val_mae: 51.4694\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 328ms/step - loss: 8.6786 - mae: 2.6900 - val_loss: 4952.3359 - val_mae: 50.9837\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 324ms/step - loss: 9.5759 - mae: 2.8008 - val_loss: 4937.1191 - val_mae: 50.8418\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 326ms/step - loss: 9.8931 - mae: 2.8290 - val_loss: 4957.2866 - val_mae: 51.0410\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 345ms/step - loss: 9.3682 - mae: 2.7754 - val_loss: 5004.4888 - val_mae: 51.4979\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 331ms/step - loss: 8.4944 - mae: 2.6600 - val_loss: 5065.2446 - val_mae: 52.0789\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 332ms/step - loss: 7.9579 - mae: 2.5148 - val_loss: 5124.3018 - val_mae: 52.6381\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 343ms/step - loss: 8.0379 - mae: 2.3747 - val_loss: 5168.1616 - val_mae: 53.0513\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 331ms/step - loss: 8.4670 - mae: 2.3370 - val_loss: 5188.0723 - val_mae: 53.2413\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 342ms/step - loss: 8.7582 - mae: 2.3325 - val_loss: 5181.6606 - val_mae: 53.1884\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 319ms/step - loss: 8.6361 - mae: 2.3285 - val_loss: 5152.8091 - val_mae: 52.9280\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 331ms/step - loss: 8.2106 - mae: 2.3249 - val_loss: 5110.0806 - val_mae: 52.5370\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 323ms/step - loss: 7.8164 - mae: 2.3667 - val_loss: 5064.2314 - val_mae: 52.1141\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 315ms/step - loss: 7.7043 - mae: 2.4611 - val_loss: 5025.5801 - val_mae: 51.7563\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 299ms/step - loss: 7.8566 - mae: 2.5398 - val_loss: 5001.7471 - val_mae: 51.5375\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 339ms/step - loss: 8.0469 - mae: 2.5851 - val_loss: 4996.2617 - val_mae: 51.4930\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 334ms/step - loss: 8.0622 - mae: 2.5887 - val_loss: 5008.1875 - val_mae: 51.6145\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 316ms/step - loss: 7.8736 - mae: 2.5525 - val_loss: 5032.7983 - val_mae: 51.8560\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 323ms/step - loss: 7.6261 - mae: 2.4875 - val_loss: 5062.9912 - val_mae: 52.1491\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 306ms/step - loss: 7.4859 - mae: 2.4102 - val_loss: 5091.0508 - val_mae: 52.4207\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 296ms/step - loss: 7.5022 - mae: 2.3380 - val_loss: 5110.4673 - val_mae: 52.6105\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 7.5898 - mae: 2.2853 - val_loss: 5117.2954 - val_mae: 52.6826\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 299ms/step - loss: 7.6237 - mae: 2.2784 - val_loss: 5110.8579 - val_mae: 52.6312\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 310ms/step - loss: 7.5475 - mae: 2.2741 - val_loss: 5093.5562 - val_mae: 52.4783\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 305ms/step - loss: 7.4044 - mae: 2.2944 - val_loss: 5070.0249 - val_mae: 52.2666\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 307ms/step - loss: 7.2848 - mae: 2.3374 - val_loss: 5045.8398 - val_mae: 52.0480\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 310ms/step - loss: 7.2442 - mae: 2.3820 - val_loss: 5026.1777 - val_mae: 51.8714\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 306ms/step - loss: 7.2645 - mae: 2.4164 - val_loss: 5014.6616 - val_mae: 51.7716\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 311ms/step - loss: 7.2819 - mae: 2.4325 - val_loss: 5012.6973 - val_mae: 51.7624\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 319ms/step - loss: 7.2470 - mae: 2.4268 - val_loss: 5019.4097 - val_mae: 51.8355\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 304ms/step - loss: 7.1632 - mae: 2.4015 - val_loss: 5032.0103 - val_mae: 51.9643\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 322ms/step - loss: 7.0739 - mae: 2.3629 - val_loss: 5046.6299 - val_mae: 52.1118\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 316ms/step - loss: 7.0190 - mae: 2.3197 - val_loss: 5059.3091 - val_mae: 52.2408\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 322ms/step - loss: 7.0033 - mae: 2.2809 - val_loss: 5066.8872 - val_mae: 52.3218\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 310ms/step - loss: 6.9986 - mae: 2.2536 - val_loss: 5067.6870 - val_mae: 52.3392\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 6.9734 - mae: 2.2413 - val_loss: 5061.7520 - val_mae: 52.2934\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 6.9190 - mae: 2.2441 - val_loss: 5050.6836 - val_mae: 52.1994\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 308ms/step - loss: 6.8523 - mae: 2.2583 - val_loss: 5037.1099 - val_mae: 52.0818\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 308ms/step - loss: 6.7976 - mae: 2.2780 - val_loss: 5023.9790 - val_mae: 51.9681\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 309ms/step - loss: 6.7646 - mae: 2.2967 - val_loss: 5013.8574 - val_mae: 51.8828\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 316ms/step - loss: 6.7435 - mae: 2.3086 - val_loss: 5008.3423 - val_mae: 51.8410\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 307ms/step - loss: 6.7163 - mae: 2.3101 - val_loss: 5007.8267 - val_mae: 51.8467\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 299ms/step - loss: 6.6735 - mae: 2.3001 - val_loss: 5011.4814 - val_mae: 51.8920\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 6.6206 - mae: 2.2807 - val_loss: 5017.5894 - val_mae: 51.9606\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 308ms/step - loss: 6.5706 - mae: 2.2557 - val_loss: 5024.0088 - val_mae: 52.0321\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 308ms/step - loss: 6.5316 - mae: 2.2300 - val_loss: 5028.7422 - val_mae: 52.0877\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 6.5011 - mae: 2.2080 - val_loss: 5030.3608 - val_mae: 52.1139\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 312ms/step - loss: 6.4699 - mae: 2.1930 - val_loss: 5028.3032 - val_mae: 52.1056\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 303ms/step - loss: 6.4311 - mae: 2.1861 - val_loss: 5022.9341 - val_mae: 52.0661\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 6.3853 - mae: 2.1866 - val_loss: 5015.3672 - val_mae: 52.0059\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 313ms/step - loss: 6.3390 - mae: 2.1920 - val_loss: 5007.1343 - val_mae: 51.9395\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 334ms/step - loss: 6.2975 - mae: 2.1988 - val_loss: 4999.7612 - val_mae: 51.8811\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 305ms/step - loss: 6.2613 - mae: 2.2036 - val_loss: 4994.4351 - val_mae: 51.8422\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 308ms/step - loss: 6.2259 - mae: 2.2038 - val_loss: 4991.7363 - val_mae: 51.8282\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 6.1870 - mae: 2.1979 - val_loss: 4991.5586 - val_mae: 51.8381\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 307ms/step - loss: 6.1438 - mae: 2.1862 - val_loss: 4993.2114 - val_mae: 51.8655\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 315ms/step - loss: 6.0994 - mae: 2.1704 - val_loss: 4995.6128 - val_mae: 51.9000\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 299ms/step - loss: 6.0571 - mae: 2.1527 - val_loss: 4997.6162 - val_mae: 51.9307\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 297ms/step - loss: 6.0178 - mae: 2.1360 - val_loss: 4998.2739 - val_mae: 51.9488\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 5.9793 - mae: 2.1221 - val_loss: 4997.0439 - val_mae: 51.9490\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 304ms/step - loss: 5.9391 - mae: 2.1125 - val_loss: 4993.9004 - val_mae: 51.9312\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 325ms/step - loss: 5.8964 - mae: 2.1070 - val_loss: 4989.2832 - val_mae: 51.8995\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 317ms/step - loss: 5.8524 - mae: 2.1048 - val_loss: 4983.9448 - val_mae: 51.8611\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 305ms/step - loss: 5.8090 - mae: 2.1041 - val_loss: 4978.7476 - val_mae: 51.8240\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 5.7669 - mae: 2.1029 - val_loss: 4974.4272 - val_mae: 51.7954\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 5.7251 - mae: 2.0996 - val_loss: 4971.4438 - val_mae: 51.7796\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 305ms/step - loss: 5.6824 - mae: 2.0932 - val_loss: 4969.8574 - val_mae: 51.7772\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 305ms/step - loss: 5.6382 - mae: 2.0834 - val_loss: 4969.4067 - val_mae: 51.7857\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 307ms/step - loss: 5.5931 - mae: 2.0710 - val_loss: 4969.5566 - val_mae: 51.8000\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 5.5479 - mae: 2.0570 - val_loss: 4969.6699 - val_mae: 51.8141\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 5.5034 - mae: 2.0430 - val_loss: 4969.1641 - val_mae: 51.8224\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 322ms/step - loss: 5.4592 - mae: 2.0302 - val_loss: 4967.6592 - val_mae: 51.8214\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 298ms/step - loss: 5.4147 - mae: 2.0197 - val_loss: 4965.0498 - val_mae: 51.8100\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 298ms/step - loss: 5.3693 - mae: 2.0115 - val_loss: 4961.5361 - val_mae: 51.7901\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "Predicted GDP for Zakarpattia_Oblast in 2021: [100.77795]\n",
      "Actual GDP for Zakarpattia_Oblast in 2021: 97.9375\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - 0s 429ms/step - loss: 19.8327 - mae: 3.7300 - val_loss: 4883.3755 - val_mae: 51.9265\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 303ms/step - loss: 13.2627 - mae: 2.7370 - val_loss: 4746.9702 - val_mae: 50.4732\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 298ms/step - loss: 7.1157 - mae: 2.1892 - val_loss: 4602.8945 - val_mae: 48.8917\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 298ms/step - loss: 5.9667 - mae: 2.1681 - val_loss: 4486.7188 - val_mae: 47.5806\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 312ms/step - loss: 9.2336 - mae: 2.6955 - val_loss: 4422.1182 - val_mae: 47.1942\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 12.5345 - mae: 3.0522 - val_loss: 4416.4888 - val_mae: 47.1814\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 314ms/step - loss: 12.5072 - mae: 3.0443 - val_loss: 4462.9771 - val_mae: 47.3529\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 305ms/step - loss: 9.4918 - mae: 2.7230 - val_loss: 4544.9600 - val_mae: 48.2945\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 304ms/step - loss: 6.2119 - mae: 2.1967 - val_loss: 4639.8232 - val_mae: 49.3606\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 5.0486 - mae: 2.0368 - val_loss: 4723.0879 - val_mae: 50.2788\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 299ms/step - loss: 6.2264 - mae: 2.0162 - val_loss: 4774.2480 - val_mae: 50.8430\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 313ms/step - loss: 7.9939 - mae: 2.0290 - val_loss: 4782.4951 - val_mae: 50.9510\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 306ms/step - loss: 8.4575 - mae: 2.0776 - val_loss: 4749.0659 - val_mae: 50.6197\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 312ms/step - loss: 7.2158 - mae: 1.9435 - val_loss: 4685.2930 - val_mae: 49.9632\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 296ms/step - loss: 5.3694 - mae: 1.9010 - val_loss: 4608.0850 - val_mae: 49.1521\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 305ms/step - loss: 4.2993 - mae: 1.8673 - val_loss: 4535.1572 - val_mae: 48.3745\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 409ms/step - loss: 4.5000 - mae: 1.8350 - val_loss: 4480.9062 - val_mae: 47.7966\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 313ms/step - loss: 5.3522 - mae: 2.0799 - val_loss: 4453.7515 - val_mae: 47.5204\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 321ms/step - loss: 5.8356 - mae: 2.1769 - val_loss: 4455.1816 - val_mae: 47.5674\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 311ms/step - loss: 5.4391 - mae: 2.1043 - val_loss: 4480.3823 - val_mae: 47.8824\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 306ms/step - loss: 4.4748 - mae: 1.8910 - val_loss: 4519.8647 - val_mae: 48.3552\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 314ms/step - loss: 3.6638 - mae: 1.6834 - val_loss: 4561.7217 - val_mae: 48.8510\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 299ms/step - loss: 3.4781 - mae: 1.6564 - val_loss: 4594.3042 - val_mae: 49.2419\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 3.8036 - mae: 1.6288 - val_loss: 4608.9653 - val_mae: 49.4354\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 307ms/step - loss: 4.1338 - mae: 1.6000 - val_loss: 4602.0679 - val_mae: 49.3948\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 4.0581 - mae: 1.5699 - val_loss: 4575.5015 - val_mae: 49.1400\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 310ms/step - loss: 3.5837 - mae: 1.5386 - val_loss: 4535.6416 - val_mae: 48.7369\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 314ms/step - loss: 3.0523 - mae: 1.5066 - val_loss: 4491.2729 - val_mae: 48.2805\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 314ms/step - loss: 2.7951 - mae: 1.4743 - val_loss: 4451.2251 - val_mae: 47.8688\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 310ms/step - loss: 2.8608 - mae: 1.4796 - val_loss: 4422.3452 - val_mae: 47.5805\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 308ms/step - loss: 3.0268 - mae: 1.5457 - val_loss: 4408.2041 - val_mae: 47.4586\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 3.0326 - mae: 1.5591 - val_loss: 4408.6670 - val_mae: 47.5027\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 305ms/step - loss: 2.8043 - mae: 1.4852 - val_loss: 4420.3174 - val_mae: 47.6739\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 307ms/step - loss: 2.4812 - mae: 1.3822 - val_loss: 4437.5112 - val_mae: 47.9073\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 408ms/step - loss: 2.2614 - mae: 1.2988 - val_loss: 4453.7773 - val_mae: 48.1292\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 315ms/step - loss: 2.2244 - mae: 1.2721 - val_loss: 4463.4131 - val_mae: 48.2758\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 318ms/step - loss: 2.2847 - mae: 1.2453 - val_loss: 4462.8096 - val_mae: 48.3071\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 317ms/step - loss: 2.2925 - mae: 1.2179 - val_loss: 4451.2168 - val_mae: 48.2145\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 310ms/step - loss: 2.1733 - mae: 1.1900 - val_loss: 4430.6826 - val_mae: 48.0204\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 307ms/step - loss: 1.9783 - mae: 1.1618 - val_loss: 4405.2627 - val_mae: 47.7696\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 308ms/step - loss: 1.8174 - mae: 1.1335 - val_loss: 4379.8325 - val_mae: 47.5168\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 299ms/step - loss: 1.7547 - mae: 1.1412 - val_loss: 4358.7915 - val_mae: 47.3127\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 308ms/step - loss: 1.7601 - mae: 1.1619 - val_loss: 4345.0674 - val_mae: 47.1916\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 307ms/step - loss: 1.7527 - mae: 1.1609 - val_loss: 4339.5938 - val_mae: 47.1649\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 306ms/step - loss: 1.6799 - mae: 1.1354 - val_loss: 4341.2534 - val_mae: 47.2200\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 317ms/step - loss: 1.5584 - mae: 1.0887 - val_loss: 4347.3521 - val_mae: 47.3257\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 318ms/step - loss: 1.4466 - mae: 1.0290 - val_loss: 4354.3999 - val_mae: 47.4417\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 307ms/step - loss: 1.3864 - mae: 0.9671 - val_loss: 4358.9902 - val_mae: 47.5288\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 308ms/step - loss: 1.3685 - mae: 0.9306 - val_loss: 4358.7065 - val_mae: 47.5593\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 304ms/step - loss: 1.3499 - mae: 0.9185 - val_loss: 4352.6011 - val_mae: 47.5221\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 343ms/step - loss: 1.2975 - mae: 0.9033 - val_loss: 4341.3794 - val_mae: 47.4250\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 335ms/step - loss: 1.2166 - mae: 0.8712 - val_loss: 4327.0054 - val_mae: 47.2904\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 307ms/step - loss: 1.1390 - mae: 0.8619 - val_loss: 4312.1226 - val_mae: 47.1488\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 307ms/step - loss: 1.0893 - mae: 0.8724 - val_loss: 4299.2754 - val_mae: 47.0298\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 1.0643 - mae: 0.8775 - val_loss: 4290.2969 - val_mae: 46.9552\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 333ms/step - loss: 1.0403 - mae: 0.8712 - val_loss: 4285.9043 - val_mae: 46.9336\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 314ms/step - loss: 0.9985 - mae: 0.8513 - val_loss: 4285.6240 - val_mae: 46.9596\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 317ms/step - loss: 0.9414 - mae: 0.8193 - val_loss: 4288.0239 - val_mae: 47.0164\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 0.8863 - mae: 0.7837 - val_loss: 4291.1499 - val_mae: 47.0811\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 303ms/step - loss: 0.8469 - mae: 0.7718 - val_loss: 4293.0884 - val_mae: 47.1311\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 310ms/step - loss: 0.8213 - mae: 0.7597 - val_loss: 4292.4507 - val_mae: 47.1502\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 309ms/step - loss: 0.7964 - mae: 0.7475 - val_loss: 4288.7026 - val_mae: 47.1322\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 311ms/step - loss: 0.7621 - mae: 0.7352 - val_loss: 4282.2197 - val_mae: 47.0813\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 298ms/step - loss: 0.7202 - mae: 0.7228 - val_loss: 4274.1260 - val_mae: 47.0106\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 313ms/step - loss: 0.6806 - mae: 0.7104 - val_loss: 4265.8940 - val_mae: 46.9376\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 304ms/step - loss: 0.6502 - mae: 0.6979 - val_loss: 4258.9482 - val_mae: 46.8788\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 314ms/step - loss: 0.6274 - mae: 0.6854 - val_loss: 4254.2671 - val_mae: 46.8461\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 360ms/step - loss: 0.6045 - mae: 0.6729 - val_loss: 4252.1763 - val_mae: 46.8434\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 0.5765 - mae: 0.6604 - val_loss: 4252.3008 - val_mae: 46.8663\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 299ms/step - loss: 0.5452 - mae: 0.6481 - val_loss: 4253.7617 - val_mae: 46.9044\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 0.5159 - mae: 0.6357 - val_loss: 4255.4307 - val_mae: 46.9444\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 0.4921 - mae: 0.6232 - val_loss: 4256.2637 - val_mae: 46.9737\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 0.4723 - mae: 0.6108 - val_loss: 4255.5718 - val_mae: 46.9845\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 315ms/step - loss: 0.4523 - mae: 0.5984 - val_loss: 4253.1904 - val_mae: 46.9748\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 312ms/step - loss: 0.4296 - mae: 0.5859 - val_loss: 4249.4448 - val_mae: 46.9482\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 332ms/step - loss: 0.4059 - mae: 0.5734 - val_loss: 4245.0474 - val_mae: 46.9134\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 345ms/step - loss: 0.3842 - mae: 0.5611 - val_loss: 4240.8555 - val_mae: 46.8805\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 305ms/step - loss: 0.3660 - mae: 0.5488 - val_loss: 4237.5879 - val_mae: 46.8580\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 310ms/step - loss: 0.3498 - mae: 0.5366 - val_loss: 4235.6841 - val_mae: 46.8513\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 309ms/step - loss: 0.3333 - mae: 0.5245 - val_loss: 4235.1792 - val_mae: 46.8608\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 321ms/step - loss: 0.3157 - mae: 0.5125 - val_loss: 4235.7349 - val_mae: 46.8825\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 308ms/step - loss: 0.2981 - mae: 0.5007 - val_loss: 4236.7661 - val_mae: 46.9095\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 346ms/step - loss: 0.2822 - mae: 0.4891 - val_loss: 4237.6392 - val_mae: 46.9342\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 340ms/step - loss: 0.2684 - mae: 0.4775 - val_loss: 4237.8252 - val_mae: 46.9502\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 354ms/step - loss: 0.2556 - mae: 0.4660 - val_loss: 4237.0688 - val_mae: 46.9545\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 304ms/step - loss: 0.2426 - mae: 0.4546 - val_loss: 4235.4111 - val_mae: 46.9476\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 307ms/step - loss: 0.2292 - mae: 0.4434 - val_loss: 4233.1704 - val_mae: 46.9334\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 0.2165 - mae: 0.4323 - val_loss: 4230.8052 - val_mae: 46.9173\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 305ms/step - loss: 0.2051 - mae: 0.4214 - val_loss: 4228.7891 - val_mae: 46.9051\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 0.1949 - mae: 0.4107 - val_loss: 4227.4429 - val_mae: 46.9006\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 316ms/step - loss: 0.1852 - mae: 0.4001 - val_loss: 4226.8872 - val_mae: 46.9052\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 297ms/step - loss: 0.1754 - mae: 0.3897 - val_loss: 4227.0186 - val_mae: 46.9176\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 293ms/step - loss: 0.1657 - mae: 0.3794 - val_loss: 4227.5415 - val_mae: 46.9344\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 303ms/step - loss: 0.1566 - mae: 0.3694 - val_loss: 4228.0933 - val_mae: 46.9513\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 305ms/step - loss: 0.1485 - mae: 0.3595 - val_loss: 4228.3394 - val_mae: 46.9642\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 307ms/step - loss: 0.1411 - mae: 0.3498 - val_loss: 4228.0771 - val_mae: 46.9707\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 303ms/step - loss: 0.1339 - mae: 0.3402 - val_loss: 4227.2749 - val_mae: 46.9705\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 303ms/step - loss: 0.1269 - mae: 0.3309 - val_loss: 4226.0654 - val_mae: 46.9651\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 0.1201 - mae: 0.3217 - val_loss: 4224.6904 - val_mae: 46.9575\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 358ms/step - loss: 0.1138 - mae: 0.3127 - val_loss: 4223.4258 - val_mae: 46.9509\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "Predicted GDP for Zaporizhia_Oblast in 2021: [98.43368]\n",
      "Actual GDP for Zaporizhia_Oblast in 2021: 100.0\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - 0s 435ms/step - loss: 28.6886 - mae: 4.4306 - val_loss: 4224.6719 - val_mae: 48.9034\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 325ms/step - loss: 28.5036 - mae: 4.4231 - val_loss: 4204.4287 - val_mae: 48.8835\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 28.3025 - mae: 4.4113 - val_loss: 4185.3257 - val_mae: 48.8473\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 28.1151 - mae: 4.3958 - val_loss: 4172.5205 - val_mae: 48.7963\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 358ms/step - loss: 27.8826 - mae: 4.3773 - val_loss: 4167.2310 - val_mae: 48.7333\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 363ms/step - loss: 27.5743 - mae: 4.3563 - val_loss: 4167.0117 - val_mae: 48.6612\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 348ms/step - loss: 27.2319 - mae: 4.3334 - val_loss: 4167.3784 - val_mae: 48.5831\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 349ms/step - loss: 26.9034 - mae: 4.3089 - val_loss: 4163.7368 - val_mae: 48.5024\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 335ms/step - loss: 26.5829 - mae: 4.2832 - val_loss: 4153.3438 - val_mae: 48.4215\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 323ms/step - loss: 26.2334 - mae: 4.2564 - val_loss: 4136.2163 - val_mae: 48.3406\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 337ms/step - loss: 25.8469 - mae: 4.2288 - val_loss: 4114.7534 - val_mae: 48.2583\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 327ms/step - loss: 25.4520 - mae: 4.2004 - val_loss: 4092.5312 - val_mae: 48.1724\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 356ms/step - loss: 25.0698 - mae: 4.1710 - val_loss: 4072.8621 - val_mae: 48.0811\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 309ms/step - loss: 24.6938 - mae: 4.1411 - val_loss: 4057.5144 - val_mae: 47.9841\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 389ms/step - loss: 24.3004 - mae: 4.1103 - val_loss: 4046.1516 - val_mae: 47.8819\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 321ms/step - loss: 23.8900 - mae: 4.0787 - val_loss: 4036.7078 - val_mae: 47.7755\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 323ms/step - loss: 23.4805 - mae: 4.0466 - val_loss: 4026.3823 - val_mae: 47.6668\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 368ms/step - loss: 23.0796 - mae: 4.0138 - val_loss: 4012.8152 - val_mae: 47.5573\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 388ms/step - loss: 22.6767 - mae: 3.9804 - val_loss: 3995.0225 - val_mae: 47.4473\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 467ms/step - loss: 22.2605 - mae: 3.9466 - val_loss: 3973.6340 - val_mae: 47.3367\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 21.8350 - mae: 3.9122 - val_loss: 3950.5093 - val_mae: 47.2242\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 343ms/step - loss: 21.4109 - mae: 3.8771 - val_loss: 3929.6177 - val_mae: 47.1183\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 351ms/step - loss: 20.9901 - mae: 3.8412 - val_loss: 3910.5957 - val_mae: 47.0069\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 378ms/step - loss: 20.5666 - mae: 3.8047 - val_loss: 3893.8362 - val_mae: 46.8889\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 386ms/step - loss: 20.1360 - mae: 3.7678 - val_loss: 3878.5957 - val_mae: 46.7656\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 357ms/step - loss: 19.7007 - mae: 3.7300 - val_loss: 3863.4062 - val_mae: 46.6392\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 331ms/step - loss: 19.2655 - mae: 3.6914 - val_loss: 3846.7222 - val_mae: 46.5113\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 339ms/step - loss: 18.8287 - mae: 3.6519 - val_loss: 3827.6082 - val_mae: 46.3832\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 325ms/step - loss: 18.3861 - mae: 3.6116 - val_loss: 3806.0703 - val_mae: 46.2551\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 329ms/step - loss: 17.9362 - mae: 3.5705 - val_loss: 3782.9592 - val_mae: 46.1258\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 423ms/step - loss: 17.4836 - mae: 3.5285 - val_loss: 3759.5742 - val_mae: 45.9935\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 327ms/step - loss: 17.0324 - mae: 3.4857 - val_loss: 3737.0447 - val_mae: 45.8565\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 323ms/step - loss: 16.5785 - mae: 3.4420 - val_loss: 3715.8450 - val_mae: 45.7140\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 335ms/step - loss: 16.1196 - mae: 3.3972 - val_loss: 3695.6475 - val_mae: 45.5665\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 394ms/step - loss: 15.6575 - mae: 3.3514 - val_loss: 3675.5286 - val_mae: 45.4156\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 445ms/step - loss: 15.1987 - mae: 3.3049 - val_loss: 3654.4622 - val_mae: 45.2633\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 371ms/step - loss: 14.7433 - mae: 3.2583 - val_loss: 3631.7581 - val_mae: 45.1109\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 343ms/step - loss: 14.2856 - mae: 3.2108 - val_loss: 3607.4297 - val_mae: 44.9586\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 328ms/step - loss: 13.8253 - mae: 3.1624 - val_loss: 3582.1367 - val_mae: 44.8056\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 328ms/step - loss: 13.3648 - mae: 3.1130 - val_loss: 3556.7961 - val_mae: 44.6500\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 341ms/step - loss: 12.9064 - mae: 3.0628 - val_loss: 3532.1494 - val_mae: 44.4904\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 334ms/step - loss: 12.4531 - mae: 3.0119 - val_loss: 3508.4175 - val_mae: 44.3269\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 344ms/step - loss: 11.9992 - mae: 2.9599 - val_loss: 3485.2434 - val_mae: 44.1599\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 340ms/step - loss: 11.5459 - mae: 2.9066 - val_loss: 3461.9094 - val_mae: 43.9905\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 337ms/step - loss: 11.0977 - mae: 2.8526 - val_loss: 3437.7158 - val_mae: 43.8205\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 390ms/step - loss: 10.6550 - mae: 2.7981 - val_loss: 3412.3723 - val_mae: 43.6508\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 347ms/step - loss: 10.2185 - mae: 2.7430 - val_loss: 3386.1438 - val_mae: 43.4810\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 364ms/step - loss: 9.7846 - mae: 2.6871 - val_loss: 3359.7092 - val_mae: 43.3105\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 405ms/step - loss: 9.3542 - mae: 2.6301 - val_loss: 3333.7576 - val_mae: 43.1373\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 459ms/step - loss: 8.9278 - mae: 2.5721 - val_loss: 3308.6372 - val_mae: 42.9611\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 8.5068 - mae: 2.5133 - val_loss: 3284.1946 - val_mae: 42.7822\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 334ms/step - loss: 8.0906 - mae: 2.4534 - val_loss: 3259.9048 - val_mae: 42.6021\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 331ms/step - loss: 7.6818 - mae: 2.3927 - val_loss: 3235.1943 - val_mae: 42.4222\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 315ms/step - loss: 7.2792 - mae: 2.3311 - val_loss: 3209.8083 - val_mae: 42.2433\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 328ms/step - loss: 6.8841 - mae: 2.2691 - val_loss: 3184.3599 - val_mae: 42.0676\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 333ms/step - loss: 6.5004 - mae: 2.2067 - val_loss: 3159.2966 - val_mae: 41.8939\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 388ms/step - loss: 6.1256 - mae: 2.1437 - val_loss: 3134.7456 - val_mae: 41.7188\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 477ms/step - loss: 5.7586 - mae: 2.0798 - val_loss: 3110.9570 - val_mae: 41.5416\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 384ms/step - loss: 5.4022 - mae: 2.0153 - val_loss: 3087.7830 - val_mae: 41.3629\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 355ms/step - loss: 5.0563 - mae: 1.9501 - val_loss: 3064.8306 - val_mae: 41.1848\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 369ms/step - loss: 4.7231 - mae: 1.8847 - val_loss: 3041.7393 - val_mae: 41.0086\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 359ms/step - loss: 4.4004 - mae: 1.8189 - val_loss: 3018.3899 - val_mae: 40.8346\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 322ms/step - loss: 4.0874 - mae: 1.7521 - val_loss: 2995.0488 - val_mae: 40.6632\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 319ms/step - loss: 3.7859 - mae: 1.6850 - val_loss: 2972.1509 - val_mae: 40.4930\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 312ms/step - loss: 3.4973 - mae: 1.6177 - val_loss: 2950.0317 - val_mae: 40.3220\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 324ms/step - loss: 3.2221 - mae: 1.5504 - val_loss: 2928.7361 - val_mae: 40.1512\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 325ms/step - loss: 2.9595 - mae: 1.4830 - val_loss: 2908.0022 - val_mae: 39.9815\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 310ms/step - loss: 2.7117 - mae: 1.4162 - val_loss: 2887.4612 - val_mae: 39.8140\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 313ms/step - loss: 2.4765 - mae: 1.3495 - val_loss: 2866.9470 - val_mae: 39.6516\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 305ms/step - loss: 2.2541 - mae: 1.2831 - val_loss: 2846.5737 - val_mae: 39.4936\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 320ms/step - loss: 2.0469 - mae: 1.2179 - val_loss: 2826.7163 - val_mae: 39.3382\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 314ms/step - loss: 1.8536 - mae: 1.1531 - val_loss: 2807.6855 - val_mae: 39.1841\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 315ms/step - loss: 1.6738 - mae: 1.0889 - val_loss: 2789.5500 - val_mae: 39.0311\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 314ms/step - loss: 1.5086 - mae: 1.0261 - val_loss: 2772.1501 - val_mae: 38.8802\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 322ms/step - loss: 1.3556 - mae: 0.9640 - val_loss: 2755.1836 - val_mae: 38.7330\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 314ms/step - loss: 1.2142 - mae: 0.9028 - val_loss: 2738.4934 - val_mae: 38.5905\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 312ms/step - loss: 1.0843 - mae: 0.8478 - val_loss: 2722.1653 - val_mae: 38.4528\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 320ms/step - loss: 0.9654 - mae: 0.7958 - val_loss: 2706.4739 - val_mae: 38.3192\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 320ms/step - loss: 0.8573 - mae: 0.7450 - val_loss: 2691.6672 - val_mae: 38.1890\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 319ms/step - loss: 0.7595 - mae: 0.6942 - val_loss: 2677.8027 - val_mae: 38.0623\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 318ms/step - loss: 0.6716 - mae: 0.6434 - val_loss: 2664.7161 - val_mae: 37.9404\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 308ms/step - loss: 0.5929 - mae: 0.5935 - val_loss: 2652.1816 - val_mae: 37.8246\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 327ms/step - loss: 0.5231 - mae: 0.5461 - val_loss: 2640.1143 - val_mae: 37.7154\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 316ms/step - loss: 0.4616 - mae: 0.5010 - val_loss: 2628.6045 - val_mae: 37.6125\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 312ms/step - loss: 0.4081 - mae: 0.4646 - val_loss: 2617.8730 - val_mae: 37.5154\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 309ms/step - loss: 0.3618 - mae: 0.4309 - val_loss: 2608.0913 - val_mae: 37.4231\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 308ms/step - loss: 0.3224 - mae: 0.4050 - val_loss: 2599.2070 - val_mae: 37.3361\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 0.2891 - mae: 0.3914 - val_loss: 2591.0117 - val_mae: 37.2556\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 0.2606 - mae: 0.3776 - val_loss: 2583.6741 - val_mae: 37.1843\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 0.2363 - mae: 0.3627 - val_loss: 2577.0212 - val_mae: 37.1207\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 315ms/step - loss: 0.2154 - mae: 0.3477 - val_loss: 2571.0762 - val_mae: 37.0628\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 299ms/step - loss: 0.1973 - mae: 0.3350 - val_loss: 2565.9736 - val_mae: 37.0098\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 305ms/step - loss: 0.1816 - mae: 0.3239 - val_loss: 2561.6494 - val_mae: 36.9615\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 293ms/step - loss: 0.1678 - mae: 0.3141 - val_loss: 2557.9160 - val_mae: 36.9189\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 309ms/step - loss: 0.1557 - mae: 0.3040 - val_loss: 2554.6060 - val_mae: 36.8826\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 0.1453 - mae: 0.2921 - val_loss: 2551.7161 - val_mae: 36.8532\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 309ms/step - loss: 0.1359 - mae: 0.2785 - val_loss: 2549.3508 - val_mae: 36.8287\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 335ms/step - loss: 0.1274 - mae: 0.2651 - val_loss: 2547.5935 - val_mae: 36.8081\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 306ms/step - loss: 0.1195 - mae: 0.2554 - val_loss: 2546.3938 - val_mae: 36.7913\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 305ms/step - loss: 0.1122 - mae: 0.2495 - val_loss: 2545.5896 - val_mae: 36.7783\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "Predicted GDP for Ivano-Frankivsk_Oblast in 2021: [98.89972]\n",
      "Actual GDP for Ivano-Frankivsk_Oblast in 2021: 102.625\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - 0s 447ms/step - loss: 2051.0647 - mae: 44.9222 - val_loss: 2399.2107 - val_mae: 41.7376\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 299ms/step - loss: 574.9175 - mae: 23.4926 - val_loss: 2098.3552 - val_mae: 40.1225\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 60.2331 - mae: 5.8350 - val_loss: 1995.2979 - val_mae: 41.1221\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 293ms/step - loss: 271.0768 - mae: 15.1308 - val_loss: 1875.3044 - val_mae: 39.8578\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 312ms/step - loss: 542.9911 - mae: 22.4416 - val_loss: 1885.2374 - val_mae: 40.3100\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 308ms/step - loss: 593.6884 - mae: 23.7074 - val_loss: 1892.1732 - val_mae: 39.7819\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 340ms/step - loss: 451.5495 - mae: 20.7280 - val_loss: 1922.1716 - val_mae: 38.7399\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 308ms/step - loss: 263.6041 - mae: 15.6852 - val_loss: 2008.5391 - val_mae: 37.6241\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 312ms/step - loss: 110.9218 - mae: 9.8099 - val_loss: 2151.9463 - val_mae: 36.4407\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 305ms/step - loss: 24.2111 - mae: 4.4782 - val_loss: 2338.5605 - val_mae: 35.3090\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 17.8982 - mae: 2.9314 - val_loss: 2535.1189 - val_mae: 36.7640\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 310ms/step - loss: 69.0492 - mae: 7.5830 - val_loss: 2687.7341 - val_mae: 39.2429\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 316ms/step - loss: 125.6270 - mae: 10.6531 - val_loss: 2755.5872 - val_mae: 40.1535\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 324ms/step - loss: 141.6757 - mae: 11.3774 - val_loss: 2730.2891 - val_mae: 39.5207\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 311ms/step - loss: 113.9535 - mae: 10.0886 - val_loss: 2641.7122 - val_mae: 37.7855\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 65.3574 - mae: 7.3242 - val_loss: 2531.6541 - val_mae: 35.6563\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 296ms/step - loss: 25.9674 - mae: 3.8462 - val_loss: 2432.8701 - val_mae: 36.5088\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 304ms/step - loss: 11.1959 - mae: 2.7056 - val_loss: 2354.0564 - val_mae: 37.2387\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 304ms/step - loss: 19.0328 - mae: 4.0331 - val_loss: 2302.1606 - val_mae: 37.8184\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 315ms/step - loss: 37.5239 - mae: 5.5488 - val_loss: 2277.2314 - val_mae: 38.1951\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 298ms/step - loss: 53.9101 - mae: 6.5976 - val_loss: 2277.3071 - val_mae: 38.3678\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 313ms/step - loss: 59.6136 - mae: 7.0207 - val_loss: 2300.1814 - val_mae: 38.3505\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 298ms/step - loss: 52.9442 - mae: 6.5626 - val_loss: 2343.8745 - val_mae: 38.1743\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 322ms/step - loss: 38.0347 - mae: 5.5612 - val_loss: 2406.5139 - val_mae: 37.8952\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 363ms/step - loss: 21.7254 - mae: 4.1826 - val_loss: 2483.2427 - val_mae: 37.5825\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 339ms/step - loss: 10.5331 - mae: 3.0067 - val_loss: 2565.0649 - val_mae: 37.2582\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 321ms/step - loss: 7.7717 - mae: 2.1745 - val_loss: 2641.4558 - val_mae: 36.9763\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 316ms/step - loss: 12.2799 - mae: 2.6202 - val_loss: 2701.4324 - val_mae: 36.7701\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 318ms/step - loss: 19.7249 - mae: 3.6168 - val_loss: 2736.5063 - val_mae: 37.3102\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 329ms/step - loss: 25.0403 - mae: 4.3269 - val_loss: 2743.1045 - val_mae: 37.3898\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 311ms/step - loss: 25.2454 - mae: 4.3891 - val_loss: 2723.4858 - val_mae: 37.0163\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 311ms/step - loss: 20.5493 - mae: 3.8607 - val_loss: 2684.6924 - val_mae: 36.9767\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 311ms/step - loss: 13.6099 - mae: 3.0277 - val_loss: 2635.5427 - val_mae: 37.2109\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 309ms/step - loss: 7.6684 - mae: 2.0740 - val_loss: 2584.8892 - val_mae: 37.4547\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 310ms/step - loss: 4.8207 - mae: 1.7312 - val_loss: 2539.7881 - val_mae: 37.6765\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 318ms/step - loss: 5.2868 - mae: 2.0531 - val_loss: 2504.8862 - val_mae: 37.8521\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 319ms/step - loss: 7.7576 - mae: 2.4976 - val_loss: 2482.6316 - val_mae: 37.9674\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 10.3850 - mae: 2.8039 - val_loss: 2473.6335 - val_mae: 38.0130\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 304ms/step - loss: 11.6147 - mae: 2.9903 - val_loss: 2477.3267 - val_mae: 37.9922\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 387ms/step - loss: 10.9047 - mae: 2.8935 - val_loss: 2492.1865 - val_mae: 37.9143\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 299ms/step - loss: 8.6782 - mae: 2.5492 - val_loss: 2515.8101 - val_mae: 37.7921\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 304ms/step - loss: 5.9740 - mae: 2.1504 - val_loss: 2545.0430 - val_mae: 37.6432\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 305ms/step - loss: 3.8519 - mae: 1.6938 - val_loss: 2576.0945 - val_mae: 37.4866\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 308ms/step - loss: 2.9614 - mae: 1.4591 - val_loss: 2604.9307 - val_mae: 37.3411\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 306ms/step - loss: 3.3052 - mae: 1.4484 - val_loss: 2627.8018 - val_mae: 37.2228\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 306ms/step - loss: 4.3342 - mae: 1.6480 - val_loss: 2641.8796 - val_mae: 37.1434\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 309ms/step - loss: 5.2870 - mae: 1.9051 - val_loss: 2645.7827 - val_mae: 37.1085\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 307ms/step - loss: 5.5851 - mae: 1.9787 - val_loss: 2639.7537 - val_mae: 37.1164\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 317ms/step - loss: 5.0746 - mae: 1.8751 - val_loss: 2625.4985 - val_mae: 37.1603\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 339ms/step - loss: 4.0192 - mae: 1.6230 - val_loss: 2605.7305 - val_mae: 37.2292\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 368ms/step - loss: 2.8939 - mae: 1.3620 - val_loss: 2583.5500 - val_mae: 37.3101\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 336ms/step - loss: 2.1240 - mae: 1.2696 - val_loss: 2561.9258 - val_mae: 37.3898\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 306ms/step - loss: 1.8932 - mae: 1.2362 - val_loss: 2543.2803 - val_mae: 37.4567\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 310ms/step - loss: 2.1077 - mae: 1.2333 - val_loss: 2529.3113 - val_mae: 37.5025\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 2.4964 - mae: 1.3027 - val_loss: 2520.9304 - val_mae: 37.5219\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 2.7705 - mae: 1.3727 - val_loss: 2518.3369 - val_mae: 37.5138\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 322ms/step - loss: 2.7550 - mae: 1.3628 - val_loss: 2521.0815 - val_mae: 37.4801\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 318ms/step - loss: 2.4425 - mae: 1.2786 - val_loss: 2528.1953 - val_mae: 37.4258\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 310ms/step - loss: 1.9652 - mae: 1.1610 - val_loss: 2538.3071 - val_mae: 37.3578\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 318ms/step - loss: 1.5177 - mae: 1.0604 - val_loss: 2549.8022 - val_mae: 37.2840\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 304ms/step - loss: 1.2378 - mae: 1.0184 - val_loss: 2560.9993 - val_mae: 37.2126\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 296ms/step - loss: 1.1736 - mae: 0.9976 - val_loss: 2570.3630 - val_mae: 37.1507\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 304ms/step - loss: 1.2742 - mae: 0.9780 - val_loss: 2576.6895 - val_mae: 37.1045\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 326ms/step - loss: 1.4214 - mae: 0.9912 - val_loss: 2579.2800 - val_mae: 37.0772\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 339ms/step - loss: 1.4885 - mae: 0.9984 - val_loss: 2578.0203 - val_mae: 37.0691\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 342ms/step - loss: 1.4211 - mae: 0.9751 - val_loss: 2573.3677 - val_mae: 37.0781\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 312ms/step - loss: 1.2408 - mae: 0.9251 - val_loss: 2566.2090 - val_mae: 37.0999\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 310ms/step - loss: 1.0212 - mae: 0.8616 - val_loss: 2557.6843 - val_mae: 37.1293\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 331ms/step - loss: 0.8450 - mae: 0.8349 - val_loss: 2548.9868 - val_mae: 37.1600\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 337ms/step - loss: 0.7630 - mae: 0.8113 - val_loss: 2541.1731 - val_mae: 37.1870\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 305ms/step - loss: 0.7630 - mae: 0.7884 - val_loss: 2535.0586 - val_mae: 37.2061\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 306ms/step - loss: 0.8062 - mae: 0.8039 - val_loss: 2531.1604 - val_mae: 37.2148\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 323ms/step - loss: 0.8419 - mae: 0.8130 - val_loss: 2529.6670 - val_mae: 37.2119\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 303ms/step - loss: 0.8331 - mae: 0.8054 - val_loss: 2530.4529 - val_mae: 37.1980\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 312ms/step - loss: 0.7728 - mae: 0.7822 - val_loss: 2533.1443 - val_mae: 37.1751\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 304ms/step - loss: 0.6805 - mae: 0.7459 - val_loss: 2537.1533 - val_mae: 37.1460\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 320ms/step - loss: 0.5883 - mae: 0.7007 - val_loss: 2541.7869 - val_mae: 37.1142\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 342ms/step - loss: 0.5234 - mae: 0.6620 - val_loss: 2546.3345 - val_mae: 37.0834\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 311ms/step - loss: 0.4961 - mae: 0.6474 - val_loss: 2550.1589 - val_mae: 37.0565\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 326ms/step - loss: 0.4976 - mae: 0.6329 - val_loss: 2552.7690 - val_mae: 37.0359\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 319ms/step - loss: 0.5085 - mae: 0.6185 - val_loss: 2553.8838 - val_mae: 37.0231\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 309ms/step - loss: 0.5092 - mae: 0.6040 - val_loss: 2553.4446 - val_mae: 37.0182\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 348ms/step - loss: 0.4896 - mae: 0.5895 - val_loss: 2551.6135 - val_mae: 37.0206\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 351ms/step - loss: 0.4517 - mae: 0.5750 - val_loss: 2548.7114 - val_mae: 37.0286\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 313ms/step - loss: 0.4063 - mae: 0.5606 - val_loss: 2545.1843 - val_mae: 37.0400\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 0.3666 - mae: 0.5463 - val_loss: 2541.5039 - val_mae: 37.0527\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 308ms/step - loss: 0.3410 - mae: 0.5325 - val_loss: 2538.1150 - val_mae: 37.0643\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 0.3303 - mae: 0.5303 - val_loss: 2535.3811 - val_mae: 37.0730\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 312ms/step - loss: 0.3287 - mae: 0.5365 - val_loss: 2533.5427 - val_mae: 37.0776\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 303ms/step - loss: 0.3274 - mae: 0.5368 - val_loss: 2532.7107 - val_mae: 37.0775\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 304ms/step - loss: 0.3198 - mae: 0.5304 - val_loss: 2532.8516 - val_mae: 37.0729\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 307ms/step - loss: 0.3039 - mae: 0.5175 - val_loss: 2533.8201 - val_mae: 37.0644\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 335ms/step - loss: 0.2825 - mae: 0.4992 - val_loss: 2535.3811 - val_mae: 37.0533\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 315ms/step - loss: 0.2609 - mae: 0.4774 - val_loss: 2537.2495 - val_mae: 37.0411\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 308ms/step - loss: 0.2442 - mae: 0.4545 - val_loss: 2539.1223 - val_mae: 37.0292\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 0.2338 - mae: 0.4337 - val_loss: 2540.7363 - val_mae: 37.0190\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 324ms/step - loss: 0.2285 - mae: 0.4250 - val_loss: 2541.8882 - val_mae: 37.0115\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 356ms/step - loss: 0.2249 - mae: 0.4162 - val_loss: 2542.4587 - val_mae: 37.0071\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 298ms/step - loss: 0.2198 - mae: 0.4072 - val_loss: 2542.4219 - val_mae: 37.0060\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 306ms/step - loss: 0.2117 - mae: 0.3980 - val_loss: 2541.8394 - val_mae: 37.0079\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "Predicted GDP for Kyiv_Oblast in 2021: [91.19607]\n",
      "Actual GDP for Kyiv_Oblast in 2021: 106.125\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - 0s 434ms/step - loss: 989.6293 - mae: 31.0428 - val_loss: 3232.5149 - val_mae: 51.0080\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 317ms/step - loss: 696.2322 - mae: 25.8782 - val_loss: 3664.8557 - val_mae: 50.7897\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 354.8496 - mae: 18.1194 - val_loss: 4396.7939 - val_mae: 50.8431\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 305ms/step - loss: 128.5786 - mae: 10.0545 - val_loss: 5314.9902 - val_mae: 51.5773\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 304ms/step - loss: 42.8385 - mae: 5.3305 - val_loss: 6363.2461 - val_mae: 59.5602\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 34.3270 - mae: 4.7790 - val_loss: 7419.2593 - val_mae: 66.7245\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 310ms/step - loss: 93.9129 - mae: 8.3564 - val_loss: 8323.8721 - val_mae: 72.2170\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 307ms/step - loss: 177.0559 - mae: 12.1890 - val_loss: 8922.9795 - val_mae: 75.3958\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 292ms/step - loss: 228.1540 - mae: 14.1378 - val_loss: 9145.0107 - val_mae: 76.0852\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 351ms/step - loss: 217.3799 - mae: 13.7660 - val_loss: 9023.9453 - val_mae: 74.5974\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 334ms/step - loss: 157.1952 - mae: 11.3953 - val_loss: 8673.2842 - val_mae: 71.5985\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 319ms/step - loss: 85.6928 - mae: 7.9966 - val_loss: 8227.6592 - val_mae: 67.8743\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 428ms/step - loss: 37.5233 - mae: 5.1136 - val_loss: 7670.3745 - val_mae: 63.6339\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 461ms/step - loss: 26.2484 - mae: 4.3762 - val_loss: 7212.7603 - val_mae: 60.1340\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 423ms/step - loss: 44.6480 - mae: 5.2614 - val_loss: 6891.5010 - val_mae: 60.0970\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 350ms/step - loss: 74.8228 - mae: 7.2403 - val_loss: 6711.3022 - val_mae: 60.0748\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 321ms/step - loss: 99.0199 - mae: 8.6452 - val_loss: 6664.4629 - val_mae: 60.0691\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 320ms/step - loss: 106.5186 - mae: 9.0833 - val_loss: 6738.9805 - val_mae: 60.0801\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 322ms/step - loss: 95.6934 - mae: 8.4779 - val_loss: 6919.9341 - val_mae: 60.1057\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 313ms/step - loss: 72.5334 - mae: 7.1835 - val_loss: 7187.6738 - val_mae: 60.1418\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 324ms/step - loss: 47.0374 - mae: 5.5749 - val_loss: 7514.7930 - val_mae: 62.3922\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 313ms/step - loss: 28.8712 - mae: 4.2372 - val_loss: 7864.4648 - val_mae: 65.0964\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 310ms/step - loss: 23.4770 - mae: 4.1555 - val_loss: 8192.2783 - val_mae: 67.5356\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 387ms/step - loss: 30.0587 - mae: 4.5225 - val_loss: 8452.9082 - val_mae: 69.4152\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 348ms/step - loss: 42.4349 - mae: 5.4154 - val_loss: 8609.9316 - val_mae: 70.5256\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 365ms/step - loss: 52.5424 - mae: 6.1881 - val_loss: 8646.8154 - val_mae: 70.7846\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 316ms/step - loss: 54.7896 - mae: 6.3427 - val_loss: 8569.6426 - val_mae: 70.2443\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 420ms/step - loss: 48.6124 - mae: 5.9171 - val_loss: 8403.8018 - val_mae: 69.0700\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 482ms/step - loss: 37.8544 - mae: 5.1106 - val_loss: 8186.2368 - val_mae: 67.4995\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 418ms/step - loss: 27.8662 - mae: 4.3101 - val_loss: 7955.2236 - val_mae: 65.7920\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 353ms/step - loss: 22.5010 - mae: 3.9597 - val_loss: 7742.9155 - val_mae: 64.1845\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 334ms/step - loss: 22.6406 - mae: 3.9254 - val_loss: 7572.2627 - val_mae: 62.8641\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 337ms/step - loss: 26.5414 - mae: 4.0106 - val_loss: 7456.7720 - val_mae: 61.9560\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 338ms/step - loss: 31.2412 - mae: 4.4472 - val_loss: 7401.8765 - val_mae: 61.5219\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 348ms/step - loss: 34.1157 - mae: 4.6789 - val_loss: 7406.5811 - val_mae: 61.5638\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 330ms/step - loss: 33.9047 - mae: 4.6772 - val_loss: 7464.7886 - val_mae: 62.0321\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 353ms/step - loss: 30.9447 - mae: 4.4662 - val_loss: 7566.1177 - val_mae: 62.8361\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 384ms/step - loss: 26.7185 - mae: 4.1075 - val_loss: 7696.5493 - val_mae: 63.8553\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 338ms/step - loss: 23.0269 - mae: 3.8663 - val_loss: 7839.3525 - val_mae: 64.9529\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 321ms/step - loss: 21.1628 - mae: 3.7629 - val_loss: 7976.5571 - val_mae: 65.9909\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 337ms/step - loss: 21.4156 - mae: 3.7533 - val_loss: 8091.2480 - val_mae: 66.8473\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 349ms/step - loss: 23.0806 - mae: 3.7658 - val_loss: 8170.0640 - val_mae: 67.4310\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 320ms/step - loss: 24.9328 - mae: 3.9526 - val_loss: 8205.4404 - val_mae: 67.6940\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 457ms/step - loss: 25.8860 - mae: 4.0620 - val_loss: 8196.6943 - val_mae: 67.6349\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 351ms/step - loss: 25.4906 - mae: 4.0131 - val_loss: 8149.7036 - val_mae: 67.2959\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 328ms/step - loss: 24.0375 - mae: 3.8267 - val_loss: 8075.2188 - val_mae: 66.7523\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 340ms/step - loss: 22.2760 - mae: 3.6586 - val_loss: 7986.4478 - val_mae: 66.0977\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 328ms/step - loss: 20.9636 - mae: 3.6376 - val_loss: 7896.6519 - val_mae: 65.4291\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 314ms/step - loss: 20.5063 - mae: 3.6173 - val_loss: 7817.2827 - val_mae: 64.8333\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 321ms/step - loss: 20.8421 - mae: 3.7488 - val_loss: 7756.8267 - val_mae: 64.3771\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 341ms/step - loss: 21.5698 - mae: 3.8709 - val_loss: 7720.3896 - val_mae: 64.1028\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 343ms/step - loss: 22.1991 - mae: 3.9459 - val_loss: 7709.6938 - val_mae: 64.0259\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 321ms/step - loss: 22.3878 - mae: 3.9697 - val_loss: 7723.3999 - val_mae: 64.1369\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 317ms/step - loss: 22.0630 - mae: 3.9447 - val_loss: 7757.5835 - val_mae: 64.4048\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 330ms/step - loss: 21.4010 - mae: 3.8788 - val_loss: 7806.3472 - val_mae: 64.7827\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 330ms/step - loss: 20.7024 - mae: 3.7840 - val_loss: 7862.5127 - val_mae: 65.2146\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 332ms/step - loss: 20.2342 - mae: 3.6746 - val_loss: 7918.4775 - val_mae: 65.6423\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 324ms/step - loss: 20.1148 - mae: 3.5656 - val_loss: 7967.1650 - val_mae: 66.0129\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 314ms/step - loss: 20.2841 - mae: 3.5296 - val_loss: 8002.9058 - val_mae: 66.2852\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 320ms/step - loss: 20.5620 - mae: 3.5268 - val_loss: 8022.1597 - val_mae: 66.4340\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 320ms/step - loss: 20.7527 - mae: 3.5230 - val_loss: 8023.8667 - val_mae: 66.4522\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 315ms/step - loss: 20.7395 - mae: 3.5183 - val_loss: 8009.4321 - val_mae: 66.3505\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 317ms/step - loss: 20.5250 - mae: 3.5128 - val_loss: 7982.2886 - val_mae: 66.1539\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 319ms/step - loss: 20.2081 - mae: 3.5066 - val_loss: 7947.1694 - val_mae: 65.8969\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 384ms/step - loss: 19.9181 - mae: 3.5001 - val_loss: 7909.2822 - val_mae: 65.6182\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 19.7478 - mae: 3.5495 - val_loss: 7873.5928 - val_mae: 65.3550\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 310ms/step - loss: 19.7178 - mae: 3.6117 - val_loss: 7844.1777 - val_mae: 65.1383\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 348ms/step - loss: 19.7812 - mae: 3.6618 - val_loss: 7823.8604 - val_mae: 64.9901\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 335ms/step - loss: 19.8584 - mae: 3.6941 - val_loss: 7814.0444 - val_mae: 64.9215\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 323ms/step - loss: 19.8795 - mae: 3.7059 - val_loss: 7814.6963 - val_mae: 64.9325\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 19.8147 - mae: 3.6971 - val_loss: 7824.5391 - val_mae: 65.0134\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 312ms/step - loss: 19.6811 - mae: 3.6703 - val_loss: 7841.2910 - val_mae: 65.1466\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 303ms/step - loss: 19.5245 - mae: 3.6301 - val_loss: 7862.0322 - val_mae: 65.3098\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 318ms/step - loss: 19.3934 - mae: 3.5821 - val_loss: 7883.6025 - val_mae: 65.4791\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 322ms/step - loss: 19.3166 - mae: 3.5326 - val_loss: 7903.0166 - val_mae: 65.6318\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 311ms/step - loss: 19.2929 - mae: 3.4873 - val_loss: 7917.8101 - val_mae: 65.7494\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 303ms/step - loss: 19.2970 - mae: 3.4707 - val_loss: 7926.3413 - val_mae: 65.8198\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 19.2960 - mae: 3.4696 - val_loss: 7927.9253 - val_mae: 65.8381\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 19.2659 - mae: 3.4680 - val_loss: 7922.8516 - val_mae: 65.8064\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 298ms/step - loss: 19.2010 - mae: 3.4658 - val_loss: 7912.2549 - val_mae: 65.7331\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 323ms/step - loss: 19.1135 - mae: 3.4630 - val_loss: 7897.8672 - val_mae: 65.6313\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 308ms/step - loss: 19.0241 - mae: 3.4599 - val_loss: 7881.7354 - val_mae: 65.5161\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 309ms/step - loss: 18.9507 - mae: 3.4730 - val_loss: 7865.8960 - val_mae: 65.4031\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 306ms/step - loss: 18.9005 - mae: 3.4959 - val_loss: 7852.1504 - val_mae: 65.3057\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 298ms/step - loss: 18.8686 - mae: 3.5147 - val_loss: 7841.8354 - val_mae: 65.2342\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 303ms/step - loss: 18.8428 - mae: 3.5271 - val_loss: 7835.7271 - val_mae: 65.1946\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 308ms/step - loss: 18.8105 - mae: 3.5314 - val_loss: 7833.9800 - val_mae: 65.1882\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 304ms/step - loss: 18.7644 - mae: 3.5273 - val_loss: 7836.2134 - val_mae: 65.2120\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 18.7057 - mae: 3.5156 - val_loss: 7841.5664 - val_mae: 65.2595\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 306ms/step - loss: 18.6407 - mae: 3.4980 - val_loss: 7848.8784 - val_mae: 65.3219\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 374ms/step - loss: 18.5780 - mae: 3.4766 - val_loss: 7856.8486 - val_mae: 65.3892\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 18.5234 - mae: 3.4541 - val_loss: 7864.2241 - val_mae: 65.4520\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 18.4779 - mae: 3.4327 - val_loss: 7869.9375 - val_mae: 65.5023\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 314ms/step - loss: 18.4380 - mae: 3.4257 - val_loss: 7873.2339 - val_mae: 65.5343\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 18.3985 - mae: 3.4226 - val_loss: 7873.7534 - val_mae: 65.5453\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 297ms/step - loss: 18.3548 - mae: 3.4192 - val_loss: 7871.5244 - val_mae: 65.5357\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 18.3051 - mae: 3.4155 - val_loss: 7866.9341 - val_mae: 65.5083\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 295ms/step - loss: 18.2508 - mae: 3.4115 - val_loss: 7860.6309 - val_mae: 65.4679\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 18.1951 - mae: 3.4073 - val_loss: 7853.4277 - val_mae: 65.4208\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 378ms/step - loss: 18.1414 - mae: 3.4057 - val_loss: 7846.1538 - val_mae: 65.3732\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "Predicted GDP for Kirovohrad_Oblast in 2021: [111.657036]\n",
      "Actual GDP for Kirovohrad_Oblast in 2021: 113.9375\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - 0s 480ms/step - loss: 4934.2129 - mae: 63.2500 - val_loss: 10269.9590 - val_mae: 99.8471\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 319ms/step - loss: 4025.3491 - mae: 55.6297 - val_loss: 7975.2178 - val_mae: 87.6328\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 322ms/step - loss: 2854.5073 - mae: 46.6853 - val_loss: 5777.6826 - val_mae: 74.0805\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 330ms/step - loss: 1880.0228 - mae: 40.1514 - val_loss: 4003.1489 - val_mae: 60.9864\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 324ms/step - loss: 1257.1316 - mae: 33.8791 - val_loss: 2705.6924 - val_mae: 49.2729\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 312ms/step - loss: 961.9595 - mae: 28.6102 - val_loss: 1824.7139 - val_mae: 39.4003\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 321ms/step - loss: 904.1498 - mae: 26.2052 - val_loss: 1292.8994 - val_mae: 31.9291\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 312ms/step - loss: 978.3765 - mae: 24.3333 - val_loss: 968.0017 - val_mae: 26.4413\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 412ms/step - loss: 1098.2174 - mae: 23.0155 - val_loss: 762.5568 - val_mae: 22.3282\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 337ms/step - loss: 1213.1237 - mae: 22.1932 - val_loss: 639.1354 - val_mae: 19.4800\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 314ms/step - loss: 1303.7390 - mae: 22.1385 - val_loss: 557.2195 - val_mae: 17.3768\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 332ms/step - loss: 1373.2878 - mae: 22.3475 - val_loss: 505.7296 - val_mae: 15.9387\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 322ms/step - loss: 1419.3767 - mae: 23.3132 - val_loss: 474.2185 - val_mae: 15.7481\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 314ms/step - loss: 1442.8326 - mae: 23.9249 - val_loss: 457.9853 - val_mae: 15.6347\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 1445.6333 - mae: 24.0907 - val_loss: 452.3455 - val_mae: 15.5215\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 312ms/step - loss: 1433.0789 - mae: 23.9289 - val_loss: 455.2777 - val_mae: 15.4372\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 308ms/step - loss: 1410.4902 - mae: 23.5467 - val_loss: 466.2096 - val_mae: 15.3747\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 307ms/step - loss: 1379.0073 - mae: 22.9556 - val_loss: 485.0818 - val_mae: 15.8152\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 314ms/step - loss: 1340.1310 - mae: 22.2018 - val_loss: 511.6293 - val_mae: 16.6636\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 332ms/step - loss: 1295.7100 - mae: 21.6233 - val_loss: 545.9355 - val_mae: 17.6823\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 339ms/step - loss: 1247.4094 - mae: 21.5835 - val_loss: 588.2393 - val_mae: 18.8511\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 316ms/step - loss: 1196.6205 - mae: 21.5456 - val_loss: 637.9812 - val_mae: 20.1330\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 313ms/step - loss: 1144.5891 - mae: 21.5031 - val_loss: 694.0042 - val_mae: 21.4844\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 311ms/step - loss: 1095.0613 - mae: 21.5897 - val_loss: 758.1400 - val_mae: 22.9337\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 312ms/step - loss: 1046.2802 - mae: 21.9215 - val_loss: 830.6549 - val_mae: 24.4695\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 323ms/step - loss: 998.7599 - mae: 22.2677 - val_loss: 911.6353 - val_mae: 26.0789\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 307ms/step - loss: 953.8329 - mae: 22.6246 - val_loss: 1000.7816 - val_mae: 27.7445\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 315ms/step - loss: 913.2604 - mae: 22.9995 - val_loss: 1097.5242 - val_mae: 29.4480\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 313ms/step - loss: 877.3268 - mae: 23.3848 - val_loss: 1200.7935 - val_mae: 31.1664\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 847.1111 - mae: 23.7793 - val_loss: 1308.9816 - val_mae: 32.8731\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 822.9493 - mae: 24.1663 - val_loss: 1420.5023 - val_mae: 34.5535\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 343ms/step - loss: 805.1227 - mae: 24.5392 - val_loss: 1532.1797 - val_mae: 36.1617\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 372ms/step - loss: 793.4852 - mae: 24.8905 - val_loss: 1640.7205 - val_mae: 37.6623\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 474ms/step - loss: 787.4534 - mae: 25.2120 - val_loss: 1742.5525 - val_mae: 39.0218\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 442ms/step - loss: 786.0619 - mae: 25.4963 - val_loss: 1834.0126 - val_mae: 40.2082\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 356ms/step - loss: 787.9725 - mae: 25.7360 - val_loss: 1911.7712 - val_mae: 41.1961\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 334ms/step - loss: 791.7346 - mae: 25.9257 - val_loss: 1972.5703 - val_mae: 41.9593\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 314ms/step - loss: 795.5869 - mae: 26.0581 - val_loss: 2001.6925 - val_mae: 42.3379\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 328ms/step - loss: 796.0445 - mae: 26.0949 - val_loss: 2004.6473 - val_mae: 42.4068\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 494ms/step - loss: 793.0929 - mae: 26.0581 - val_loss: 1984.5033 - val_mae: 42.2034\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 350ms/step - loss: 786.4329 - mae: 25.9536 - val_loss: 1944.2651 - val_mae: 41.7580\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 362ms/step - loss: 776.4655 - mae: 25.7904 - val_loss: 1887.5793 - val_mae: 41.1075\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 336ms/step - loss: 763.9308 - mae: 25.5776 - val_loss: 1818.0679 - val_mae: 40.2859\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 1s 505ms/step - loss: 749.6538 - mae: 25.3228 - val_loss: 1737.9929 - val_mae: 39.3135\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 439ms/step - loss: 734.3602 - mae: 25.0308 - val_loss: 1644.7771 - val_mae: 38.1475\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 359ms/step - loss: 718.2308 - mae: 24.6915 - val_loss: 1547.1616 - val_mae: 36.8876\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 329ms/step - loss: 702.7770 - mae: 24.3278 - val_loss: 1449.3079 - val_mae: 35.5830\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 347ms/step - loss: 688.5873 - mae: 23.9509 - val_loss: 1352.8970 - val_mae: 34.2548\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 378ms/step - loss: 675.7319 - mae: 23.5653 - val_loss: 1261.6655 - val_mae: 32.9575\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 399ms/step - loss: 664.0138 - mae: 23.1815 - val_loss: 1176.7610 - val_mae: 31.7149\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 364ms/step - loss: 652.9622 - mae: 22.8053 - val_loss: 1103.1921 - val_mae: 30.6145\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 330ms/step - loss: 642.0103 - mae: 22.4578 - val_loss: 1042.7512 - val_mae: 29.6979\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 331ms/step - loss: 629.8231 - mae: 22.1436 - val_loss: 995.7719 - val_mae: 28.9934\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 339ms/step - loss: 615.6823 - mae: 21.8672 - val_loss: 961.6074 - val_mae: 28.5060\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 341ms/step - loss: 599.4973 - mae: 21.6310 - val_loss: 937.5575 - val_mae: 28.2008\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 437ms/step - loss: 581.1078 - mae: 21.4215 - val_loss: 920.6097 - val_mae: 28.0309\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 331ms/step - loss: 560.7090 - mae: 21.2203 - val_loss: 910.6852 - val_mae: 28.0191\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 322ms/step - loss: 538.1108 - mae: 21.0480 - val_loss: 907.2765 - val_mae: 28.1326\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 321ms/step - loss: 517.6812 - mae: 20.8879 - val_loss: 891.7722 - val_mae: 28.0434\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 342ms/step - loss: 497.9486 - mae: 20.6608 - val_loss: 858.2661 - val_mae: 27.6342\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 328ms/step - loss: 477.6027 - mae: 20.3382 - val_loss: 806.3135 - val_mae: 26.8759\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 327ms/step - loss: 456.5288 - mae: 19.9360 - val_loss: 746.5208 - val_mae: 25.8988\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 337ms/step - loss: 434.2383 - mae: 19.4663 - val_loss: 675.8965 - val_mae: 24.6454\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 340ms/step - loss: 409.6108 - mae: 18.9187 - val_loss: 594.3973 - val_mae: 23.0812\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 320ms/step - loss: 381.6689 - mae: 18.2730 - val_loss: 507.0595 - val_mae: 21.2649\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 313ms/step - loss: 350.2811 - mae: 17.5089 - val_loss: 420.9746 - val_mae: 19.3141\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 328ms/step - loss: 315.9633 - mae: 16.6443 - val_loss: 340.4196 - val_mae: 17.3384\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 307ms/step - loss: 280.6221 - mae: 15.6812 - val_loss: 270.8579 - val_mae: 15.5148\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 391ms/step - loss: 251.2192 - mae: 14.8416 - val_loss: 210.8234 - val_mae: 13.7890\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 314ms/step - loss: 225.5602 - mae: 14.0383 - val_loss: 158.3609 - val_mae: 12.0159\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 366ms/step - loss: 200.7051 - mae: 13.2099 - val_loss: 112.8108 - val_mae: 10.1744\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 326ms/step - loss: 175.6279 - mae: 12.3106 - val_loss: 98.1749 - val_mae: 9.5328\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 351ms/step - loss: 163.4395 - mae: 11.7500 - val_loss: 84.3732 - val_mae: 8.7661\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 329ms/step - loss: 147.5538 - mae: 11.0983 - val_loss: 67.4689 - val_mae: 7.6374\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 330ms/step - loss: 128.9089 - mae: 10.3895 - val_loss: 52.1641 - val_mae: 6.3826\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 331ms/step - loss: 114.0677 - mae: 9.7151 - val_loss: 40.5926 - val_mae: 5.2700\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 325ms/step - loss: 103.5499 - mae: 9.0410 - val_loss: 31.6721 - val_mae: 4.2877\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 346ms/step - loss: 93.4443 - mae: 8.3552 - val_loss: 27.3224 - val_mae: 3.7372\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 392ms/step - loss: 80.5963 - mae: 7.6445 - val_loss: 25.0007 - val_mae: 3.5700\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 346ms/step - loss: 67.6771 - mae: 6.8925 - val_loss: 23.2863 - val_mae: 3.4715\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 412ms/step - loss: 60.1395 - mae: 6.3632 - val_loss: 22.1990 - val_mae: 3.5414\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 351ms/step - loss: 56.3144 - mae: 6.3467 - val_loss: 23.1742 - val_mae: 4.0181\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 345ms/step - loss: 48.7084 - mae: 5.7828 - val_loss: 26.8534 - val_mae: 4.7986\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 325ms/step - loss: 41.7526 - mae: 4.9050 - val_loss: 34.4646 - val_mae: 5.6949\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 335ms/step - loss: 39.5472 - mae: 4.6188 - val_loss: 41.2416 - val_mae: 6.3078\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 308ms/step - loss: 37.5812 - mae: 4.5423 - val_loss: 44.4135 - val_mae: 6.5283\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 313ms/step - loss: 33.3030 - mae: 4.4663 - val_loss: 45.2801 - val_mae: 6.5267\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 310ms/step - loss: 31.6944 - mae: 4.4002 - val_loss: 46.6820 - val_mae: 6.6017\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 304ms/step - loss: 32.3771 - mae: 4.3133 - val_loss: 50.9312 - val_mae: 6.9672\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 30.3889 - mae: 4.2140 - val_loss: 57.9269 - val_mae: 7.5218\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 305ms/step - loss: 28.6305 - mae: 4.4613 - val_loss: 63.0030 - val_mae: 7.8728\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 417ms/step - loss: 28.8115 - mae: 4.6899 - val_loss: 62.8768 - val_mae: 7.8416\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 323ms/step - loss: 27.3236 - mae: 4.5184 - val_loss: 59.4168 - val_mae: 7.5572\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 307ms/step - loss: 25.0328 - mae: 4.0013 - val_loss: 56.3558 - val_mae: 7.2807\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 312ms/step - loss: 24.7827 - mae: 3.6765 - val_loss: 56.1285 - val_mae: 7.2408\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 377ms/step - loss: 23.3684 - mae: 3.5694 - val_loss: 58.8104 - val_mae: 7.4601\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 460ms/step - loss: 20.8480 - mae: 3.5135 - val_loss: 61.4453 - val_mae: 7.6681\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 360ms/step - loss: 19.8426 - mae: 3.6064 - val_loss: 61.0823 - val_mae: 7.6364\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 346ms/step - loss: 18.6426 - mae: 3.4769 - val_loss: 57.2674 - val_mae: 7.3248\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 334ms/step - loss: 16.4059 - mae: 3.1387 - val_loss: 52.9958 - val_mae: 6.9467\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "Predicted GDP for Luhansk_Oblast in 2021: [35.845882]\n",
      "Actual GDP for Luhansk_Oblast in 2021: 27.015625\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - 0s 494ms/step - loss: 1076.3655 - mae: 27.1436 - val_loss: 1188.7058 - val_mae: 28.5510\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 1177.3864 - mae: 29.5442 - val_loss: 868.5955 - val_mae: 24.7251\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 792.1896 - mae: 23.4255 - val_loss: 206.3159 - val_mae: 13.2975\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 541.9061 - mae: 19.3624 - val_loss: 93.3419 - val_mae: 9.3921\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 329ms/step - loss: 655.7910 - mae: 21.6531 - val_loss: 338.7135 - val_mae: 15.8823\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 327ms/step - loss: 270.4879 - mae: 12.9154 - val_loss: 711.2370 - val_mae: 21.6776\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 313ms/step - loss: 400.4787 - mae: 16.6666 - val_loss: 610.1259 - val_mae: 20.0830\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 324ms/step - loss: 275.3676 - mae: 13.4743 - val_loss: 240.1643 - val_mae: 13.2559\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 360ms/step - loss: 129.1124 - mae: 9.1300 - val_loss: 92.5275 - val_mae: 8.7728\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 400ms/step - loss: 264.2891 - mae: 14.5060 - val_loss: 225.5442 - val_mae: 12.5916\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 468ms/step - loss: 73.8778 - mae: 7.1884 - val_loss: 519.9716 - val_mae: 18.0095\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 363ms/step - loss: 107.4589 - mae: 9.0091 - val_loss: 573.4459 - val_mae: 18.7467\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 353ms/step - loss: 135.4629 - mae: 10.8882 - val_loss: 326.9774 - val_mae: 14.5645\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 353ms/step - loss: 19.2206 - mae: 3.3592 - val_loss: 124.1148 - val_mae: 9.5723\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 328ms/step - loss: 101.7494 - mae: 9.1015 - val_loss: 174.3152 - val_mae: 11.0386\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 423ms/step - loss: 49.8832 - mae: 6.1200 - val_loss: 399.8550 - val_mae: 15.8466\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 325ms/step - loss: 20.2817 - mae: 3.4440 - val_loss: 514.3752 - val_mae: 17.7216\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 334ms/step - loss: 66.2877 - mae: 7.6128 - val_loss: 425.1552 - val_mae: 16.2676\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 345ms/step - loss: 23.2168 - mae: 4.0394 - val_loss: 264.2338 - val_mae: 13.1863\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 330ms/step - loss: 16.0682 - mae: 3.5376 - val_loss: 202.4986 - val_mae: 11.7426\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 342ms/step - loss: 49.2034 - mae: 6.7305 - val_loss: 276.2197 - val_mae: 13.4223\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 330ms/step - loss: 10.6803 - mae: 2.9483 - val_loss: 411.7924 - val_mae: 15.9977\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 350ms/step - loss: 13.9686 - mae: 3.2902 - val_loss: 465.2888 - val_mae: 16.8905\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 335ms/step - loss: 32.1989 - mae: 5.4269 - val_loss: 386.6163 - val_mae: 15.5483\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 346ms/step - loss: 7.4937 - mae: 2.4213 - val_loss: 273.4660 - val_mae: 13.3458\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 318ms/step - loss: 7.9675 - mae: 2.6214 - val_loss: 231.3224 - val_mae: 12.4058\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 386ms/step - loss: 23.2650 - mae: 4.6983 - val_loss: 281.5553 - val_mae: 13.5128\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 341ms/step - loss: 5.0193 - mae: 1.9976 - val_loss: 372.6230 - val_mae: 15.2863\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 348ms/step - loss: 5.9138 - mae: 2.2396 - val_loss: 412.8874 - val_mae: 15.9979\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 484ms/step - loss: 16.8255 - mae: 3.9917 - val_loss: 365.4434 - val_mae: 15.1527\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 386ms/step - loss: 5.2680 - mae: 2.0883 - val_loss: 285.4794 - val_mae: 13.5899\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 343ms/step - loss: 3.4705 - mae: 1.5697 - val_loss: 248.2141 - val_mae: 12.7838\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 333ms/step - loss: 12.5890 - mae: 3.3493 - val_loss: 277.6756 - val_mae: 13.4250\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 330ms/step - loss: 4.2853 - mae: 1.8065 - val_loss: 342.3873 - val_mae: 14.7208\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 337ms/step - loss: 2.2372 - mae: 1.2610 - val_loss: 379.7509 - val_mae: 15.4123\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 346ms/step - loss: 8.8262 - mae: 2.8806 - val_loss: 354.7024 - val_mae: 14.9534\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 354ms/step - loss: 3.7773 - mae: 1.8243 - val_loss: 297.6542 - val_mae: 13.8414\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 326ms/step - loss: 0.9402 - mae: 0.8388 - val_loss: 263.5201 - val_mae: 13.1231\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 421ms/step - loss: 6.1311 - mae: 2.4046 - val_loss: 277.4984 - val_mae: 13.4236\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 315ms/step - loss: 3.1473 - mae: 1.6961 - val_loss: 323.0417 - val_mae: 14.3506\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 321ms/step - loss: 0.4927 - mae: 0.6449 - val_loss: 356.5511 - val_mae: 14.9906\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 328ms/step - loss: 4.2292 - mae: 1.9688 - val_loss: 346.7050 - val_mae: 14.8068\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 324ms/step - loss: 2.7736 - mae: 1.5631 - val_loss: 306.9126 - val_mae: 14.0329\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 337ms/step - loss: 0.2709 - mae: 0.3317 - val_loss: 276.3442 - val_mae: 13.4028\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 308ms/step - loss: 2.8904 - mae: 1.6505 - val_loss: 279.5152 - val_mae: 13.4706\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 304ms/step - loss: 2.3070 - mae: 1.4660 - val_loss: 309.8528 - val_mae: 14.0939\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 317ms/step - loss: 0.1924 - mae: 0.3721 - val_loss: 337.5764 - val_mae: 14.6367\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 310ms/step - loss: 1.9320 - mae: 1.3039 - val_loss: 336.5371 - val_mae: 14.6173\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 1.8587 - mae: 1.2889 - val_loss: 310.0804 - val_mae: 14.1003\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 318ms/step - loss: 0.1352 - mae: 0.3060 - val_loss: 284.8118 - val_mae: 13.5847\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 346ms/step - loss: 1.2183 - mae: 1.0628 - val_loss: 282.2055 - val_mae: 13.5305\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 1s 553ms/step - loss: 1.4358 - mae: 1.1646 - val_loss: 301.6933 - val_mae: 13.9324\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 467ms/step - loss: 0.1078 - mae: 0.2616 - val_loss: 323.6517 - val_mae: 14.3694\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 0.7636 - mae: 0.8389 - val_loss: 327.4614 - val_mae: 14.4437\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 305ms/step - loss: 1.1087 - mae: 1.0287 - val_loss: 311.1709 - val_mae: 14.1230\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 316ms/step - loss: 0.1344 - mae: 0.3266 - val_loss: 291.4946 - val_mae: 13.7239\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 292ms/step - loss: 0.4567 - mae: 0.6247 - val_loss: 285.8267 - val_mae: 13.6063\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 346ms/step - loss: 0.8559 - mae: 0.8830 - val_loss: 297.7315 - val_mae: 13.8516\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 299ms/step - loss: 0.1534 - mae: 0.3443 - val_loss: 314.7284 - val_mae: 14.1933\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 0.2735 - mae: 0.4882 - val_loss: 321.1631 - val_mae: 14.3200\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 307ms/step - loss: 0.6418 - mae: 0.7841 - val_loss: 312.1773 - val_mae: 14.1422\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 298ms/step - loss: 0.1651 - mae: 0.3690 - val_loss: 297.2682 - val_mae: 13.8413\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 426ms/step - loss: 0.1419 - mae: 0.3444 - val_loss: 290.2855 - val_mae: 13.6975\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 328ms/step - loss: 0.4626 - mae: 0.6510 - val_loss: 296.5789 - val_mae: 13.8266\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 298ms/step - loss: 0.1544 - mae: 0.3593 - val_loss: 309.3814 - val_mae: 14.0853\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 305ms/step - loss: 0.0645 - mae: 0.2260 - val_loss: 316.7779 - val_mae: 14.2322\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 313ms/step - loss: 0.3203 - mae: 0.5562 - val_loss: 312.6671 - val_mae: 14.1503\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 285ms/step - loss: 0.1432 - mae: 0.3658 - val_loss: 301.9639 - val_mae: 13.9351\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 0.0240 - mae: 0.1301 - val_loss: 294.8822 - val_mae: 13.7905\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 0.2182 - mae: 0.4599 - val_loss: 297.2733 - val_mae: 13.8392\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 0.1286 - mae: 0.3515 - val_loss: 306.1625 - val_mae: 14.0192\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 279ms/step - loss: 0.0093 - mae: 0.0743 - val_loss: 313.0965 - val_mae: 14.1577\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 327ms/step - loss: 0.1459 - mae: 0.3692 - val_loss: 312.0838 - val_mae: 14.1374\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 308ms/step - loss: 0.1122 - mae: 0.3218 - val_loss: 304.8282 - val_mae: 13.9918\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 0.0044 - mae: 0.0605 - val_loss: 298.4449 - val_mae: 13.8622\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 0.0938 - mae: 0.3034 - val_loss: 298.4632 - val_mae: 13.8624\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 279ms/step - loss: 0.0925 - mae: 0.3020 - val_loss: 304.2259 - val_mae: 13.9792\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 0.0034 - mae: 0.0492 - val_loss: 310.0696 - val_mae: 14.0965\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 285ms/step - loss: 0.0579 - mae: 0.2330 - val_loss: 310.7423 - val_mae: 14.1098\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 0.0738 - mae: 0.2655 - val_loss: 306.1223 - val_mae: 14.0171\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 0.0050 - mae: 0.0599 - val_loss: 300.8401 - val_mae: 13.9102\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 0.0346 - mae: 0.1823 - val_loss: 299.5974 - val_mae: 13.8848\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 279ms/step - loss: 0.0579 - mae: 0.2367 - val_loss: 303.0399 - val_mae: 13.9546\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 0.0074 - mae: 0.0751 - val_loss: 307.6144 - val_mae: 14.0466\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 357ms/step - loss: 0.0205 - mae: 0.1372 - val_loss: 309.0755 - val_mae: 14.0758\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 0.0445 - mae: 0.2069 - val_loss: 306.3629 - val_mae: 14.0213\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 0.0090 - mae: 0.0864 - val_loss: 302.2686 - val_mae: 13.9386\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 281ms/step - loss: 0.0115 - mae: 0.0982 - val_loss: 300.5127 - val_mae: 13.9029\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 0.0330 - mae: 0.1759 - val_loss: 302.3539 - val_mae: 13.9402\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 281ms/step - loss: 0.0091 - mae: 0.0881 - val_loss: 305.7441 - val_mae: 14.0085\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 0.0058 - mae: 0.0703 - val_loss: 307.4473 - val_mae: 14.0427\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 298ms/step - loss: 0.0235 - mae: 0.1508 - val_loss: 306.0437 - val_mae: 14.0145\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 281ms/step - loss: 0.0086 - mae: 0.0893 - val_loss: 303.0480 - val_mae: 13.9540\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 0.0026 - mae: 0.0456 - val_loss: 301.2446 - val_mae: 13.9174\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 0.0164 - mae: 0.1264 - val_loss: 302.0724 - val_mae: 13.9342\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 280ms/step - loss: 0.0078 - mae: 0.0862 - val_loss: 304.4897 - val_mae: 13.9830\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 390ms/step - loss: 0.0012 - mae: 0.0312 - val_loss: 306.1385 - val_mae: 14.0163\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 279ms/step - loss: 0.0115 - mae: 0.1039 - val_loss: 305.5775 - val_mae: 14.0050\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 285ms/step - loss: 0.0070 - mae: 0.0798 - val_loss: 303.4987 - val_mae: 13.9630\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 6.2267e-04 - mae: 0.0190 - val_loss: 301.8916 - val_mae: 13.9305\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "Predicted GDP for Lviv_Oblast in 2021: [100.900215]\n",
      "Actual GDP for Lviv_Oblast in 2021: 114.9375\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - 0s 406ms/step - loss: 1687.3762 - mae: 38.9362 - val_loss: 813.1337 - val_mae: 21.9996\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 281ms/step - loss: 918.7432 - mae: 26.2751 - val_loss: 217.5945 - val_mae: 12.4515\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 276ms/step - loss: 342.2303 - mae: 16.4449 - val_loss: 16.5245 - val_mae: 3.8088\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 903.1403 - mae: 22.1676 - val_loss: 35.7176 - val_mae: 5.9468\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 276ms/step - loss: 927.0375 - mae: 24.5867 - val_loss: 31.6225 - val_mae: 5.5981\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 353ms/step - loss: 264.4951 - mae: 13.5077 - val_loss: 216.1923 - val_mae: 12.4714\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 201.0358 - mae: 11.3015 - val_loss: 408.5673 - val_mae: 16.3514\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 449.0863 - mae: 19.1606 - val_loss: 432.9565 - val_mae: 16.7960\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 304ms/step - loss: 514.6262 - mae: 21.1813 - val_loss: 287.0361 - val_mae: 14.1334\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 317.1378 - mae: 15.9839 - val_loss: 95.1875 - val_mae: 8.9986\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 91.0656 - mae: 7.7097 - val_loss: 16.3388 - val_mae: 2.9410\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 123.0899 - mae: 8.7720 - val_loss: 51.9985 - val_mae: 7.1302\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 312ms/step - loss: 313.5499 - mae: 15.6953 - val_loss: 46.9359 - val_mae: 6.8171\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 265.0240 - mae: 14.2925 - val_loss: 18.0784 - val_mae: 3.0856\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 82.4276 - mae: 7.0530 - val_loss: 57.4049 - val_mae: 7.4101\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 60.1661 - mae: 6.8727 - val_loss: 127.5124 - val_mae: 10.3154\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 156.0682 - mae: 11.1503 - val_loss: 152.5776 - val_mae: 11.1208\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 368ms/step - loss: 202.9199 - mae: 12.9950 - val_loss: 113.4113 - val_mae: 9.8792\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 145.3154 - mae: 10.7989 - val_loss: 50.5365 - val_mae: 7.0517\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 56.0226 - mae: 6.7733 - val_loss: 20.7551 - val_mae: 3.5400\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 39.5461 - mae: 4.6299 - val_loss: 35.1756 - val_mae: 5.8852\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 100.0068 - mae: 8.1303 - val_loss: 46.2289 - val_mae: 6.7988\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 124.2304 - mae: 9.5724 - val_loss: 31.1872 - val_mae: 5.4298\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 70.8357 - mae: 6.6625 - val_loss: 22.7993 - val_mae: 3.8168\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 27.3350 - mae: 3.7325 - val_loss: 38.2040 - val_mae: 6.1625\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 280ms/step - loss: 42.5230 - mae: 5.9132 - val_loss: 56.7616 - val_mae: 7.4859\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 75.0132 - mae: 7.8343 - val_loss: 56.6111 - val_mae: 7.4824\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 285ms/step - loss: 75.9781 - mae: 7.8794 - val_loss: 39.6291 - val_mae: 6.2728\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 309ms/step - loss: 45.4283 - mae: 6.0406 - val_loss: 25.5375 - val_mae: 4.3122\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 308ms/step - loss: 21.0508 - mae: 3.9057 - val_loss: 28.0703 - val_mae: 4.7594\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 29.1824 - mae: 3.9906 - val_loss: 38.1896 - val_mae: 6.0795\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 315ms/step - loss: 49.6845 - mae: 5.7559 - val_loss: 38.5787 - val_mae: 6.1088\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 47.1320 - mae: 5.6242 - val_loss: 30.0094 - val_mae: 5.0122\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 25.8655 - mae: 3.7941 - val_loss: 25.8980 - val_mae: 3.7449\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 15.7619 - mae: 3.1130 - val_loss: 29.9317 - val_mae: 5.0419\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 24.3926 - mae: 4.3963 - val_loss: 34.2812 - val_mae: 5.6555\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 34.0088 - mae: 5.2409 - val_loss: 32.7970 - val_mae: 5.4477\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 292ms/step - loss: 30.3261 - mae: 4.9292 - val_loss: 28.2164 - val_mae: 4.5540\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 285ms/step - loss: 18.2280 - mae: 3.7678 - val_loss: 27.5029 - val_mae: 4.0559\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 316ms/step - loss: 12.3354 - mae: 2.5953 - val_loss: 32.3927 - val_mae: 5.2384\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 312ms/step - loss: 17.4797 - mae: 3.1865 - val_loss: 36.9511 - val_mae: 5.8434\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 279ms/step - loss: 23.1994 - mae: 3.9612 - val_loss: 35.8312 - val_mae: 5.6956\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 293ms/step - loss: 19.9144 - mae: 3.6194 - val_loss: 31.2099 - val_mae: 4.9633\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 296ms/step - loss: 12.3274 - mae: 2.6041 - val_loss: 28.3396 - val_mae: 4.0360\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 10.0347 - mae: 2.4427 - val_loss: 28.4652 - val_mae: 4.1811\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 281ms/step - loss: 13.6135 - mae: 3.1377 - val_loss: 29.1241 - val_mae: 4.4476\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 16.1978 - mae: 3.4866 - val_loss: 28.8023 - val_mae: 4.2133\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 13.7564 - mae: 3.1403 - val_loss: 28.8790 - val_mae: 3.9948\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 9.3514 - mae: 2.4627 - val_loss: 31.1979 - val_mae: 4.8118\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 8.0788 - mae: 2.1109 - val_loss: 34.9363 - val_mae: 5.4758\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 10.2465 - mae: 2.4718 - val_loss: 37.0631 - val_mae: 5.7583\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 347ms/step - loss: 11.6056 - mae: 2.7719 - val_loss: 35.9847 - val_mae: 5.6032\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 280ms/step - loss: 9.7597 - mae: 2.4408 - val_loss: 33.1792 - val_mae: 5.1395\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 293ms/step - loss: 7.1203 - mae: 1.9616 - val_loss: 30.9729 - val_mae: 4.6033\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 280ms/step - loss: 6.6957 - mae: 1.9790 - val_loss: 30.1280 - val_mae: 4.2258\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 281ms/step - loss: 8.0168 - mae: 2.2475 - val_loss: 30.1132 - val_mae: 4.1468\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 281ms/step - loss: 8.5302 - mae: 2.3515 - val_loss: 30.6694 - val_mae: 4.3823\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 7.2617 - mae: 2.1043 - val_loss: 32.2027 - val_mae: 4.8346\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 292ms/step - loss: 5.7152 - mae: 1.7981 - val_loss: 34.7581 - val_mae: 5.3338\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 280ms/step - loss: 5.5211 - mae: 1.7628 - val_loss: 37.2473 - val_mae: 5.7020\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 308ms/step - loss: 6.3020 - mae: 1.8957 - val_loss: 38.2160 - val_mae: 5.8236\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 321ms/step - loss: 6.4723 - mae: 1.9908 - val_loss: 37.2887 - val_mae: 5.6914\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 279ms/step - loss: 5.5698 - mae: 1.8036 - val_loss: 35.4263 - val_mae: 5.4004\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 4.6718 - mae: 1.6218 - val_loss: 33.8349 - val_mae: 5.0966\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 280ms/step - loss: 4.6547 - mae: 1.5783 - val_loss: 33.0740 - val_mae: 4.9121\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 276ms/step - loss: 5.0874 - mae: 1.6500 - val_loss: 33.1702 - val_mae: 4.9184\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 297ms/step - loss: 5.0619 - mae: 1.6321 - val_loss: 34.0922 - val_mae: 5.1088\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 4.4537 - mae: 1.5278 - val_loss: 35.7684 - val_mae: 5.4080\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 296ms/step - loss: 3.9329 - mae: 1.4470 - val_loss: 37.7588 - val_mae: 5.7054\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 3.9502 - mae: 1.5403 - val_loss: 39.2445 - val_mae: 5.8991\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 4.1793 - mae: 1.6331 - val_loss: 39.5747 - val_mae: 5.9357\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 354ms/step - loss: 4.0821 - mae: 1.6297 - val_loss: 38.7948 - val_mae: 5.8288\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 280ms/step - loss: 3.6664 - mae: 1.4970 - val_loss: 37.5430 - val_mae: 5.6469\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 3.3749 - mae: 1.3775 - val_loss: 36.5154 - val_mae: 5.4803\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 3.4092 - mae: 1.2934 - val_loss: 36.1013 - val_mae: 5.4028\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 280ms/step - loss: 3.5094 - mae: 1.2823 - val_loss: 36.4017 - val_mae: 5.4459\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 3.3895 - mae: 1.2499 - val_loss: 37.3530 - val_mae: 5.5916\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 3.1132 - mae: 1.2303 - val_loss: 38.7117 - val_mae: 5.7840\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 281ms/step - loss: 2.9485 - mae: 1.2938 - val_loss: 40.0206 - val_mae: 5.9531\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 298ms/step - loss: 2.9718 - mae: 1.3644 - val_loss: 40.7829 - val_mae: 6.0441\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 3.0010 - mae: 1.4231 - val_loss: 40.7636 - val_mae: 6.0376\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 352ms/step - loss: 2.8851 - mae: 1.3858 - val_loss: 40.1376 - val_mae: 5.9548\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 279ms/step - loss: 2.7019 - mae: 1.2738 - val_loss: 39.3345 - val_mae: 5.8444\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 2.6089 - mae: 1.1906 - val_loss: 38.7644 - val_mae: 5.7604\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 2.6174 - mae: 1.1262 - val_loss: 38.6602 - val_mae: 5.7405\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 280ms/step - loss: 2.6061 - mae: 1.0997 - val_loss: 39.0671 - val_mae: 5.7935\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 293ms/step - loss: 2.5071 - mae: 1.0946 - val_loss: 39.8708 - val_mae: 5.8989\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 2.3861 - mae: 1.1217 - val_loss: 40.8164 - val_mae: 6.0173\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 2.3293 - mae: 1.1696 - val_loss: 41.5801 - val_mae: 6.1079\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 296ms/step - loss: 2.3249 - mae: 1.2336 - val_loss: 41.9093 - val_mae: 6.1441\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 280ms/step - loss: 2.2958 - mae: 1.2442 - val_loss: 41.7626 - val_mae: 6.1233\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 2.2161 - mae: 1.1990 - val_loss: 41.3176 - val_mae: 6.0658\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 437ms/step - loss: 2.1366 - mae: 1.1185 - val_loss: 40.8516 - val_mae: 6.0039\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 2.1012 - mae: 1.0456 - val_loss: 40.6024 - val_mae: 5.9683\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 2.0889 - mae: 1.0311 - val_loss: 40.6890 - val_mae: 5.9760\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 2.0544 - mae: 1.0241 - val_loss: 41.0978 - val_mae: 6.0251\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 280ms/step - loss: 1.9932 - mae: 1.0165 - val_loss: 41.7016 - val_mae: 6.0973\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 321ms/step - loss: 1.9397 - mae: 1.0428 - val_loss: 42.2989 - val_mae: 6.1666\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 319ms/step - loss: 1.9137 - mae: 1.0915 - val_loss: 42.6946 - val_mae: 6.2105\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 318ms/step - loss: 1.8962 - mae: 1.1159 - val_loss: 42.7882 - val_mae: 6.2188\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "Predicted GDP for Mykolaiv_Oblast in 2021: [114.081795]\n",
      "Actual GDP for Mykolaiv_Oblast in 2021: 111.5\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - 0s 432ms/step - loss: 3639.3491 - mae: 60.0601 - val_loss: 1780.3501 - val_mae: 33.8457\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 2525.8418 - mae: 49.9218 - val_loss: 895.5709 - val_mae: 25.3459\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 303ms/step - loss: 1042.7317 - mae: 31.7373 - val_loss: 147.0479 - val_mae: 12.0327\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 328ms/step - loss: 54.7728 - mae: 6.3775 - val_loss: 273.6310 - val_mae: 15.9644\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 336ms/step - loss: 968.6572 - mae: 30.3184 - val_loss: 570.7313 - val_mae: 21.8440\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 295ms/step - loss: 1772.0732 - mae: 41.4845 - val_loss: 193.1150 - val_mae: 13.7686\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 354ms/step - loss: 668.7949 - mae: 25.0054 - val_loss: 86.2712 - val_mae: 8.9969\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 445ms/step - loss: 39.0914 - mae: 5.2030 - val_loss: 342.3397 - val_mae: 17.3128\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 398ms/step - loss: 274.6955 - mae: 15.6405 - val_loss: 612.5703 - val_mae: 21.9433\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 365ms/step - loss: 679.6251 - mae: 25.5374 - val_loss: 714.8990 - val_mae: 23.4042\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 344ms/step - loss: 849.4791 - mae: 28.6904 - val_loss: 598.7458 - val_mae: 21.8077\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 348ms/step - loss: 670.2589 - mae: 25.3800 - val_loss: 361.7489 - val_mae: 17.8268\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 344ms/step - loss: 315.6087 - mae: 16.9985 - val_loss: 139.4785 - val_mae: 11.8081\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 407ms/step - loss: 45.6790 - mae: 5.6891 - val_loss: 87.7743 - val_mae: 7.9585\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 360ms/step - loss: 136.6039 - mae: 10.3306 - val_loss: 163.7938 - val_mae: 12.7833\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 339ms/step - loss: 406.0756 - mae: 19.3685 - val_loss: 156.0874 - val_mae: 12.4563\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 339ms/step - loss: 371.8393 - mae: 18.4949 - val_loss: 93.3962 - val_mae: 8.3989\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 342ms/step - loss: 133.9210 - mae: 10.3313 - val_loss: 103.1071 - val_mae: 9.6334\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 339ms/step - loss: 24.7316 - mae: 4.2087 - val_loss: 185.4368 - val_mae: 13.5793\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 374ms/step - loss: 90.3352 - mae: 8.2060 - val_loss: 263.8832 - val_mae: 15.8711\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 335ms/step - loss: 192.8129 - mae: 13.0740 - val_loss: 286.1051 - val_mae: 16.4303\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 340ms/step - loss: 226.1646 - mae: 14.3183 - val_loss: 247.1352 - val_mae: 15.4515\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 324ms/step - loss: 173.3708 - mae: 12.3504 - val_loss: 175.8594 - val_mae: 13.2536\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 332ms/step - loss: 82.1255 - mae: 7.8263 - val_loss: 113.8966 - val_mae: 10.2955\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 339ms/step - loss: 24.1473 - mae: 3.9251 - val_loss: 90.4084 - val_mae: 7.2752\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 392ms/step - loss: 41.6534 - mae: 5.1785 - val_loss: 98.6490 - val_mae: 8.5370\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 311ms/step - loss: 102.2014 - mae: 8.9687 - val_loss: 105.9830 - val_mae: 9.3065\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 127.5066 - mae: 10.2968 - val_loss: 98.0349 - val_mae: 8.3390\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 314ms/step - loss: 89.4808 - mae: 8.2938 - val_loss: 92.5416 - val_mae: 7.3354\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 316ms/step - loss: 36.4291 - mae: 4.8875 - val_loss: 106.6043 - val_mae: 9.5922\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 296ms/step - loss: 19.0487 - mae: 3.6466 - val_loss: 134.7411 - val_mae: 11.4628\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 306ms/step - loss: 38.9342 - mae: 5.0751 - val_loss: 158.5254 - val_mae: 12.5737\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 305ms/step - loss: 65.7487 - mae: 6.9660 - val_loss: 164.0435 - val_mae: 12.8005\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 299ms/step - loss: 73.0927 - mae: 7.5013 - val_loss: 149.9666 - val_mae: 12.1950\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 322ms/step - loss: 56.3900 - mae: 6.3093 - val_loss: 125.9783 - val_mae: 10.9422\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 312ms/step - loss: 30.7612 - mae: 4.3872 - val_loss: 105.4260 - val_mae: 9.3474\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 16.7464 - mae: 3.4367 - val_loss: 96.1690 - val_mae: 7.8056\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 312ms/step - loss: 22.9912 - mae: 4.1990 - val_loss: 95.9682 - val_mae: 7.1267\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 313ms/step - loss: 38.6695 - mae: 5.0727 - val_loss: 97.2109 - val_mae: 7.5286\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 320ms/step - loss: 45.0873 - mae: 5.5457 - val_loss: 96.4001 - val_mae: 7.1115\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 306ms/step - loss: 35.7803 - mae: 4.8563 - val_loss: 96.8235 - val_mae: 7.7165\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 309ms/step - loss: 21.2366 - mae: 4.0431 - val_loss: 102.3957 - val_mae: 8.8649\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 305ms/step - loss: 14.6281 - mae: 3.3624 - val_loss: 112.0353 - val_mae: 9.8819\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 307ms/step - loss: 18.6904 - mae: 3.4910 - val_loss: 120.5899 - val_mae: 10.5355\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 312ms/step - loss: 26.3231 - mae: 4.0551 - val_loss: 123.3110 - val_mae: 10.7153\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 29.2986 - mae: 4.3652 - val_loss: 119.1846 - val_mae: 10.4253\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 25.1190 - mae: 3.9522 - val_loss: 111.0946 - val_mae: 9.7656\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 304ms/step - loss: 17.7142 - mae: 3.3910 - val_loss: 103.4264 - val_mae: 8.9102\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 337ms/step - loss: 13.1481 - mae: 3.0518 - val_loss: 99.0270 - val_mae: 8.0737\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 319ms/step - loss: 14.2135 - mae: 3.4517 - val_loss: 97.7741 - val_mae: 7.4615\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 306ms/step - loss: 18.2927 - mae: 3.7367 - val_loss: 97.8003 - val_mae: 7.2106\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 319ms/step - loss: 20.4016 - mae: 3.8246 - val_loss: 97.9169 - val_mae: 7.3478\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 297ms/step - loss: 18.2824 - mae: 3.7019 - val_loss: 98.5839 - val_mae: 7.7866\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 306ms/step - loss: 14.1233 - mae: 3.4163 - val_loss: 100.6945 - val_mae: 8.3676\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 317ms/step - loss: 11.5886 - mae: 3.0549 - val_loss: 104.0202 - val_mae: 8.9174\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 12.0987 - mae: 2.8866 - val_loss: 107.0812 - val_mae: 9.2965\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 14.1052 - mae: 3.0683 - val_loss: 108.3010 - val_mae: 9.4263\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 313ms/step - loss: 15.1455 - mae: 3.1245 - val_loss: 107.1664 - val_mae: 9.2949\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 306ms/step - loss: 14.1213 - mae: 3.0489 - val_loss: 104.4770 - val_mae: 8.9509\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 312ms/step - loss: 11.9302 - mae: 2.8662 - val_loss: 101.6603 - val_mae: 8.4884\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 309ms/step - loss: 10.3409 - mae: 2.7199 - val_loss: 99.7700 - val_mae: 8.0236\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 317ms/step - loss: 10.3192 - mae: 2.9252 - val_loss: 98.9698 - val_mae: 7.6668\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 305ms/step - loss: 11.2908 - mae: 3.0724 - val_loss: 98.8074 - val_mae: 7.4934\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 309ms/step - loss: 11.8886 - mae: 3.1213 - val_loss: 98.8836 - val_mae: 7.5241\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 320ms/step - loss: 11.3384 - mae: 3.0610 - val_loss: 99.2051 - val_mae: 7.7222\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 311ms/step - loss: 10.0843 - mae: 2.9115 - val_loss: 99.9490 - val_mae: 8.0109\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 374ms/step - loss: 9.1412 - mae: 2.7144 - val_loss: 101.0456 - val_mae: 8.2990\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 297ms/step - loss: 9.0463 - mae: 2.5183 - val_loss: 102.0666 - val_mae: 8.5071\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 308ms/step - loss: 9.4824 - mae: 2.5621 - val_loss: 102.5152 - val_mae: 8.5851\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 322ms/step - loss: 9.7405 - mae: 2.5934 - val_loss: 102.1938 - val_mae: 8.5201\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 311ms/step - loss: 9.4181 - mae: 2.5523 - val_loss: 101.3248 - val_mae: 8.3346\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 307ms/step - loss: 8.7089 - mae: 2.4507 - val_loss: 100.3599 - val_mae: 8.0782\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 305ms/step - loss: 8.1193 - mae: 2.4289 - val_loss: 99.6651 - val_mae: 7.8142\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 7.9690 - mae: 2.5284 - val_loss: 99.3376 - val_mae: 7.6050\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 321ms/step - loss: 8.1371 - mae: 2.6003 - val_loss: 99.2530 - val_mae: 7.4910\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 307ms/step - loss: 8.2412 - mae: 2.6223 - val_loss: 99.2784 - val_mae: 7.4860\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 350ms/step - loss: 8.0326 - mae: 2.5871 - val_loss: 99.3862 - val_mae: 7.5735\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 331ms/step - loss: 7.6029 - mae: 2.5034 - val_loss: 99.6108 - val_mae: 7.7148\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 311ms/step - loss: 7.2338 - mae: 2.3918 - val_loss: 99.9316 - val_mae: 7.8612\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 310ms/step - loss: 7.1024 - mae: 2.2781 - val_loss: 100.2306 - val_mae: 7.9685\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 310ms/step - loss: 7.1408 - mae: 2.2044 - val_loss: 100.3650 - val_mae: 8.0073\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 7.1473 - mae: 2.2118 - val_loss: 100.2751 - val_mae: 7.9684\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 314ms/step - loss: 6.9930 - mae: 2.1851 - val_loss: 100.0270 - val_mae: 7.8628\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 329ms/step - loss: 6.7216 - mae: 2.1463 - val_loss: 99.7565 - val_mae: 7.7173\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 307ms/step - loss: 6.4782 - mae: 2.1769 - val_loss: 99.5732 - val_mae: 7.5666\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 309ms/step - loss: 6.3609 - mae: 2.2217 - val_loss: 99.5012 - val_mae: 7.4437\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 322ms/step - loss: 6.3389 - mae: 2.2522 - val_loss: 99.4988 - val_mae: 7.3715\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 400ms/step - loss: 6.3036 - mae: 2.2565 - val_loss: 99.5184 - val_mae: 7.3579\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 305ms/step - loss: 6.1818 - mae: 2.2304 - val_loss: 99.5468 - val_mae: 7.3947\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 5.9964 - mae: 2.1785 - val_loss: 99.5951 - val_mae: 7.4612\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 314ms/step - loss: 5.8275 - mae: 2.1114 - val_loss: 99.6647 - val_mae: 7.5313\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 306ms/step - loss: 5.7273 - mae: 2.0429 - val_loss: 99.7321 - val_mae: 7.5810\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 315ms/step - loss: 5.6778 - mae: 1.9853 - val_loss: 99.7653 - val_mae: 7.5937\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 5.6210 - mae: 1.9631 - val_loss: 99.7534 - val_mae: 7.5643\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 299ms/step - loss: 5.5190 - mae: 1.9455 - val_loss: 99.7175 - val_mae: 7.4989\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 299ms/step - loss: 5.3845 - mae: 1.9347 - val_loss: 99.6928 - val_mae: 7.4122\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 304ms/step - loss: 5.2599 - mae: 1.9491 - val_loss: 99.7013 - val_mae: 7.3233\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 319ms/step - loss: 5.1726 - mae: 1.9650 - val_loss: 99.7382 - val_mae: 7.2498\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 369ms/step - loss: 5.1140 - mae: 1.9734 - val_loss: 99.7809 - val_mae: 7.2042\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 311ms/step - loss: 5.0520 - mae: 1.9677 - val_loss: 99.8101 - val_mae: 7.1901\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "Predicted GDP for Odessa_Oblast in 2021: [112.0394]\n",
      "Actual GDP for Odessa_Oblast in 2021: 116.5\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - 0s 449ms/step - loss: 14730.3594 - mae: 114.0788 - val_loss: 112.4993 - val_mae: 10.3064\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 373ms/step - loss: 1411.6357 - mae: 31.8871 - val_loss: 768.1127 - val_mae: 24.4733\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 318ms/step - loss: 767.5573 - mae: 26.6470 - val_loss: 1384.6608 - val_mae: 31.0442\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 311ms/step - loss: 2618.2603 - mae: 50.8880 - val_loss: 1777.2554 - val_mae: 34.3681\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 335ms/step - loss: 3784.9092 - mae: 61.2795 - val_loss: 1993.1804 - val_mae: 36.0349\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 311ms/step - loss: 4381.9688 - mae: 65.9528 - val_loss: 2100.6062 - val_mae: 36.8804\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 322ms/step - loss: 4616.9648 - mae: 67.7032 - val_loss: 2136.1565 - val_mae: 37.2492\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 316ms/step - loss: 4605.5874 - mae: 67.6147 - val_loss: 2118.2363 - val_mae: 37.2970\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 344ms/step - loss: 4401.9014 - mae: 66.0836 - val_loss: 2060.2537 - val_mae: 37.1363\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 357ms/step - loss: 4039.2158 - mae: 63.2537 - val_loss: 1966.6420 - val_mae: 36.7836\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 321ms/step - loss: 3530.9458 - mae: 59.0323 - val_loss: 1836.9041 - val_mae: 36.1935\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 317ms/step - loss: 2876.3721 - mae: 53.0323 - val_loss: 1666.4290 - val_mae: 35.2857\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 313ms/step - loss: 2104.1780 - mae: 44.7669 - val_loss: 1456.4332 - val_mae: 33.9854\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 315ms/step - loss: 1296.1736 - mae: 33.5002 - val_loss: 1218.3163 - val_mae: 32.2383\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 318ms/step - loss: 662.5543 - mae: 22.5291 - val_loss: 974.2711 - val_mae: 30.0026\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 311ms/step - loss: 554.2190 - mae: 19.2640 - val_loss: 781.3637 - val_mae: 27.6903\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 319ms/step - loss: 1080.7137 - mae: 26.8423 - val_loss: 690.6862 - val_mae: 26.2725\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 332ms/step - loss: 1457.6882 - mae: 31.6336 - val_loss: 686.0297 - val_mae: 26.1919\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 315ms/step - loss: 1167.7389 - mae: 28.3424 - val_loss: 730.8638 - val_mae: 27.0211\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 397ms/step - loss: 617.0784 - mae: 20.7218 - val_loss: 796.9294 - val_mae: 28.1200\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 285.5424 - mae: 14.7989 - val_loss: 860.0717 - val_mae: 29.0575\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 316ms/step - loss: 235.0458 - mae: 11.9184 - val_loss: 904.3049 - val_mae: 29.6734\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 329ms/step - loss: 334.5865 - mae: 15.5361 - val_loss: 921.1960 - val_mae: 29.9269\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 323ms/step - loss: 437.4684 - mae: 19.3984 - val_loss: 910.3231 - val_mae: 29.8370\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 451.7995 - mae: 20.0441 - val_loss: 875.8932 - val_mae: 29.4247\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 314ms/step - loss: 372.2061 - mae: 17.9994 - val_loss: 826.2111 - val_mae: 28.7186\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 366ms/step - loss: 244.1817 - mae: 13.8710 - val_loss: 773.0692 - val_mae: 27.7737\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 441ms/step - loss: 127.9824 - mae: 8.3942 - val_loss: 728.6525 - val_mae: 26.6753\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 498ms/step - loss: 73.6599 - mae: 7.6907 - val_loss: 701.8274 - val_mae: 25.5698\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 335ms/step - loss: 96.7111 - mae: 8.7687 - val_loss: 695.0079 - val_mae: 24.6826\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 328ms/step - loss: 162.6472 - mae: 10.6558 - val_loss: 700.4241 - val_mae: 24.2001\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 316ms/step - loss: 206.0668 - mae: 12.1087 - val_loss: 708.6989 - val_mae: 24.1926\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 328ms/step - loss: 189.4629 - mae: 11.5972 - val_loss: 716.7766 - val_mae: 24.5824\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 359ms/step - loss: 130.4569 - mae: 9.5200 - val_loss: 725.7559 - val_mae: 25.1889\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 321ms/step - loss: 71.8839 - mae: 7.1748 - val_loss: 736.6710 - val_mae: 25.8309\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 327ms/step - loss: 41.9625 - mae: 5.4719 - val_loss: 748.7723 - val_mae: 26.3902\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 328ms/step - loss: 43.7700 - mae: 5.8767 - val_loss: 759.9155 - val_mae: 26.7999\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 325ms/step - loss: 62.6026 - mae: 6.3579 - val_loss: 767.8852 - val_mae: 27.0304\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 339ms/step - loss: 80.8972 - mae: 7.4828 - val_loss: 771.7612 - val_mae: 27.0786\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 426ms/step - loss: 86.5604 - mae: 7.8935 - val_loss: 772.0032 - val_mae: 26.9548\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 331ms/step - loss: 76.8643 - mae: 7.3715 - val_loss: 770.5981 - val_mae: 26.6842\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 453ms/step - loss: 56.8210 - mae: 6.2543 - val_loss: 770.1029 - val_mae: 26.3052\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 404ms/step - loss: 35.6735 - mae: 4.9748 - val_loss: 772.5756 - val_mae: 25.8650\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 326ms/step - loss: 22.1165 - mae: 4.2268 - val_loss: 778.7727 - val_mae: 25.4222\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 310ms/step - loss: 20.2346 - mae: 3.7311 - val_loss: 787.5421 - val_mae: 25.0391\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 313ms/step - loss: 27.4422 - mae: 4.3361 - val_loss: 796.2750 - val_mae: 24.7688\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 317ms/step - loss: 36.3987 - mae: 4.9453 - val_loss: 802.2840 - val_mae: 24.6422\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 314ms/step - loss: 39.7873 - mae: 5.0861 - val_loss: 804.2826 - val_mae: 24.6630\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 309ms/step - loss: 34.9293 - mae: 4.7539 - val_loss: 802.8167 - val_mae: 24.8071\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 384ms/step - loss: 24.8037 - mae: 4.0371 - val_loss: 799.5268 - val_mae: 25.0286\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 324ms/step - loss: 14.9988 - mae: 3.0819 - val_loss: 796.1393 - val_mae: 25.2769\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 353ms/step - loss: 9.7348 - mae: 2.7679 - val_loss: 793.7085 - val_mae: 25.5058\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 318ms/step - loss: 9.8150 - mae: 2.6563 - val_loss: 792.5256 - val_mae: 25.6803\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 307ms/step - loss: 13.0380 - mae: 3.0885 - val_loss: 792.4622 - val_mae: 25.7794\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 306ms/step - loss: 16.1792 - mae: 3.4094 - val_loss: 793.2972 - val_mae: 25.7937\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 313ms/step - loss: 16.8705 - mae: 3.5203 - val_loss: 795.0336 - val_mae: 25.7275\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 333ms/step - loss: 14.5620 - mae: 3.2695 - val_loss: 797.8095 - val_mae: 25.5942\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 316ms/step - loss: 10.3826 - mae: 2.7158 - val_loss: 801.7256 - val_mae: 25.4150\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 394ms/step - loss: 6.2979 - mae: 2.1568 - val_loss: 806.6320 - val_mae: 25.2161\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 313ms/step - loss: 3.9622 - mae: 1.7823 - val_loss: 811.9924 - val_mae: 25.0258\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 313ms/step - loss: 3.8965 - mae: 1.6711 - val_loss: 816.9325 - val_mae: 24.8702\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 321ms/step - loss: 5.3354 - mae: 1.6814 - val_loss: 820.5493 - val_mae: 24.7691\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 307ms/step - loss: 6.8152 - mae: 2.0302 - val_loss: 822.2458 - val_mae: 24.7329\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 7.1227 - mae: 2.1738 - val_loss: 821.9406 - val_mae: 24.7595\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 5.9762 - mae: 1.9627 - val_loss: 820.0688 - val_mae: 24.8368\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 296ms/step - loss: 4.0316 - mae: 1.5176 - val_loss: 817.3582 - val_mae: 24.9454\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 320ms/step - loss: 2.3068 - mae: 1.1316 - val_loss: 814.5428 - val_mae: 25.0635\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 299ms/step - loss: 1.5187 - mae: 1.1052 - val_loss: 812.1716 - val_mae: 25.1704\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 1.7264 - mae: 1.2268 - val_loss: 810.5560 - val_mae: 25.2504\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 298ms/step - loss: 2.4468 - mae: 1.3376 - val_loss: 809.8118 - val_mae: 25.2933\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 319ms/step - loss: 3.0290 - mae: 1.4599 - val_loss: 809.9433 - val_mae: 25.2963\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 309ms/step - loss: 3.0466 - mae: 1.4640 - val_loss: 810.8768 - val_mae: 25.2618\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 311ms/step - loss: 2.4783 - mae: 1.3113 - val_loss: 812.4949 - val_mae: 25.1979\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 303ms/step - loss: 1.6425 - mae: 1.0896 - val_loss: 814.6030 - val_mae: 25.1165\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 325ms/step - loss: 0.9630 - mae: 0.8602 - val_loss: 816.9144 - val_mae: 25.0309\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 292ms/step - loss: 0.7122 - mae: 0.7094 - val_loss: 819.0762 - val_mae: 24.9544\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 297ms/step - loss: 0.8800 - mae: 0.8112 - val_loss: 820.7406 - val_mae: 24.8975\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 293ms/step - loss: 1.2286 - mae: 0.9497 - val_loss: 821.6508 - val_mae: 24.8673\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 326ms/step - loss: 1.4629 - mae: 1.0211 - val_loss: 821.7101 - val_mae: 24.8653\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 343ms/step - loss: 1.4135 - mae: 1.0062 - val_loss: 821.0071 - val_mae: 24.8888\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 1.1126 - mae: 0.9070 - val_loss: 819.7698 - val_mae: 24.9309\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 296ms/step - loss: 0.7397 - mae: 0.7524 - val_loss: 818.2927 - val_mae: 24.9823\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 296ms/step - loss: 0.4846 - mae: 0.6489 - val_loss: 816.8558 - val_mae: 25.0335\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 313ms/step - loss: 0.4361 - mae: 0.5479 - val_loss: 815.6807 - val_mae: 25.0759\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 298ms/step - loss: 0.5483 - mae: 0.5673 - val_loss: 814.9075 - val_mae: 25.1035\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 297ms/step - loss: 0.6959 - mae: 0.6365 - val_loss: 814.5986 - val_mae: 25.1134\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 303ms/step - loss: 0.7602 - mae: 0.6790 - val_loss: 814.7460 - val_mae: 25.1053\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 297ms/step - loss: 0.6956 - mae: 0.6463 - val_loss: 815.2865 - val_mae: 25.0820\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 347ms/step - loss: 0.5392 - mae: 0.5475 - val_loss: 816.1093 - val_mae: 25.0482\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 325ms/step - loss: 0.3760 - mae: 0.4833 - val_loss: 817.0710 - val_mae: 25.0101\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 296ms/step - loss: 0.2807 - mae: 0.4646 - val_loss: 818.0088 - val_mae: 24.9738\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 298ms/step - loss: 0.2768 - mae: 0.4708 - val_loss: 818.7652 - val_mae: 24.9447\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 314ms/step - loss: 0.3322 - mae: 0.4968 - val_loss: 819.2219 - val_mae: 24.9265\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 312ms/step - loss: 0.3870 - mae: 0.5382 - val_loss: 819.3201 - val_mae: 24.9209\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 308ms/step - loss: 0.3935 - mae: 0.5405 - val_loss: 819.0737 - val_mae: 24.9275\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 313ms/step - loss: 0.3424 - mae: 0.5049 - val_loss: 818.5549 - val_mae: 24.9437\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 0.2611 - mae: 0.4390 - val_loss: 817.8780 - val_mae: 24.9659\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 298ms/step - loss: 0.1906 - mae: 0.3739 - val_loss: 817.1681 - val_mae: 24.9898\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 328ms/step - loss: 0.1583 - mae: 0.3601 - val_loss: 816.5386 - val_mae: 25.0113\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 334ms/step - loss: 0.1648 - mae: 0.3465 - val_loss: 816.0726 - val_mae: 25.0271\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "Predicted GDP for Poltava_Oblast in 2021: [69.92989]\n",
      "Actual GDP for Poltava_Oblast in 2021: 81.125\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - 0s 441ms/step - loss: 2225.0461 - mae: 47.0571 - val_loss: 2124.8228 - val_mae: 45.8140\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 2001.5352 - mae: 44.6200 - val_loss: 2059.2087 - val_mae: 45.3697\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 303ms/step - loss: 1605.8699 - mae: 39.9428 - val_loss: 2033.9380 - val_mae: 44.7672\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 308ms/step - loss: 1093.4529 - mae: 32.9126 - val_loss: 2137.3936 - val_mae: 43.9930\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 296ms/step - loss: 554.0299 - mae: 23.3264 - val_loss: 2506.8052 - val_mae: 43.0872\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 295ms/step - loss: 133.1322 - mae: 11.1154 - val_loss: 3275.1526 - val_mae: 42.0688\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 356ms/step - loss: 19.3384 - mae: 3.9456 - val_loss: 4385.4771 - val_mae: 51.9769\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 308.4487 - mae: 17.3042 - val_loss: 5301.2725 - val_mae: 60.6463\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 295ms/step - loss: 722.5931 - mae: 26.7186 - val_loss: 5443.6738 - val_mae: 62.0218\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 293ms/step - loss: 817.2170 - mae: 28.4353 - val_loss: 4879.5879 - val_mae: 57.1114\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 306ms/step - loss: 552.5720 - mae: 23.3208 - val_loss: 4037.1863 - val_mae: 48.7232\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 292ms/step - loss: 217.1610 - mae: 14.4326 - val_loss: 3288.7507 - val_mae: 41.4384\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 299ms/step - loss: 31.5047 - mae: 5.1027 - val_loss: 2769.3123 - val_mae: 42.0433\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 23.9985 - mae: 4.0073 - val_loss: 2461.3135 - val_mae: 42.5364\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 297ms/step - loss: 118.3919 - mae: 10.4403 - val_loss: 2295.6177 - val_mae: 42.8568\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 315ms/step - loss: 230.5047 - mae: 14.8666 - val_loss: 2217.4412 - val_mae: 43.0019\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 306.6119 - mae: 17.2359 - val_loss: 2193.7852 - val_mae: 42.9771\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 372ms/step - loss: 325.2011 - mae: 17.7672 - val_loss: 2212.6272 - val_mae: 42.8038\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 287.9899 - mae: 16.6887 - val_loss: 2274.3735 - val_mae: 42.4996\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 303ms/step - loss: 211.7527 - mae: 14.2254 - val_loss: 2386.0698 - val_mae: 42.0870\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 293ms/step - loss: 122.1369 - mae: 10.6240 - val_loss: 2553.8240 - val_mae: 41.5951\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 299ms/step - loss: 47.4913 - mae: 6.1942 - val_loss: 2773.8381 - val_mae: 41.0663\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 298ms/step - loss: 10.8117 - mae: 2.4911 - val_loss: 3023.1765 - val_mae: 40.5515\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 292ms/step - loss: 20.0093 - mae: 4.0524 - val_loss: 3259.6843 - val_mae: 40.6201\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 309ms/step - loss: 61.7630 - mae: 7.2833 - val_loss: 3427.4644 - val_mae: 42.9157\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 299ms/step - loss: 106.4737 - mae: 9.8914 - val_loss: 3484.7109 - val_mae: 43.7035\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 298ms/step - loss: 125.6804 - mae: 10.8209 - val_loss: 3424.4451 - val_mae: 42.9848\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 299ms/step - loss: 110.0150 - mae: 10.0715 - val_loss: 3275.2686 - val_mae: 41.0579\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 72.1501 - mae: 7.9719 - val_loss: 3083.3223 - val_mae: 40.1151\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 293ms/step - loss: 34.0792 - mae: 5.3035 - val_loss: 2891.1372 - val_mae: 40.3946\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 299ms/step - loss: 12.0681 - mae: 2.9436 - val_loss: 2726.2812 - val_mae: 40.6611\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 298ms/step - loss: 10.1467 - mae: 2.4224 - val_loss: 2600.9250 - val_mae: 40.8809\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 22.3965 - mae: 3.8683 - val_loss: 2516.8606 - val_mae: 41.0336\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 312ms/step - loss: 38.8922 - mae: 5.4847 - val_loss: 2471.0234 - val_mae: 41.1099\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 50.9051 - mae: 6.4876 - val_loss: 2459.1780 - val_mae: 41.1083\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 53.6972 - mae: 6.7004 - val_loss: 2477.4905 - val_mae: 41.0347\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 47.0813 - mae: 6.1898 - val_loss: 2522.4771 - val_mae: 40.9002\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 339ms/step - loss: 34.4517 - mae: 5.0730 - val_loss: 2589.9856 - val_mae: 40.7208\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 348ms/step - loss: 20.9652 - mae: 3.7267 - val_loss: 2673.8840 - val_mae: 40.5162\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 304ms/step - loss: 11.4412 - mae: 2.5303 - val_loss: 2765.1982 - val_mae: 40.3080\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 8.5461 - mae: 2.1645 - val_loss: 2852.3750 - val_mae: 40.1191\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 11.8684 - mae: 2.9193 - val_loss: 2923.0051 - val_mae: 39.9692\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 305ms/step - loss: 18.3555 - mae: 3.8630 - val_loss: 2966.7290 - val_mae: 39.8726\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 298ms/step - loss: 23.9775 - mae: 4.4551 - val_loss: 2978.0962 - val_mae: 39.8357\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 25.7862 - mae: 4.6211 - val_loss: 2958.0132 - val_mae: 39.8558\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 23.2045 - mae: 4.3801 - val_loss: 2913.0417 - val_mae: 39.9229\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 307ms/step - loss: 17.8904 - mae: 3.8075 - val_loss: 2853.0303 - val_mae: 40.0215\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 376ms/step - loss: 12.4693 - mae: 3.0153 - val_loss: 2788.3245 - val_mae: 40.1343\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 312ms/step - loss: 9.0635 - mae: 2.4064 - val_loss: 2727.6611 - val_mae: 40.2447\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 293ms/step - loss: 8.4446 - mae: 2.1583 - val_loss: 2677.2053 - val_mae: 40.3389\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 313ms/step - loss: 10.0324 - mae: 2.4072 - val_loss: 2640.5312 - val_mae: 40.4071\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 360ms/step - loss: 12.4793 - mae: 2.6584 - val_loss: 2619.1211 - val_mae: 40.4438\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 480ms/step - loss: 14.4090 - mae: 2.9277 - val_loss: 2612.9124 - val_mae: 40.4474\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 360ms/step - loss: 14.9653 - mae: 3.0120 - val_loss: 2620.7354 - val_mae: 40.4201\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 335ms/step - loss: 14.0306 - mae: 2.8735 - val_loss: 2640.5210 - val_mae: 40.3667\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 324ms/step - loss: 12.1204 - mae: 2.6202 - val_loss: 2669.3750 - val_mae: 40.2947\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 328ms/step - loss: 10.0640 - mae: 2.4093 - val_loss: 2703.6650 - val_mae: 40.2127\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 392ms/step - loss: 8.6212 - mae: 2.2280 - val_loss: 2739.2095 - val_mae: 40.1300\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 328ms/step - loss: 8.1821 - mae: 2.1371 - val_loss: 2771.6855 - val_mae: 40.0553\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 327ms/step - loss: 8.6578 - mae: 2.3160 - val_loss: 2797.1956 - val_mae: 39.9960\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 317ms/step - loss: 9.5890 - mae: 2.5604 - val_loss: 2812.8999 - val_mae: 39.9569\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 329ms/step - loss: 10.4026 - mae: 2.7101 - val_loss: 2817.4785 - val_mae: 39.9403\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 335ms/step - loss: 10.6850 - mae: 2.7557 - val_loss: 2811.3052 - val_mae: 39.9452\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 316ms/step - loss: 10.3405 - mae: 2.7013 - val_loss: 2796.2493 - val_mae: 39.9681\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 317ms/step - loss: 9.5769 - mae: 2.5635 - val_loss: 2775.1851 - val_mae: 40.0036\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 324ms/step - loss: 8.7525 - mae: 2.3675 - val_loss: 2751.3877 - val_mae: 40.0455\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 318ms/step - loss: 8.1853 - mae: 2.1532 - val_loss: 2727.9958 - val_mae: 40.0875\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 392ms/step - loss: 8.0209 - mae: 2.1244 - val_loss: 2707.6091 - val_mae: 40.1241\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 448ms/step - loss: 8.2063 - mae: 2.1567 - val_loss: 2692.0918 - val_mae: 40.1510\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 360ms/step - loss: 8.5540 - mae: 2.2313 - val_loss: 2682.5076 - val_mae: 40.1655\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 308ms/step - loss: 8.8500 - mae: 2.2762 - val_loss: 2679.1863 - val_mae: 40.1666\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 308ms/step - loss: 8.9467 - mae: 2.2892 - val_loss: 2681.7832 - val_mae: 40.1548\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 8.8112 - mae: 2.2718 - val_loss: 2689.4099 - val_mae: 40.1323\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 335ms/step - loss: 8.5158 - mae: 2.2288 - val_loss: 2700.7361 - val_mae: 40.1020\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 318ms/step - loss: 8.1889 - mae: 2.1672 - val_loss: 2714.1416 - val_mae: 40.0675\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 410ms/step - loss: 7.9502 - mae: 2.1138 - val_loss: 2727.8792 - val_mae: 40.0326\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 489ms/step - loss: 7.8630 - mae: 2.1119 - val_loss: 2740.2693 - val_mae: 40.0008\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 430ms/step - loss: 7.9156 - mae: 2.1101 - val_loss: 2749.9021 - val_mae: 39.9749\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 390ms/step - loss: 8.0396 - mae: 2.1517 - val_loss: 2755.8081 - val_mae: 39.9570\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 334ms/step - loss: 8.1493 - mae: 2.2098 - val_loss: 2757.5601 - val_mae: 39.9477\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 8.1821 - mae: 2.2281 - val_loss: 2755.3013 - val_mae: 39.9468\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 315ms/step - loss: 8.1216 - mae: 2.2079 - val_loss: 2749.6648 - val_mae: 39.9530\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 306ms/step - loss: 7.9961 - mae: 2.1553 - val_loss: 2741.6477 - val_mae: 39.9643\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 310ms/step - loss: 7.8580 - mae: 2.1049 - val_loss: 2732.4197 - val_mae: 39.9782\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 327ms/step - loss: 7.7556 - mae: 2.1007 - val_loss: 2723.1626 - val_mae: 39.9923\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 324ms/step - loss: 7.7129 - mae: 2.0996 - val_loss: 2714.9163 - val_mae: 40.0043\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 317ms/step - loss: 7.7244 - mae: 2.0985 - val_loss: 2708.4802 - val_mae: 40.0126\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 342ms/step - loss: 7.7625 - mae: 2.0973 - val_loss: 2704.3525 - val_mae: 40.0159\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 408ms/step - loss: 7.7945 - mae: 2.0960 - val_loss: 2702.7180 - val_mae: 40.0139\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 321ms/step - loss: 7.7967 - mae: 2.0999 - val_loss: 2703.4697 - val_mae: 40.0067\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 7.7633 - mae: 2.0931 - val_loss: 2706.2422 - val_mae: 39.9951\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 488ms/step - loss: 7.7050 - mae: 2.0916 - val_loss: 2710.4810 - val_mae: 39.9805\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 368ms/step - loss: 7.6417 - mae: 2.0900 - val_loss: 2715.5139 - val_mae: 39.9641\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 326ms/step - loss: 7.5920 - mae: 2.0883 - val_loss: 2720.6331 - val_mae: 39.9475\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 296ms/step - loss: 7.5656 - mae: 2.0867 - val_loss: 2725.1836 - val_mae: 39.9321\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 317ms/step - loss: 7.5605 - mae: 2.0851 - val_loss: 2728.6318 - val_mae: 39.9191\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 304ms/step - loss: 7.5660 - mae: 2.0835 - val_loss: 2730.6211 - val_mae: 39.9091\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 436ms/step - loss: 7.5693 - mae: 2.0821 - val_loss: 2731.0071 - val_mae: 39.9025\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 462ms/step - loss: 7.5610 - mae: 2.0807 - val_loss: 2729.8499 - val_mae: 39.8993\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 359ms/step - loss: 7.5387 - mae: 2.0793 - val_loss: 2727.3958 - val_mae: 39.8988\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "Predicted GDP for Rivne_Oblast in 2021: [102.01455]\n",
      "Actual GDP for Rivne_Oblast in 2021: 105.0625\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - 1s 517ms/step - loss: 53.0419 - mae: 6.8898 - val_loss: 2659.5491 - val_mae: 37.2988\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 344ms/step - loss: 35.4056 - mae: 5.4714 - val_loss: 2584.3625 - val_mae: 37.7364\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 333ms/step - loss: 15.4533 - mae: 3.2339 - val_loss: 2504.6960 - val_mae: 38.2291\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 439ms/step - loss: 5.4952 - mae: 2.0522 - val_loss: 2436.7681 - val_mae: 38.6800\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 484ms/step - loss: 8.2277 - mae: 2.3479 - val_loss: 2388.7786 - val_mae: 39.0139\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 387ms/step - loss: 17.8608 - mae: 3.6033 - val_loss: 2363.2532 - val_mae: 39.2022\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 399ms/step - loss: 26.1002 - mae: 4.6177 - val_loss: 2359.5654 - val_mae: 39.2371\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 317ms/step - loss: 27.6601 - mae: 4.7920 - val_loss: 2375.8076 - val_mae: 39.1333\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 318ms/step - loss: 22.2639 - mae: 4.1984 - val_loss: 2409.0098 - val_mae: 38.9206\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 318ms/step - loss: 13.6030 - mae: 3.0830 - val_loss: 2454.5457 - val_mae: 38.6404\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 303ms/step - loss: 6.6103 - mae: 2.0874 - val_loss: 2505.5115 - val_mae: 38.3409\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 331ms/step - loss: 4.5883 - mae: 1.8776 - val_loss: 2553.0637 - val_mae: 38.0716\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 314ms/step - loss: 7.4419 - mae: 2.3109 - val_loss: 2588.2690 - val_mae: 37.8835\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 375ms/step - loss: 12.0453 - mae: 2.8205 - val_loss: 2604.6248 - val_mae: 37.8055\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 455ms/step - loss: 14.6505 - mae: 3.2019 - val_loss: 2600.3745 - val_mae: 37.8427\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 386ms/step - loss: 13.5120 - mae: 3.0337 - val_loss: 2578.8496 - val_mae: 37.9762\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 309ms/step - loss: 9.7113 - mae: 2.5829 - val_loss: 2546.8406 - val_mae: 38.1700\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 5.8513 - mae: 2.0865 - val_loss: 2512.0442 - val_mae: 38.3806\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 336ms/step - loss: 4.0352 - mae: 1.7827 - val_loss: 2481.0798 - val_mae: 38.5736\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 341ms/step - loss: 4.6336 - mae: 1.6807 - val_loss: 2458.4282 - val_mae: 38.7209\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 6.5342 - mae: 2.0515 - val_loss: 2446.4075 - val_mae: 38.8054\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 292ms/step - loss: 8.1325 - mae: 2.3017 - val_loss: 2445.5479 - val_mae: 38.8216\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 293ms/step - loss: 8.3560 - mae: 2.3437 - val_loss: 2454.9900 - val_mae: 38.7750\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 7.1571 - mae: 2.1242 - val_loss: 2472.7136 - val_mae: 38.6795\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 5.3277 - mae: 1.8318 - val_loss: 2495.6919 - val_mae: 38.5548\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 3.8977 - mae: 1.5519 - val_loss: 2520.1550 - val_mae: 38.4241\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 292ms/step - loss: 3.5030 - mae: 1.6719 - val_loss: 2542.0671 - val_mae: 38.3100\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 317ms/step - loss: 4.0675 - mae: 1.8276 - val_loss: 2557.8896 - val_mae: 38.2310\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 327ms/step - loss: 4.9502 - mae: 1.9326 - val_loss: 2565.3601 - val_mae: 38.1981\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 316ms/step - loss: 5.4291 - mae: 1.9935 - val_loss: 2564.0032 - val_mae: 38.2130\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 5.1782 - mae: 1.9486 - val_loss: 2555.1311 - val_mae: 38.2686\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 4.3942 - mae: 1.8525 - val_loss: 2541.3489 - val_mae: 38.3512\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 299ms/step - loss: 3.5718 - mae: 1.7258 - val_loss: 2525.7993 - val_mae: 38.4442\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 354ms/step - loss: 3.1376 - mae: 1.5842 - val_loss: 2511.4531 - val_mae: 38.5311\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 437ms/step - loss: 3.1986 - mae: 1.4513 - val_loss: 2500.6060 - val_mae: 38.5987\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 341ms/step - loss: 3.5497 - mae: 1.4774 - val_loss: 2494.6606 - val_mae: 38.6384\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 343ms/step - loss: 3.8607 - mae: 1.5209 - val_loss: 2494.1042 - val_mae: 38.6470\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 297ms/step - loss: 3.8967 - mae: 1.5232 - val_loss: 2498.5847 - val_mae: 38.6263\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 363ms/step - loss: 3.6337 - mae: 1.4866 - val_loss: 2507.0571 - val_mae: 38.5827\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 296ms/step - loss: 3.2320 - mae: 1.4196 - val_loss: 2517.9548 - val_mae: 38.5257\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 306ms/step - loss: 2.9081 - mae: 1.4065 - val_loss: 2529.4165 - val_mae: 38.4660\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 2.7967 - mae: 1.4828 - val_loss: 2539.5908 - val_mae: 38.4137\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 2.8827 - mae: 1.5482 - val_loss: 2546.9517 - val_mae: 38.3773\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 3.0359 - mae: 1.5910 - val_loss: 2550.5752 - val_mae: 38.3614\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 308ms/step - loss: 3.1102 - mae: 1.6043 - val_loss: 2550.2827 - val_mae: 38.3668\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 298ms/step - loss: 3.0368 - mae: 1.5870 - val_loss: 2546.6277 - val_mae: 38.3906\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 299ms/step - loss: 2.8526 - mae: 1.5433 - val_loss: 2540.7056 - val_mae: 38.4269\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 420ms/step - loss: 2.6572 - mae: 1.4818 - val_loss: 2533.8760 - val_mae: 38.4683\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 330ms/step - loss: 2.5397 - mae: 1.4129 - val_loss: 2527.4961 - val_mae: 38.5073\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 303ms/step - loss: 2.5252 - mae: 1.3475 - val_loss: 2522.6733 - val_mae: 38.5377\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 2.5730 - mae: 1.2994 - val_loss: 2520.1223 - val_mae: 38.5553\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 303ms/step - loss: 2.6133 - mae: 1.2920 - val_loss: 2520.1179 - val_mae: 38.5583\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 313ms/step - loss: 2.5955 - mae: 1.2846 - val_loss: 2522.4988 - val_mae: 38.5477\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 306ms/step - loss: 2.5148 - mae: 1.2774 - val_loss: 2526.7458 - val_mae: 38.5265\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 2.4060 - mae: 1.2712 - val_loss: 2532.0713 - val_mae: 38.4991\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 309ms/step - loss: 2.3157 - mae: 1.3003 - val_loss: 2537.5776 - val_mae: 38.4706\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 342ms/step - loss: 2.2721 - mae: 1.3309 - val_loss: 2542.4036 - val_mae: 38.4459\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 332ms/step - loss: 2.2697 - mae: 1.3561 - val_loss: 2545.8665 - val_mae: 38.4286\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 2.2799 - mae: 1.3705 - val_loss: 2547.5837 - val_mae: 38.4210\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 295ms/step - loss: 2.2718 - mae: 1.3712 - val_loss: 2547.5046 - val_mae: 38.4232\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 2.2326 - mae: 1.3579 - val_loss: 2545.9033 - val_mae: 38.4338\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 314ms/step - loss: 2.1713 - mae: 1.3327 - val_loss: 2543.2893 - val_mae: 38.4498\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 299ms/step - loss: 2.1098 - mae: 1.2995 - val_loss: 2540.2944 - val_mae: 38.4680\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 2.0664 - mae: 1.2635 - val_loss: 2537.5430 - val_mae: 38.4847\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 296ms/step - loss: 2.0450 - mae: 1.2293 - val_loss: 2535.5488 - val_mae: 38.4971\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 296ms/step - loss: 2.0359 - mae: 1.2013 - val_loss: 2534.6421 - val_mae: 38.5034\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 375ms/step - loss: 2.0242 - mae: 1.1833 - val_loss: 2534.9341 - val_mae: 38.5029\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 292ms/step - loss: 1.9995 - mae: 1.1761 - val_loss: 2536.3298 - val_mae: 38.4961\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 1.9622 - mae: 1.1719 - val_loss: 2538.5566 - val_mae: 38.4846\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 296ms/step - loss: 1.9206 - mae: 1.1781 - val_loss: 2541.2249 - val_mae: 38.4706\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 306ms/step - loss: 1.8846 - mae: 1.1879 - val_loss: 2543.9077 - val_mae: 38.4566\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 1.8589 - mae: 1.1977 - val_loss: 2546.2109 - val_mae: 38.4447\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 296ms/step - loss: 1.8415 - mae: 1.2045 - val_loss: 2547.8423 - val_mae: 38.4365\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 293ms/step - loss: 1.8256 - mae: 1.2060 - val_loss: 2548.6528 - val_mae: 38.4329\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 305ms/step - loss: 1.8051 - mae: 1.2010 - val_loss: 2548.6511 - val_mae: 38.4338\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 373ms/step - loss: 1.7780 - mae: 1.1895 - val_loss: 2547.9863 - val_mae: 38.4382\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 309ms/step - loss: 1.7482 - mae: 1.1732 - val_loss: 2546.9075 - val_mae: 38.4449\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 295ms/step - loss: 1.7193 - mae: 1.1537 - val_loss: 2545.7009 - val_mae: 38.4522\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 1.6947 - mae: 1.1333 - val_loss: 2544.6521 - val_mae: 38.4586\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 317ms/step - loss: 1.6746 - mae: 1.1142 - val_loss: 2543.9814 - val_mae: 38.4628\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 316ms/step - loss: 1.6564 - mae: 1.0983 - val_loss: 2543.8203 - val_mae: 38.4640\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 306ms/step - loss: 1.6374 - mae: 1.0866 - val_loss: 2544.2012 - val_mae: 38.4620\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 322ms/step - loss: 1.6158 - mae: 1.0793 - val_loss: 2545.0566 - val_mae: 38.4574\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 1s 559ms/step - loss: 1.5920 - mae: 1.0759 - val_loss: 2546.2461 - val_mae: 38.4508\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 389ms/step - loss: 1.5681 - mae: 1.0753 - val_loss: 2547.5774 - val_mae: 38.4434\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 316ms/step - loss: 1.5459 - mae: 1.0758 - val_loss: 2548.8584 - val_mae: 38.4362\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 299ms/step - loss: 1.5260 - mae: 1.0760 - val_loss: 2549.9199 - val_mae: 38.4303\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 310ms/step - loss: 1.5075 - mae: 1.0744 - val_loss: 2550.6472 - val_mae: 38.4262\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 295ms/step - loss: 1.4891 - mae: 1.0702 - val_loss: 2550.9946 - val_mae: 38.4242\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 295ms/step - loss: 1.4697 - mae: 1.0630 - val_loss: 2550.9905 - val_mae: 38.4242\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 297ms/step - loss: 1.4492 - mae: 1.0530 - val_loss: 2550.7170 - val_mae: 38.4257\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 467ms/step - loss: 1.4284 - mae: 1.0410 - val_loss: 2550.3010 - val_mae: 38.4280\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 482ms/step - loss: 1.4083 - mae: 1.0279 - val_loss: 2549.8784 - val_mae: 38.4303\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 340ms/step - loss: 1.3894 - mae: 1.0147 - val_loss: 2549.5730 - val_mae: 38.4320\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 328ms/step - loss: 1.3715 - mae: 1.0026 - val_loss: 2549.4739 - val_mae: 38.4324\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 351ms/step - loss: 1.3539 - mae: 0.9921 - val_loss: 2549.6301 - val_mae: 38.4314\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 324ms/step - loss: 1.3361 - mae: 0.9837 - val_loss: 2550.0352 - val_mae: 38.4290\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 331ms/step - loss: 1.3180 - mae: 0.9775 - val_loss: 2550.6404 - val_mae: 38.4255\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 328ms/step - loss: 1.2998 - mae: 0.9729 - val_loss: 2551.3691 - val_mae: 38.4212\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 400ms/step - loss: 1.2819 - mae: 0.9695 - val_loss: 2552.1274 - val_mae: 38.4167\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "Predicted GDP for Sumy_Oblast in 2021: [98.40125]\n",
      "Actual GDP for Sumy_Oblast in 2021: 100.6875\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - 0s 428ms/step - loss: 315.6549 - mae: 16.3447 - val_loss: 2874.0291 - val_mae: 47.1101\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 299ms/step - loss: 263.0955 - mae: 14.9811 - val_loss: 3009.5654 - val_mae: 46.8730\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 180.6820 - mae: 12.6196 - val_loss: 3207.4597 - val_mae: 46.4672\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 297ms/step - loss: 100.1523 - mae: 9.3688 - val_loss: 3474.6560 - val_mae: 46.0024\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 312ms/step - loss: 50.7018 - mae: 5.6334 - val_loss: 3786.0891 - val_mae: 45.6423\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 293ms/step - loss: 45.9604 - mae: 4.9346 - val_loss: 4074.5349 - val_mae: 45.4166\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 307ms/step - loss: 70.6537 - mae: 6.4272 - val_loss: 4271.5962 - val_mae: 47.1309\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 311ms/step - loss: 95.0807 - mae: 8.0920 - val_loss: 4325.1309 - val_mae: 47.7201\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 377ms/step - loss: 94.8706 - mae: 8.4202 - val_loss: 4236.1343 - val_mae: 46.7058\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 1s 517ms/step - loss: 68.6737 - mae: 7.0387 - val_loss: 4051.8176 - val_mae: 45.4750\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 485ms/step - loss: 35.1278 - mae: 5.0092 - val_loss: 3836.1792 - val_mae: 45.6522\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 370ms/step - loss: 14.9559 - mae: 3.3593 - val_loss: 3642.4766 - val_mae: 45.8213\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 352ms/step - loss: 15.2069 - mae: 3.1702 - val_loss: 3502.2671 - val_mae: 45.9515\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 344ms/step - loss: 28.6237 - mae: 4.2635 - val_loss: 3427.8071 - val_mae: 46.0242\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 460ms/step - loss: 42.0127 - mae: 5.4646 - val_loss: 3419.5027 - val_mae: 46.0345\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 466ms/step - loss: 45.7483 - mae: 5.7537 - val_loss: 3472.0166 - val_mae: 45.9872\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 362ms/step - loss: 38.4056 - mae: 5.0733 - val_loss: 3576.1367 - val_mae: 45.8947\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 417ms/step - loss: 25.6407 - mae: 4.1173 - val_loss: 3717.0181 - val_mae: 45.7743\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 327ms/step - loss: 15.6571 - mae: 3.6234 - val_loss: 3871.9973 - val_mae: 45.6484\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 316ms/step - loss: 13.8010 - mae: 3.0929 - val_loss: 4011.5498 - val_mae: 45.5396\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 320ms/step - loss: 19.4046 - mae: 3.4756 - val_loss: 4105.9907 - val_mae: 45.4671\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 314ms/step - loss: 26.7156 - mae: 4.1369 - val_loss: 4135.9619 - val_mae: 45.5088\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 362ms/step - loss: 29.5794 - mae: 4.4300 - val_loss: 4099.9419 - val_mae: 45.4621\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 465ms/step - loss: 26.0944 - mae: 4.1111 - val_loss: 4013.6111 - val_mae: 45.5196\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 391ms/step - loss: 19.2505 - mae: 3.3925 - val_loss: 3902.1526 - val_mae: 45.5983\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 323ms/step - loss: 13.7542 - mae: 2.9583 - val_loss: 3790.8391 - val_mae: 45.6800\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 389ms/step - loss: 12.3681 - mae: 3.1916 - val_loss: 3698.9968 - val_mae: 45.7487\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 305ms/step - loss: 14.5525 - mae: 3.4758 - val_loss: 3638.0332 - val_mae: 45.7932\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 17.6596 - mae: 3.6498 - val_loss: 3612.2932 - val_mae: 45.8081\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 298ms/step - loss: 19.1119 - mae: 3.6908 - val_loss: 3620.7288 - val_mae: 45.7935\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 305ms/step - loss: 17.9586 - mae: 3.6015 - val_loss: 3658.1753 - val_mae: 45.7541\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 453ms/step - loss: 15.0794 - mae: 3.4038 - val_loss: 3716.0105 - val_mae: 45.6978\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 395ms/step - loss: 12.3062 - mae: 3.1351 - val_loss: 3782.7727 - val_mae: 45.6349\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 371ms/step - loss: 11.1352 - mae: 2.8426 - val_loss: 3845.4663 - val_mae: 45.5763\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 339ms/step - loss: 11.8262 - mae: 2.8411 - val_loss: 3891.8911 - val_mae: 45.5314\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 427ms/step - loss: 13.4100 - mae: 2.9657 - val_loss: 3913.5251 - val_mae: 45.5063\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 410ms/step - loss: 14.5122 - mae: 3.1070 - val_loss: 3907.7080 - val_mae: 45.5025\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 423ms/step - loss: 14.3464 - mae: 3.0907 - val_loss: 3878.0117 - val_mae: 45.5175\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 347ms/step - loss: 13.1420 - mae: 2.9854 - val_loss: 3832.6882 - val_mae: 45.5452\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 353ms/step - loss: 11.7892 - mae: 2.9420 - val_loss: 3781.9736 - val_mae: 45.5780\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 333ms/step - loss: 11.1029 - mae: 2.9424 - val_loss: 3735.5232 - val_mae: 45.6080\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 343ms/step - loss: 11.2990 - mae: 2.9393 - val_loss: 3700.6748 - val_mae: 45.6288\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 333ms/step - loss: 11.9849 - mae: 2.9338 - val_loss: 3681.7346 - val_mae: 45.6365\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 334ms/step - loss: 12.5405 - mae: 2.9906 - val_loss: 3679.9365 - val_mae: 45.6295\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 367ms/step - loss: 12.5556 - mae: 2.9957 - val_loss: 3693.7988 - val_mae: 45.6091\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 12.0434 - mae: 2.9446 - val_loss: 3719.5820 - val_mae: 45.5786\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 323ms/step - loss: 11.3502 - mae: 2.9022 - val_loss: 3751.9224 - val_mae: 45.5429\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 334ms/step - loss: 10.8777 - mae: 2.8920 - val_loss: 3784.6736 - val_mae: 45.5074\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 339ms/step - loss: 10.8185 - mae: 2.8801 - val_loss: 3811.8706 - val_mae: 45.4767\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 375ms/step - loss: 11.0666 - mae: 2.8664 - val_loss: 3828.9641 - val_mae: 45.4544\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 334ms/step - loss: 11.3383 - mae: 2.8508 - val_loss: 3833.6743 - val_mae: 45.4421\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 346ms/step - loss: 11.3893 - mae: 2.8335 - val_loss: 3826.3611 - val_mae: 45.4395\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 332ms/step - loss: 11.1704 - mae: 2.8147 - val_loss: 3809.7100 - val_mae: 45.4445\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 391ms/step - loss: 10.8250 - mae: 2.7952 - val_loss: 3787.8960 - val_mae: 45.4538\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 358ms/step - loss: 10.5559 - mae: 2.7757 - val_loss: 3765.5303 - val_mae: 45.4636\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 321ms/step - loss: 10.4801 - mae: 2.7568 - val_loss: 3746.7180 - val_mae: 45.4707\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 393ms/step - loss: 10.5695 - mae: 2.8268 - val_loss: 3734.4043 - val_mae: 45.4724\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 1s 541ms/step - loss: 10.6976 - mae: 2.8771 - val_loss: 3730.0520 - val_mae: 45.4674\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 426ms/step - loss: 10.7407 - mae: 2.8971 - val_loss: 3733.6042 - val_mae: 45.4557\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 384ms/step - loss: 10.6555 - mae: 2.8869 - val_loss: 3743.6738 - val_mae: 45.4385\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 331ms/step - loss: 10.4919 - mae: 2.8516 - val_loss: 3757.8630 - val_mae: 45.4181\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 342ms/step - loss: 10.3422 - mae: 2.8003 - val_loss: 3773.2324 - val_mae: 45.3967\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 344ms/step - loss: 10.2743 - mae: 2.7436 - val_loss: 3786.8433 - val_mae: 45.3768\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 433ms/step - loss: 10.2910 - mae: 2.6923 - val_loss: 3796.2747 - val_mae: 45.3605\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 345ms/step - loss: 10.3398 - mae: 2.6637 - val_loss: 3800.0786 - val_mae: 45.3488\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 416ms/step - loss: 10.3573 - mae: 2.6595 - val_loss: 3797.9780 - val_mae: 45.3420\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 429ms/step - loss: 10.3127 - mae: 2.6557 - val_loss: 3790.8274 - val_mae: 45.3394\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 356ms/step - loss: 10.2222 - mae: 2.6576 - val_loss: 3780.3298 - val_mae: 45.3396\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 331ms/step - loss: 10.1297 - mae: 2.6879 - val_loss: 3768.6138 - val_mae: 45.3408\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 331ms/step - loss: 10.0722 - mae: 2.7221 - val_loss: 3757.7756 - val_mae: 45.3413\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 314ms/step - loss: 10.0568 - mae: 2.7525 - val_loss: 3749.5078 - val_mae: 45.3396\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 324ms/step - loss: 10.0619 - mae: 2.7732 - val_loss: 3744.8503 - val_mae: 45.3349\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 350ms/step - loss: 10.0572 - mae: 2.7805 - val_loss: 3744.0913 - val_mae: 45.3268\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 373ms/step - loss: 10.0248 - mae: 2.7732 - val_loss: 3746.7952 - val_mae: 45.3158\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 337ms/step - loss: 9.9688 - mae: 2.7531 - val_loss: 3751.9548 - val_mae: 45.3026\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 329ms/step - loss: 9.9084 - mae: 2.7241 - val_loss: 3758.2275 - val_mae: 45.2885\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 323ms/step - loss: 9.8623 - mae: 2.6910 - val_loss: 3764.1758 - val_mae: 45.2746\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 326ms/step - loss: 9.8370 - mae: 2.6594 - val_loss: 3768.5596 - val_mae: 45.2620\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 309ms/step - loss: 9.8242 - mae: 2.6519 - val_loss: 3770.5337 - val_mae: 45.2514\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 322ms/step - loss: 9.8094 - mae: 2.6524 - val_loss: 3769.7883 - val_mae: 45.2431\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 310ms/step - loss: 9.7823 - mae: 2.6523 - val_loss: 3766.5444 - val_mae: 45.2368\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 361ms/step - loss: 9.7432 - mae: 2.6516 - val_loss: 3761.4690 - val_mae: 45.2320\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 354ms/step - loss: 9.7003 - mae: 2.6504 - val_loss: 3755.5117 - val_mae: 45.2280\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 311ms/step - loss: 9.6631 - mae: 2.6487 - val_loss: 3749.6755 - val_mae: 45.2238\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 420ms/step - loss: 9.6357 - mae: 2.6485 - val_loss: 3744.8477 - val_mae: 45.2187\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 419ms/step - loss: 9.6150 - mae: 2.6583 - val_loss: 3741.6387 - val_mae: 45.2121\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 367ms/step - loss: 9.5942 - mae: 2.6626 - val_loss: 3740.3000 - val_mae: 45.2038\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 334ms/step - loss: 9.5680 - mae: 2.6604 - val_loss: 3740.7400 - val_mae: 45.1939\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 334ms/step - loss: 9.5358 - mae: 2.6521 - val_loss: 3742.5439 - val_mae: 45.1828\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 324ms/step - loss: 9.5008 - mae: 2.6390 - val_loss: 3745.1021 - val_mae: 45.1710\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 338ms/step - loss: 9.4679 - mae: 2.6296 - val_loss: 3747.7241 - val_mae: 45.1591\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 391ms/step - loss: 9.4398 - mae: 2.6261 - val_loss: 3749.7720 - val_mae: 45.1477\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 412ms/step - loss: 9.4152 - mae: 2.6223 - val_loss: 3750.7822 - val_mae: 45.1372\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 348ms/step - loss: 9.3911 - mae: 2.6183 - val_loss: 3750.5232 - val_mae: 45.1278\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 315ms/step - loss: 9.3647 - mae: 2.6139 - val_loss: 3749.0354 - val_mae: 45.1194\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 9.3353 - mae: 2.6094 - val_loss: 3746.5659 - val_mae: 45.1120\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 319ms/step - loss: 9.3045 - mae: 2.6046 - val_loss: 3743.5317 - val_mae: 45.1050\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 342ms/step - loss: 9.2745 - mae: 2.5998 - val_loss: 3740.4114 - val_mae: 45.0981\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 9.2466 - mae: 2.5964 - val_loss: 3737.6465 - val_mae: 45.0909\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 9.2205 - mae: 2.6012 - val_loss: 3735.5647 - val_mae: 45.0831\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "Predicted GDP for Ternopil_Oblast in 2021: [104.2571]\n",
      "Actual GDP for Ternopil_Oblast in 2021: 112.3125\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - 0s 400ms/step - loss: 2284.3799 - mae: 47.0988 - val_loss: 3656.1021 - val_mae: 55.3208\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 1029.8446 - mae: 31.3546 - val_loss: 2586.6719 - val_mae: 41.8608\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 293ms/step - loss: 176.6657 - mae: 11.9849 - val_loss: 1958.8134 - val_mae: 32.6490\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 48.4303 - mae: 6.2199 - val_loss: 1681.4633 - val_mae: 35.2759\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 358ms/step - loss: 304.6290 - mae: 16.7572 - val_loss: 1589.0850 - val_mae: 36.9138\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 318ms/step - loss: 624.4258 - mae: 24.5263 - val_loss: 1569.7803 - val_mae: 37.7842\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 392ms/step - loss: 844.5165 - mae: 28.6701 - val_loss: 1573.2657 - val_mae: 38.0925\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 415ms/step - loss: 915.5439 - mae: 29.8870 - val_loss: 1587.4901 - val_mae: 37.9682\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 332ms/step - loss: 843.1154 - mae: 28.6516 - val_loss: 1619.3557 - val_mae: 37.4327\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 344ms/step - loss: 661.7161 - mae: 25.2782 - val_loss: 1689.2255 - val_mae: 36.4558\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 314ms/step - loss: 419.3188 - mae: 19.8840 - val_loss: 1819.6244 - val_mae: 34.9493\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 319ms/step - loss: 186.6732 - mae: 12.6580 - val_loss: 2043.9037 - val_mae: 32.9325\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 391ms/step - loss: 45.4750 - mae: 5.8069 - val_loss: 2367.4297 - val_mae: 37.8641\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 360ms/step - loss: 68.3929 - mae: 6.7881 - val_loss: 2719.2410 - val_mae: 43.5403\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 327ms/step - loss: 224.4103 - mae: 13.5281 - val_loss: 2951.3821 - val_mae: 46.3936\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 337ms/step - loss: 327.4564 - mae: 16.9081 - val_loss: 2988.9897 - val_mae: 46.2168\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 284.0251 - mae: 15.6965 - val_loss: 2882.2842 - val_mae: 43.9731\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 293ms/step - loss: 169.5063 - mae: 11.6965 - val_loss: 2717.8740 - val_mae: 40.8332\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 366ms/step - loss: 73.8578 - mae: 7.3338 - val_loss: 2555.2170 - val_mae: 37.6328\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 320ms/step - loss: 29.1283 - mae: 3.7806 - val_loss: 2423.4907 - val_mae: 34.8782\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 27.4387 - mae: 4.2367 - val_loss: 2333.1753 - val_mae: 35.4534\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 364ms/step - loss: 47.4772 - mae: 6.2080 - val_loss: 2283.5554 - val_mae: 35.9826\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 457ms/step - loss: 70.8467 - mae: 7.6641 - val_loss: 2272.0938 - val_mae: 36.3082\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 418ms/step - loss: 85.0854 - mae: 8.3546 - val_loss: 2295.8064 - val_mae: 36.4755\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 85.4593 - mae: 8.3522 - val_loss: 2351.8813 - val_mae: 36.4961\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 328ms/step - loss: 73.2544 - mae: 7.7437 - val_loss: 2436.4412 - val_mae: 36.3862\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 326ms/step - loss: 53.9280 - mae: 6.6392 - val_loss: 2545.8547 - val_mae: 36.2014\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 349ms/step - loss: 34.9125 - mae: 5.1844 - val_loss: 2674.3267 - val_mae: 37.1381\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 325ms/step - loss: 22.3792 - mae: 3.8407 - val_loss: 2811.5020 - val_mae: 39.1367\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 328ms/step - loss: 19.5167 - mae: 3.0911 - val_loss: 2941.1780 - val_mae: 40.9235\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 340ms/step - loss: 25.2341 - mae: 3.8197 - val_loss: 3045.9893 - val_mae: 42.2982\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 429ms/step - loss: 34.8282 - mae: 4.8757 - val_loss: 3111.8088 - val_mae: 43.1206\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 333ms/step - loss: 42.4348 - mae: 5.5361 - val_loss: 3131.6521 - val_mae: 43.3343\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 333ms/step - loss: 44.0744 - mae: 5.6720 - val_loss: 3107.6365 - val_mae: 42.9774\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 343ms/step - loss: 39.3550 - mae: 5.3158 - val_loss: 3049.6934 - val_mae: 42.1662\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 351ms/step - loss: 31.0211 - mae: 4.5682 - val_loss: 2971.8657 - val_mae: 41.0630\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 326ms/step - loss: 22.9226 - mae: 3.6719 - val_loss: 2888.4355 - val_mae: 39.8459\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 331ms/step - loss: 17.9829 - mae: 2.9506 - val_loss: 2811.1257 - val_mae: 38.6769\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 338ms/step - loss: 17.1240 - mae: 3.0162 - val_loss: 2747.9680 - val_mae: 37.6851\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 321ms/step - loss: 19.3573 - mae: 3.5767 - val_loss: 2703.4971 - val_mae: 36.9590\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 321ms/step - loss: 22.7288 - mae: 3.9860 - val_loss: 2679.5146 - val_mae: 36.6607\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 399ms/step - loss: 25.3189 - mae: 4.2372 - val_loss: 2675.9121 - val_mae: 36.7034\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 330ms/step - loss: 25.9599 - mae: 4.3221 - val_loss: 2691.2080 - val_mae: 36.7035\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 312ms/step - loss: 24.4847 - mae: 4.1511 - val_loss: 2722.8293 - val_mae: 37.1243\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 308ms/step - loss: 21.5713 - mae: 3.8734 - val_loss: 2767.1479 - val_mae: 37.7755\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 320ms/step - loss: 18.3517 - mae: 3.4977 - val_loss: 2819.5178 - val_mae: 38.5347\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 342ms/step - loss: 15.9064 - mae: 3.0604 - val_loss: 2874.4592 - val_mae: 39.3146\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 321ms/step - loss: 14.8552 - mae: 2.7502 - val_loss: 2926.0967 - val_mae: 40.0305\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 326ms/step - loss: 15.1669 - mae: 2.7471 - val_loss: 2968.9082 - val_mae: 40.6092\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 435ms/step - loss: 16.2631 - mae: 3.0123 - val_loss: 2998.5776 - val_mae: 40.9976\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 353ms/step - loss: 17.3347 - mae: 3.2087 - val_loss: 3012.7720 - val_mae: 41.1701\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 321ms/step - loss: 17.7328 - mae: 3.2792 - val_loss: 3011.4819 - val_mae: 41.1287\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 324ms/step - loss: 17.2337 - mae: 3.2264 - val_loss: 2996.8877 - val_mae: 40.9024\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 335ms/step - loss: 16.0513 - mae: 3.0670 - val_loss: 2972.7590 - val_mae: 40.5418\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 320ms/step - loss: 14.6678 - mae: 2.8297 - val_loss: 2943.6023 - val_mae: 40.1069\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 334ms/step - loss: 13.5631 - mae: 2.6160 - val_loss: 2913.8767 - val_mae: 39.6590\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 327ms/step - loss: 13.0058 - mae: 2.5874 - val_loss: 2887.3875 - val_mae: 39.2530\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 347ms/step - loss: 12.9840 - mae: 2.5875 - val_loss: 2866.9895 - val_mae: 38.9322\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 365ms/step - loss: 13.2748 - mae: 2.7522 - val_loss: 2854.4592 - val_mae: 38.7253\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 325ms/step - loss: 13.5789 - mae: 2.8624 - val_loss: 2850.5469 - val_mae: 38.6455\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 341ms/step - loss: 13.6554 - mae: 2.9004 - val_loss: 2855.0767 - val_mae: 38.6910\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 316ms/step - loss: 13.4075 - mae: 2.8669 - val_loss: 2867.0791 - val_mae: 38.8469\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 321ms/step - loss: 12.8919 - mae: 2.7705 - val_loss: 2884.9541 - val_mae: 39.0884\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 334ms/step - loss: 12.2664 - mae: 2.6254 - val_loss: 2906.6116 - val_mae: 39.3833\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 309ms/step - loss: 11.7092 - mae: 2.4618 - val_loss: 2929.6929 - val_mae: 39.6963\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 318ms/step - loss: 11.3418 - mae: 2.4137 - val_loss: 2951.8013 - val_mae: 39.9929\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 356ms/step - loss: 11.1890 - mae: 2.4000 - val_loss: 2970.7927 - val_mae: 40.2435\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 398ms/step - loss: 11.1841 - mae: 2.3853 - val_loss: 2985.0071 - val_mae: 40.4257\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 315ms/step - loss: 11.2119 - mae: 2.3930 - val_loss: 2993.4883 - val_mae: 40.5271\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 316ms/step - loss: 11.1662 - mae: 2.4262 - val_loss: 2996.0601 - val_mae: 40.5459\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 337ms/step - loss: 10.9940 - mae: 2.4117 - val_loss: 2993.3062 - val_mae: 40.4906\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 320ms/step - loss: 10.7080 - mae: 2.3544 - val_loss: 2986.4026 - val_mae: 40.3773\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 311ms/step - loss: 10.3694 - mae: 2.2884 - val_loss: 2976.9021 - val_mae: 40.2272\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 321ms/step - loss: 10.0521 - mae: 2.2654 - val_loss: 2966.4919 - val_mae: 40.0639\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 325ms/step - loss: 9.8091 - mae: 2.2423 - val_loss: 2956.7524 - val_mae: 39.9097\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 392ms/step - loss: 9.6520 - mae: 2.2193 - val_loss: 2949.0024 - val_mae: 39.7836\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 311ms/step - loss: 9.5555 - mae: 2.1968 - val_loss: 2944.1677 - val_mae: 39.6991\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 327ms/step - loss: 9.4750 - mae: 2.1871 - val_loss: 2942.7534 - val_mae: 39.6639\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 322ms/step - loss: 9.3704 - mae: 2.1803 - val_loss: 2944.8352 - val_mae: 39.6790\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 317ms/step - loss: 9.2192 - mae: 2.1618 - val_loss: 2950.0981 - val_mae: 39.7398\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 322ms/step - loss: 9.0240 - mae: 2.1293 - val_loss: 2957.9060 - val_mae: 39.8372\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 332ms/step - loss: 8.8065 - mae: 2.0967 - val_loss: 2967.4004 - val_mae: 39.9585\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 320ms/step - loss: 8.5952 - mae: 2.0786 - val_loss: 2977.6006 - val_mae: 40.0896\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 319ms/step - loss: 8.4109 - mae: 2.0609 - val_loss: 2987.5271 - val_mae: 40.2166\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 388ms/step - loss: 8.2601 - mae: 2.0431 - val_loss: 2996.2969 - val_mae: 40.3272\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 337ms/step - loss: 8.1353 - mae: 2.0248 - val_loss: 3003.2332 - val_mae: 40.4120\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 357ms/step - loss: 8.0197 - mae: 2.0059 - val_loss: 3007.9329 - val_mae: 40.4657\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 318ms/step - loss: 7.8965 - mae: 1.9863 - val_loss: 3010.2910 - val_mae: 40.4868\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 342ms/step - loss: 7.7565 - mae: 1.9661 - val_loss: 3010.4866 - val_mae: 40.4779\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 347ms/step - loss: 7.5994 - mae: 1.9453 - val_loss: 3008.9390 - val_mae: 40.4451\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 333ms/step - loss: 7.4342 - mae: 1.9241 - val_loss: 3006.2344 - val_mae: 40.3963\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 317ms/step - loss: 7.2721 - mae: 1.9025 - val_loss: 3003.0176 - val_mae: 40.3405\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 405ms/step - loss: 7.1212 - mae: 1.8808 - val_loss: 2999.9407 - val_mae: 40.2869\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 318ms/step - loss: 6.9856 - mae: 1.8593 - val_loss: 2997.5493 - val_mae: 40.2431\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 321ms/step - loss: 6.8627 - mae: 1.8382 - val_loss: 2996.2505 - val_mae: 40.2149\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 308ms/step - loss: 6.7440 - mae: 1.8174 - val_loss: 2996.2876 - val_mae: 40.2058\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 324ms/step - loss: 6.6227 - mae: 1.7969 - val_loss: 2997.7056 - val_mae: 40.2163\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 315ms/step - loss: 6.4955 - mae: 1.7767 - val_loss: 3000.3994 - val_mae: 40.2450\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 325ms/step - loss: 6.3629 - mae: 1.7570 - val_loss: 3004.1145 - val_mae: 40.2883\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 315ms/step - loss: 6.2279 - mae: 1.7376 - val_loss: 3008.4988 - val_mae: 40.3412\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "Predicted GDP for Kharkiv_Oblast in 2021: [94.01119]\n",
      "Actual GDP for Kharkiv_Oblast in 2021: 92.125\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - 0s 441ms/step - loss: 412.6071 - mae: 20.1641 - val_loss: 3326.6001 - val_mae: 49.0088\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 318ms/step - loss: 333.9765 - mae: 18.1103 - val_loss: 3551.5271 - val_mae: 48.8853\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 322ms/step - loss: 212.0670 - mae: 14.3569 - val_loss: 3902.2925 - val_mae: 48.7215\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 324ms/step - loss: 91.1797 - mae: 9.2355 - val_loss: 4379.0840 - val_mae: 48.5322\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 311ms/step - loss: 16.1947 - mae: 3.2214 - val_loss: 4943.2476 - val_mae: 51.0516\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 314ms/step - loss: 14.5685 - mae: 3.5661 - val_loss: 5495.9204 - val_mae: 56.3436\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 326ms/step - loss: 75.7149 - mae: 8.3681 - val_loss: 5893.9468 - val_mae: 59.8595\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 351ms/step - loss: 148.3898 - mae: 11.9487 - val_loss: 6023.5044 - val_mae: 60.9661\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 360ms/step - loss: 176.6690 - mae: 13.0802 - val_loss: 5870.5938 - val_mae: 59.6809\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 322ms/step - loss: 144.5193 - mae: 11.7869 - val_loss: 5518.7725 - val_mae: 56.5947\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 328ms/step - loss: 80.7123 - mae: 8.6648 - val_loss: 5089.7759 - val_mae: 52.5747\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 323ms/step - loss: 26.7160 - mae: 4.7667 - val_loss: 4683.4380 - val_mae: 48.4470\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 320ms/step - loss: 5.8666 - mae: 1.8841 - val_loss: 4355.8804 - val_mae: 48.4347\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 329ms/step - loss: 16.5386 - mae: 3.2862 - val_loss: 4125.8682 - val_mae: 48.5095\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 330ms/step - loss: 42.5060 - mae: 6.0621 - val_loss: 3991.5947 - val_mae: 48.5520\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 312ms/step - loss: 66.0737 - mae: 7.7661 - val_loss: 3944.0894 - val_mae: 48.5603\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 75.8643 - mae: 8.3732 - val_loss: 3973.8777 - val_mae: 48.5359\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 322ms/step - loss: 69.0266 - mae: 7.9557 - val_loss: 4072.2200 - val_mae: 48.4835\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 303ms/step - loss: 50.0063 - mae: 6.6561 - val_loss: 4229.0435 - val_mae: 48.4096\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 351ms/step - loss: 27.5457 - mae: 4.6777 - val_loss: 4429.4629 - val_mae: 48.3231\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 323ms/step - loss: 10.8286 - mae: 2.4202 - val_loss: 4650.7817 - val_mae: 48.2345\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 304ms/step - loss: 5.5710 - mae: 1.8064 - val_loss: 4862.5459 - val_mae: 50.4353\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 312ms/step - loss: 11.4675 - mae: 3.1390 - val_loss: 5031.4038 - val_mae: 52.1392\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 350ms/step - loss: 22.5963 - mae: 4.4041 - val_loss: 5130.1348 - val_mae: 53.0985\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 341ms/step - loss: 31.1869 - mae: 5.0995 - val_loss: 5146.2192 - val_mae: 53.2499\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 314ms/step - loss: 32.0579 - mae: 5.1772 - val_loss: 5085.9976 - val_mae: 52.6375\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 325ms/step - loss: 25.5436 - mae: 4.6554 - val_loss: 4970.6567 - val_mae: 51.4568\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 312ms/step - loss: 15.8384 - mae: 3.7178 - val_loss: 4828.3042 - val_mae: 49.9661\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 303ms/step - loss: 8.0862 - mae: 2.5416 - val_loss: 4685.3179 - val_mae: 48.4225\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 314ms/step - loss: 5.2510 - mae: 1.7206 - val_loss: 4561.8672 - val_mae: 48.4612\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 303ms/step - loss: 7.1576 - mae: 1.9194 - val_loss: 4470.4941 - val_mae: 48.5252\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 370ms/step - loss: 11.4307 - mae: 2.5243 - val_loss: 4417.1304 - val_mae: 48.5674\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 1s 565ms/step - loss: 15.1949 - mae: 3.1556 - val_loss: 4402.7739 - val_mae: 48.5862\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 341ms/step - loss: 16.4894 - mae: 3.3566 - val_loss: 4424.9111 - val_mae: 48.5831\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 346ms/step - loss: 14.9057 - mae: 3.1149 - val_loss: 4478.1934 - val_mae: 48.5617\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 311ms/step - loss: 11.4380 - mae: 2.5303 - val_loss: 4554.7124 - val_mae: 48.5272\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 327ms/step - loss: 7.7905 - mae: 2.0299 - val_loss: 4644.2061 - val_mae: 48.4858\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 329ms/step - loss: 5.4988 - mae: 1.7112 - val_loss: 4734.7075 - val_mae: 48.8659\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 330ms/step - loss: 5.2368 - mae: 1.7722 - val_loss: 4813.9194 - val_mae: 49.7044\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 308ms/step - loss: 6.6010 - mae: 2.1850 - val_loss: 4871.2124 - val_mae: 50.3011\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 374ms/step - loss: 8.4510 - mae: 2.6369 - val_loss: 4899.7437 - val_mae: 50.5939\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 348ms/step - loss: 9.5993 - mae: 2.8540 - val_loss: 4897.7979 - val_mae: 50.5708\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 322ms/step - loss: 9.4410 - mae: 2.8270 - val_loss: 4868.8330 - val_mae: 50.2676\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 320ms/step - loss: 8.1735 - mae: 2.5839 - val_loss: 4820.4897 - val_mae: 49.7589\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 317ms/step - loss: 6.5397 - mae: 2.1818 - val_loss: 4762.3130 - val_mae: 49.1396\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 341ms/step - loss: 5.3289 - mae: 1.8192 - val_loss: 4703.7524 - val_mae: 48.5083\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 402ms/step - loss: 4.9616 - mae: 1.6375 - val_loss: 4652.7876 - val_mae: 48.5118\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 313ms/step - loss: 5.3611 - mae: 1.6926 - val_loss: 4615.1768 - val_mae: 48.5326\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 328ms/step - loss: 6.1009 - mae: 1.7994 - val_loss: 4594.1294 - val_mae: 48.5445\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 313ms/step - loss: 6.6868 - mae: 1.8592 - val_loss: 4590.4419 - val_mae: 48.5467\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 343ms/step - loss: 6.8042 - mae: 1.8799 - val_loss: 4602.7808 - val_mae: 48.5395\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 358ms/step - loss: 6.4277 - mae: 1.8323 - val_loss: 4628.2603 - val_mae: 48.5250\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 476ms/step - loss: 5.7772 - mae: 1.7578 - val_loss: 4662.6094 - val_mae: 48.5055\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 369ms/step - loss: 5.1682 - mae: 1.6585 - val_loss: 4700.7271 - val_mae: 48.4841\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 320ms/step - loss: 4.8446 - mae: 1.5918 - val_loss: 4737.3022 - val_mae: 48.8731\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 336ms/step - loss: 4.8721 - mae: 1.6885 - val_loss: 4767.5142 - val_mae: 49.1980\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 293ms/step - loss: 5.1349 - mae: 1.7911 - val_loss: 4787.7368 - val_mae: 49.4144\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 5.4229 - mae: 1.8978 - val_loss: 4796.0518 - val_mae: 49.5036\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 295ms/step - loss: 5.5539 - mae: 1.9388 - val_loss: 4792.4644 - val_mae: 49.4664\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 310ms/step - loss: 5.4605 - mae: 1.9146 - val_loss: 4778.7407 - val_mae: 49.3212\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 5.2037 - mae: 1.8348 - val_loss: 4757.9487 - val_mae: 49.0999\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 292ms/step - loss: 4.9172 - mae: 1.7192 - val_loss: 4733.8037 - val_mae: 48.8415\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 358ms/step - loss: 4.7253 - mae: 1.6487 - val_loss: 4710.0371 - val_mae: 48.5858\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 308ms/step - loss: 4.6821 - mae: 1.5788 - val_loss: 4689.8491 - val_mae: 48.4811\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 307ms/step - loss: 4.7588 - mae: 1.5660 - val_loss: 4675.5806 - val_mae: 48.4879\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 4.8746 - mae: 1.6049 - val_loss: 4668.5181 - val_mae: 48.4907\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 296ms/step - loss: 4.9465 - mae: 1.6234 - val_loss: 4668.8877 - val_mae: 48.4893\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 4.9302 - mae: 1.6208 - val_loss: 4675.9595 - val_mae: 48.4843\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 4.8337 - mae: 1.5991 - val_loss: 4688.2056 - val_mae: 48.4765\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 4.7040 - mae: 1.5629 - val_loss: 4703.5781 - val_mae: 48.5236\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 303ms/step - loss: 4.5962 - mae: 1.5350 - val_loss: 4719.7842 - val_mae: 48.7003\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 285ms/step - loss: 4.5455 - mae: 1.5725 - val_loss: 4734.5928 - val_mae: 48.8612\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 4.5528 - mae: 1.6092 - val_loss: 4746.1553 - val_mae: 48.9868\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 4.5906 - mae: 1.6369 - val_loss: 4753.2202 - val_mae: 49.0638\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 345ms/step - loss: 4.6218 - mae: 1.6522 - val_loss: 4755.2847 - val_mae: 49.0872\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 276ms/step - loss: 4.6198 - mae: 1.6538 - val_loss: 4752.5933 - val_mae: 49.0596\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 4.5803 - mae: 1.6424 - val_loss: 4746.0474 - val_mae: 48.9907\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 281ms/step - loss: 4.5189 - mae: 1.6205 - val_loss: 4736.9902 - val_mae: 48.8949\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 4.4596 - mae: 1.5916 - val_loss: 4726.9731 - val_mae: 48.7886\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 4.4212 - mae: 1.5601 - val_loss: 4717.5107 - val_mae: 48.6880\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 4.4087 - mae: 1.5300 - val_loss: 4709.8721 - val_mae: 48.6070\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 279ms/step - loss: 4.4133 - mae: 1.5049 - val_loss: 4704.9419 - val_mae: 48.5552\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 4.4198 - mae: 1.4948 - val_loss: 4703.1519 - val_mae: 48.5374\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 347ms/step - loss: 4.4151 - mae: 1.4981 - val_loss: 4704.4741 - val_mae: 48.5534\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 293ms/step - loss: 4.3944 - mae: 1.4927 - val_loss: 4708.4619 - val_mae: 48.5983\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 4.3619 - mae: 1.4852 - val_loss: 4714.3628 - val_mae: 48.6639\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 4.3270 - mae: 1.4976 - val_loss: 4721.2334 - val_mae: 48.7398\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 4.2989 - mae: 1.5126 - val_loss: 4728.0801 - val_mae: 48.8154\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 4.2815 - mae: 1.5275 - val_loss: 4734.0171 - val_mae: 48.8811\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 481ms/step - loss: 4.2731 - mae: 1.5398 - val_loss: 4738.3457 - val_mae: 48.9295\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 446ms/step - loss: 4.2676 - mae: 1.5478 - val_loss: 4740.6675 - val_mae: 48.9562\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 418ms/step - loss: 4.2588 - mae: 1.5502 - val_loss: 4740.8926 - val_mae: 48.9604\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 377ms/step - loss: 4.2434 - mae: 1.5470 - val_loss: 4739.2437 - val_mae: 48.9445\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 343ms/step - loss: 4.2223 - mae: 1.5385 - val_loss: 4736.1685 - val_mae: 48.9132\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 326ms/step - loss: 4.1989 - mae: 1.5262 - val_loss: 4732.2783 - val_mae: 48.8732\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 340ms/step - loss: 4.1775 - mae: 1.5117 - val_loss: 4728.2275 - val_mae: 48.8314\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 338ms/step - loss: 4.1604 - mae: 1.4968 - val_loss: 4724.6265 - val_mae: 48.7945\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 336ms/step - loss: 4.1475 - mae: 1.4830 - val_loss: 4721.9619 - val_mae: 48.7676\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 399ms/step - loss: 4.1368 - mae: 1.4719 - val_loss: 4720.5400 - val_mae: 48.7542\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 435ms/step - loss: 4.1253 - mae: 1.4642 - val_loss: 4720.4707 - val_mae: 48.7554\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "Predicted GDP for Kherson_Oblast in 2021: [103.13496]\n",
      "Actual GDP for Kherson_Oblast in 2021: 106.5625\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - 1s 500ms/step - loss: 40.2221 - mae: 5.0402 - val_loss: 4657.6094 - val_mae: 48.2653\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 319ms/step - loss: 33.4511 - mae: 4.3700 - val_loss: 4550.5088 - val_mae: 48.3186\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 327ms/step - loss: 24.2044 - mae: 3.8025 - val_loss: 4423.2485 - val_mae: 48.4025\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 313ms/step - loss: 17.1521 - mae: 3.4198 - val_loss: 4297.6602 - val_mae: 48.4872\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 417ms/step - loss: 14.7986 - mae: 3.2008 - val_loss: 4189.9414 - val_mae: 48.5602\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 492ms/step - loss: 16.7948 - mae: 3.5451 - val_loss: 4110.4917 - val_mae: 48.6155\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 483ms/step - loss: 20.8707 - mae: 3.8063 - val_loss: 4063.7971 - val_mae: 48.6482\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 430ms/step - loss: 24.3776 - mae: 4.1476 - val_loss: 4050.3889 - val_mae: 48.6568\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 386ms/step - loss: 25.5424 - mae: 4.2857 - val_loss: 4068.0017 - val_mae: 48.6425\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 360ms/step - loss: 24.0193 - mae: 4.1072 - val_loss: 4112.1250 - val_mae: 48.6094\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 356ms/step - loss: 20.7387 - mae: 3.7962 - val_loss: 4176.1753 - val_mae: 48.5627\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 367ms/step - loss: 17.2751 - mae: 3.5833 - val_loss: 4251.6265 - val_mae: 48.5094\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 358ms/step - loss: 15.0524 - mae: 3.3383 - val_loss: 4328.4780 - val_mae: 48.4565\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 347ms/step - loss: 14.7160 - mae: 3.1654 - val_loss: 4396.0996 - val_mae: 48.4093\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 363ms/step - loss: 15.9276 - mae: 3.2957 - val_loss: 4445.7124 - val_mae: 48.3747\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 1s 517ms/step - loss: 17.6580 - mae: 3.4416 - val_loss: 4471.3662 - val_mae: 48.3564\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 486ms/step - loss: 18.8000 - mae: 3.5137 - val_loss: 4471.2925 - val_mae: 48.3553\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 368ms/step - loss: 18.7481 - mae: 3.5084 - val_loss: 4448.1006 - val_mae: 48.3696\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 351ms/step - loss: 17.6250 - mae: 3.4336 - val_loss: 4407.8521 - val_mae: 48.3955\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 352ms/step - loss: 16.0817 - mae: 3.3064 - val_loss: 4358.4307 - val_mae: 48.4279\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 322ms/step - loss: 14.8574 - mae: 3.1495 - val_loss: 4307.7197 - val_mae: 48.4610\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 341ms/step - loss: 14.3890 - mae: 3.1471 - val_loss: 4262.6733 - val_mae: 48.4902\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 341ms/step - loss: 14.6621 - mae: 3.2879 - val_loss: 4228.4204 - val_mae: 48.5125\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 426ms/step - loss: 15.3179 - mae: 3.3958 - val_loss: 4207.9204 - val_mae: 48.5257\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 332ms/step - loss: 15.8981 - mae: 3.4604 - val_loss: 4202.1406 - val_mae: 48.5287\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 332ms/step - loss: 16.0783 - mae: 3.4776 - val_loss: 4210.3105 - val_mae: 48.5222\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 327ms/step - loss: 15.7894 - mae: 3.4497 - val_loss: 4230.1978 - val_mae: 48.5076\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 329ms/step - loss: 15.1982 - mae: 3.3842 - val_loss: 4258.4136 - val_mae: 48.4876\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 333ms/step - loss: 14.5867 - mae: 3.2925 - val_loss: 4290.8008 - val_mae: 48.4650\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 349ms/step - loss: 14.2005 - mae: 3.1885 - val_loss: 4322.9028 - val_mae: 48.4428\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 344ms/step - loss: 14.1382 - mae: 3.1024 - val_loss: 4350.4097 - val_mae: 48.4232\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 412ms/step - loss: 14.3276 - mae: 3.0981 - val_loss: 4370.1348 - val_mae: 48.4090\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 320ms/step - loss: 14.5888 - mae: 3.1159 - val_loss: 4380.1128 - val_mae: 48.4014\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 323ms/step - loss: 14.7413 - mae: 3.1403 - val_loss: 4379.9307 - val_mae: 48.4008\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 319ms/step - loss: 14.6932 - mae: 3.1334 - val_loss: 4370.6987 - val_mae: 48.4064\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 319ms/step - loss: 14.4714 - mae: 3.0987 - val_loss: 4354.7222 - val_mae: 48.4166\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 327ms/step - loss: 14.1855 - mae: 3.0772 - val_loss: 4334.9946 - val_mae: 48.4295\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 318ms/step - loss: 13.9565 - mae: 3.0731 - val_loss: 4314.6763 - val_mae: 48.4429\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 308ms/step - loss: 13.8556 - mae: 3.1008 - val_loss: 4296.5425 - val_mae: 48.4545\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 369ms/step - loss: 13.8790 - mae: 3.1563 - val_loss: 4282.8447 - val_mae: 48.4630\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 344ms/step - loss: 13.9643 - mae: 3.1981 - val_loss: 4274.9668 - val_mae: 48.4677\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 325ms/step - loss: 14.0328 - mae: 3.2217 - val_loss: 4273.3931 - val_mae: 48.4681\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 314ms/step - loss: 14.0302 - mae: 3.2253 - val_loss: 4277.7783 - val_mae: 48.4647\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 312ms/step - loss: 13.9470 - mae: 3.2101 - val_loss: 4287.0649 - val_mae: 48.4580\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 13.8155 - mae: 3.1793 - val_loss: 4299.6460 - val_mae: 48.4489\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 309ms/step - loss: 13.6857 - mae: 3.1382 - val_loss: 4313.6377 - val_mae: 48.4387\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 310ms/step - loss: 13.5983 - mae: 3.0927 - val_loss: 4327.1782 - val_mae: 48.4288\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 359ms/step - loss: 13.5668 - mae: 3.0488 - val_loss: 4338.5806 - val_mae: 48.4205\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 338ms/step - loss: 13.5748 - mae: 3.0253 - val_loss: 4346.5879 - val_mae: 48.4145\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 13.5903 - mae: 3.0208 - val_loss: 4350.5132 - val_mae: 48.4113\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 306ms/step - loss: 13.5839 - mae: 3.0163 - val_loss: 4350.2998 - val_mae: 48.4109\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 308ms/step - loss: 13.5426 - mae: 3.0117 - val_loss: 4346.4824 - val_mae: 48.4130\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 296ms/step - loss: 13.4737 - mae: 3.0073 - val_loss: 4340.0288 - val_mae: 48.4168\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 303ms/step - loss: 13.3969 - mae: 3.0028 - val_loss: 4332.1865 - val_mae: 48.4220\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 397ms/step - loss: 13.3323 - mae: 3.0230 - val_loss: 4324.2368 - val_mae: 48.4273\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 314ms/step - loss: 13.2904 - mae: 3.0465 - val_loss: 4317.3472 - val_mae: 48.4318\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 309ms/step - loss: 13.2688 - mae: 3.0667 - val_loss: 4312.4175 - val_mae: 48.4351\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 320ms/step - loss: 13.2556 - mae: 3.0807 - val_loss: 4309.9868 - val_mae: 48.4365\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 343ms/step - loss: 13.2373 - mae: 3.0869 - val_loss: 4310.2168 - val_mae: 48.4361\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 13.2055 - mae: 3.0847 - val_loss: 4312.9058 - val_mae: 48.4340\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 310ms/step - loss: 13.1602 - mae: 3.0747 - val_loss: 4317.5479 - val_mae: 48.4305\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 311ms/step - loss: 13.1078 - mae: 3.0586 - val_loss: 4323.4316 - val_mae: 48.4262\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 316ms/step - loss: 13.0572 - mae: 3.0385 - val_loss: 4329.7378 - val_mae: 48.4215\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 306ms/step - loss: 13.0150 - mae: 3.0171 - val_loss: 4335.6768 - val_mae: 48.4172\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 307ms/step - loss: 12.9820 - mae: 2.9970 - val_loss: 4340.5728 - val_mae: 48.4135\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 12.9545 - mae: 2.9800 - val_loss: 4343.9575 - val_mae: 48.4110\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 317ms/step - loss: 12.9269 - mae: 2.9679 - val_loss: 4345.6118 - val_mae: 48.4097\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 12.8947 - mae: 2.9612 - val_loss: 4345.5771 - val_mae: 48.4096\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 310ms/step - loss: 12.8565 - mae: 2.9597 - val_loss: 4344.1270 - val_mae: 48.4104\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 297ms/step - loss: 12.8143 - mae: 2.9627 - val_loss: 4341.7041 - val_mae: 48.4120\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 320ms/step - loss: 12.7719 - mae: 2.9688 - val_loss: 4338.8311 - val_mae: 48.4139\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 299ms/step - loss: 12.7324 - mae: 2.9763 - val_loss: 4336.0474 - val_mae: 48.4158\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 308ms/step - loss: 12.6968 - mae: 2.9836 - val_loss: 4333.8262 - val_mae: 48.4173\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 392ms/step - loss: 12.6642 - mae: 2.9891 - val_loss: 4332.5112 - val_mae: 48.4181\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 316ms/step - loss: 12.6325 - mae: 2.9918 - val_loss: 4332.2930 - val_mae: 48.4182\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 314ms/step - loss: 12.5989 - mae: 2.9909 - val_loss: 4333.1973 - val_mae: 48.4176\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 12.5627 - mae: 2.9866 - val_loss: 4335.0991 - val_mae: 48.4162\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 377ms/step - loss: 12.5242 - mae: 2.9792 - val_loss: 4337.7461 - val_mae: 48.4144\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 12.4849 - mae: 2.9694 - val_loss: 4340.8149 - val_mae: 48.4122\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 12.4464 - mae: 2.9582 - val_loss: 4343.9561 - val_mae: 48.4100\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 318ms/step - loss: 12.4092 - mae: 2.9469 - val_loss: 4346.8472 - val_mae: 48.4081\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 12.3732 - mae: 2.9363 - val_loss: 4349.2217 - val_mae: 48.4065\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 12.3378 - mae: 2.9274 - val_loss: 4350.9287 - val_mae: 48.4055\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 12.3019 - mae: 2.9206 - val_loss: 4351.9111 - val_mae: 48.4050\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 309ms/step - loss: 12.2648 - mae: 2.9160 - val_loss: 4352.2236 - val_mae: 48.4051\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 12.2264 - mae: 2.9135 - val_loss: 4352.0156 - val_mae: 48.4056\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 298ms/step - loss: 12.1872 - mae: 2.9127 - val_loss: 4351.4868 - val_mae: 48.4064\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 331ms/step - loss: 12.1481 - mae: 2.9129 - val_loss: 4350.8677 - val_mae: 48.4073\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 306ms/step - loss: 12.1092 - mae: 2.9133 - val_loss: 4350.3813 - val_mae: 48.4081\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 336ms/step - loss: 12.0707 - mae: 2.9134 - val_loss: 4350.2119 - val_mae: 48.4087\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 349ms/step - loss: 12.0322 - mae: 2.9124 - val_loss: 4350.4805 - val_mae: 48.4090\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 306ms/step - loss: 11.9933 - mae: 2.9100 - val_loss: 4351.2407 - val_mae: 48.4090\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 303ms/step - loss: 11.9538 - mae: 2.9062 - val_loss: 4352.4751 - val_mae: 48.4087\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 310ms/step - loss: 11.9140 - mae: 2.9008 - val_loss: 4354.0986 - val_mae: 48.4082\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 314ms/step - loss: 11.8737 - mae: 2.8943 - val_loss: 4355.9927 - val_mae: 48.4076\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 11.8334 - mae: 2.8870 - val_loss: 4358.0107 - val_mae: 48.4069\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 11.7930 - mae: 2.8793 - val_loss: 4360.0088 - val_mae: 48.4063\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 329ms/step - loss: 11.7526 - mae: 2.8717 - val_loss: 4361.8555 - val_mae: 48.4059\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 11.7120 - mae: 2.8645 - val_loss: 4363.4604 - val_mae: 48.4058\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 11.6710 - mae: 2.8581 - val_loss: 4364.7803 - val_mae: 48.4060\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "Predicted GDP for Khmelnytskyi_Oblast in 2021: [102.16598]\n",
      "Actual GDP for Khmelnytskyi_Oblast in 2021: 110.25\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - 0s 434ms/step - loss: 111.5221 - mae: 8.5641 - val_loss: 4263.4277 - val_mae: 46.7755\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 319ms/step - loss: 62.9827 - mae: 6.4270 - val_loss: 4119.8394 - val_mae: 47.0267\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 311ms/step - loss: 22.1490 - mae: 4.0482 - val_loss: 3984.3174 - val_mae: 47.3097\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 310ms/step - loss: 19.9116 - mae: 3.8031 - val_loss: 3894.6917 - val_mae: 47.5275\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 303ms/step - loss: 43.4027 - mae: 5.8943 - val_loss: 3866.9500 - val_mae: 47.6069\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 57.4751 - mae: 7.0966 - val_loss: 3902.1421 - val_mae: 47.5560\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 402ms/step - loss: 47.2186 - mae: 6.3902 - val_loss: 3991.5100 - val_mae: 47.4155\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 306ms/step - loss: 24.8402 - mae: 4.4122 - val_loss: 4117.5273 - val_mae: 47.2365\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 308ms/step - loss: 9.5746 - mae: 2.6087 - val_loss: 4253.8760 - val_mae: 47.0679\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 320ms/step - loss: 9.5251 - mae: 2.5800 - val_loss: 4370.8457 - val_mae: 46.9561\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 331ms/step - loss: 18.9958 - mae: 3.5781 - val_loss: 4444.3198 - val_mae: 47.3754\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 27.1396 - mae: 4.4053 - val_loss: 4461.9858 - val_mae: 47.5541\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 26.2620 - mae: 4.3449 - val_loss: 4428.6831 - val_mae: 47.1432\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 317ms/step - loss: 18.0684 - mae: 3.5503 - val_loss: 4362.4058 - val_mae: 47.0662\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 373ms/step - loss: 9.3050 - mae: 2.3279 - val_loss: 4286.2583 - val_mae: 47.1886\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 308ms/step - loss: 5.6169 - mae: 1.8296 - val_loss: 4221.0068 - val_mae: 47.3010\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 344ms/step - loss: 7.8276 - mae: 2.4085 - val_loss: 4180.9546 - val_mae: 47.3835\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 303ms/step - loss: 12.3654 - mae: 3.2030 - val_loss: 4173.2422 - val_mae: 47.4258\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 304ms/step - loss: 14.8208 - mae: 3.5164 - val_loss: 4198.6582 - val_mae: 47.4269\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 297ms/step - loss: 13.2564 - mae: 3.3348 - val_loss: 4252.5977 - val_mae: 47.3934\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 350ms/step - loss: 9.0164 - mae: 2.7395 - val_loss: 4325.8481 - val_mae: 47.3382\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 337ms/step - loss: 5.1298 - mae: 1.8809 - val_loss: 4405.6533 - val_mae: 47.2770\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 3.8781 - mae: 1.3922 - val_loss: 4477.7031 - val_mae: 47.4078\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 310ms/step - loss: 5.3025 - mae: 1.7259 - val_loss: 4529.2012 - val_mae: 47.9779\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 316ms/step - loss: 7.4978 - mae: 2.2326 - val_loss: 4552.3193 - val_mae: 48.2211\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 312ms/step - loss: 8.3621 - mae: 2.3990 - val_loss: 4546.2573 - val_mae: 48.1340\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 314ms/step - loss: 7.1800 - mae: 2.1851 - val_loss: 4517.0723 - val_mae: 47.7828\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 297ms/step - loss: 4.9511 - mae: 1.6797 - val_loss: 4475.2871 - val_mae: 47.3226\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 303ms/step - loss: 3.3089 - mae: 1.3302 - val_loss: 4432.5229 - val_mae: 47.3811\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 326ms/step - loss: 3.1263 - mae: 1.4538 - val_loss: 4398.6230 - val_mae: 47.4289\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 322ms/step - loss: 4.0221 - mae: 1.8372 - val_loss: 4380.0249 - val_mae: 47.4547\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 4.8663 - mae: 2.0375 - val_loss: 4379.1201 - val_mae: 47.4551\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 308ms/step - loss: 4.7739 - mae: 2.0164 - val_loss: 4394.3809 - val_mae: 47.4321\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 312ms/step - loss: 3.8364 - mae: 1.8028 - val_loss: 4420.9048 - val_mae: 47.3920\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 2.7742 - mae: 1.4247 - val_loss: 4451.2144 - val_mae: 47.3445\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 308ms/step - loss: 2.4357 - mae: 1.2500 - val_loss: 4476.9292 - val_mae: 47.3212\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 308ms/step - loss: 2.9759 - mae: 1.4115 - val_loss: 4491.2920 - val_mae: 47.4995\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 299ms/step - loss: 3.5587 - mae: 1.4911 - val_loss: 4491.4253 - val_mae: 47.4988\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 317ms/step - loss: 3.4411 - mae: 1.4711 - val_loss: 4479.0391 - val_mae: 47.3418\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 314ms/step - loss: 2.7598 - mae: 1.3666 - val_loss: 4459.3174 - val_mae: 47.3454\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 306ms/step - loss: 2.1779 - mae: 1.2148 - val_loss: 4438.8442 - val_mae: 47.3839\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 335ms/step - loss: 2.1071 - mae: 1.2415 - val_loss: 4423.4463 - val_mae: 47.4109\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 350ms/step - loss: 2.3953 - mae: 1.3528 - val_loss: 4416.8271 - val_mae: 47.4228\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 306ms/step - loss: 2.5945 - mae: 1.4495 - val_loss: 4420.0254 - val_mae: 47.4238\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 299ms/step - loss: 2.4613 - mae: 1.4149 - val_loss: 4431.5986 - val_mae: 47.4144\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 326ms/step - loss: 2.0990 - mae: 1.2712 - val_loss: 4448.2759 - val_mae: 47.3984\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 1.7684 - mae: 1.0923 - val_loss: 4465.7793 - val_mae: 47.3800\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 307ms/step - loss: 1.6673 - mae: 1.0450 - val_loss: 4479.7808 - val_mae: 47.3652\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 402ms/step - loss: 1.7746 - mae: 1.1041 - val_loss: 4487.0737 - val_mae: 47.3734\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 1.8806 - mae: 1.1247 - val_loss: 4486.3623 - val_mae: 47.3624\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 1.8147 - mae: 1.1002 - val_loss: 4478.5591 - val_mae: 47.3742\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 1.5980 - mae: 1.0341 - val_loss: 4466.2324 - val_mae: 47.3924\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 316ms/step - loss: 1.3763 - mae: 0.9359 - val_loss: 4452.7344 - val_mae: 47.4121\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 308ms/step - loss: 1.2855 - mae: 0.8788 - val_loss: 4441.2852 - val_mae: 47.4289\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 296ms/step - loss: 1.3376 - mae: 0.9957 - val_loss: 4434.0049 - val_mae: 47.4398\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 323ms/step - loss: 1.3748 - mae: 1.0487 - val_loss: 4431.6724 - val_mae: 47.4438\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 306ms/step - loss: 1.3264 - mae: 1.0326 - val_loss: 4433.7979 - val_mae: 47.4417\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 305ms/step - loss: 1.2012 - mae: 0.9539 - val_loss: 4438.8057 - val_mae: 47.4357\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 408ms/step - loss: 1.0729 - mae: 0.8333 - val_loss: 4444.5049 - val_mae: 47.4283\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 314ms/step - loss: 1.0071 - mae: 0.7562 - val_loss: 4448.6870 - val_mae: 47.4219\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 310ms/step - loss: 1.0052 - mae: 0.7787 - val_loss: 4449.7002 - val_mae: 47.4155\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 326ms/step - loss: 1.0130 - mae: 0.7907 - val_loss: 4446.8433 - val_mae: 47.4151\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 0.9794 - mae: 0.7788 - val_loss: 4440.3921 - val_mae: 47.4206\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 306ms/step - loss: 0.8994 - mae: 0.7447 - val_loss: 4431.4951 - val_mae: 47.4304\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 329ms/step - loss: 0.8089 - mae: 0.6946 - val_loss: 4421.8076 - val_mae: 47.4419\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 312ms/step - loss: 0.7474 - mae: 0.6781 - val_loss: 4413.0371 - val_mae: 47.4523\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 414ms/step - loss: 0.7223 - mae: 0.6816 - val_loss: 4406.4873 - val_mae: 47.4594\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 362ms/step - loss: 0.7064 - mae: 0.6873 - val_loss: 4402.7900 - val_mae: 47.4608\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 314ms/step - loss: 0.6718 - mae: 0.6749 - val_loss: 4401.8203 - val_mae: 47.4589\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 314ms/step - loss: 0.6136 - mae: 0.6262 - val_loss: 4402.8267 - val_mae: 47.4544\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 305ms/step - loss: 0.5502 - mae: 0.5685 - val_loss: 4404.6108 - val_mae: 47.4486\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 319ms/step - loss: 0.5049 - mae: 0.5445 - val_loss: 4405.9146 - val_mae: 47.4430\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 308ms/step - loss: 0.4812 - mae: 0.5537 - val_loss: 4405.7329 - val_mae: 47.4403\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 310ms/step - loss: 0.4665 - mae: 0.5534 - val_loss: 4403.5581 - val_mae: 47.4416\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 371ms/step - loss: 0.4401 - mae: 0.5392 - val_loss: 4399.5024 - val_mae: 47.4449\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 358ms/step - loss: 0.3979 - mae: 0.5107 - val_loss: 4394.2256 - val_mae: 47.4501\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 357ms/step - loss: 0.3547 - mae: 0.4723 - val_loss: 4388.7051 - val_mae: 47.4558\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 312ms/step - loss: 0.3255 - mae: 0.4313 - val_loss: 4383.9229 - val_mae: 47.4599\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 326ms/step - loss: 0.3107 - mae: 0.4064 - val_loss: 4380.6021 - val_mae: 47.4621\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 333ms/step - loss: 0.2954 - mae: 0.4033 - val_loss: 4378.9746 - val_mae: 47.4621\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 316ms/step - loss: 0.2708 - mae: 0.3827 - val_loss: 4378.7969 - val_mae: 47.4602\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 402ms/step - loss: 0.2406 - mae: 0.3540 - val_loss: 4379.4478 - val_mae: 47.4571\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 1s 500ms/step - loss: 0.2151 - mae: 0.3470 - val_loss: 4380.1655 - val_mae: 47.4537\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 396ms/step - loss: 0.1994 - mae: 0.3451 - val_loss: 4380.2798 - val_mae: 47.4515\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 378ms/step - loss: 0.1881 - mae: 0.3373 - val_loss: 4379.4106 - val_mae: 47.4511\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 337ms/step - loss: 0.1732 - mae: 0.3208 - val_loss: 4377.5610 - val_mae: 47.4526\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 337ms/step - loss: 0.1531 - mae: 0.2958 - val_loss: 4375.0869 - val_mae: 47.4553\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 347ms/step - loss: 0.1336 - mae: 0.2653 - val_loss: 4372.5303 - val_mae: 47.4578\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 328ms/step - loss: 0.1199 - mae: 0.2390 - val_loss: 4370.4521 - val_mae: 47.4596\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 330ms/step - loss: 0.1113 - mae: 0.2269 - val_loss: 4369.2036 - val_mae: 47.4602\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 418ms/step - loss: 0.1027 - mae: 0.2242 - val_loss: 4368.8584 - val_mae: 47.4595\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 316ms/step - loss: 0.0910 - mae: 0.2073 - val_loss: 4369.1973 - val_mae: 47.4577\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 321ms/step - loss: 0.0787 - mae: 0.1796 - val_loss: 4369.7754 - val_mae: 47.4555\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 308ms/step - loss: 0.0695 - mae: 0.1733 - val_loss: 4370.1260 - val_mae: 47.4537\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 350ms/step - loss: 0.0639 - mae: 0.1784 - val_loss: 4369.9023 - val_mae: 47.4528\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 330ms/step - loss: 0.0591 - mae: 0.1770 - val_loss: 4368.9927 - val_mae: 47.4530\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 321ms/step - loss: 0.0526 - mae: 0.1676 - val_loss: 4367.5293 - val_mae: 47.4542\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 332ms/step - loss: 0.0455 - mae: 0.1518 - val_loss: 4365.8359 - val_mae: 47.4558\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 309ms/step - loss: 0.0403 - mae: 0.1349 - val_loss: 4364.2896 - val_mae: 47.4571\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 316ms/step - loss: 0.0375 - mae: 0.1333 - val_loss: 4363.1680 - val_mae: 47.4576\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "Predicted GDP for Cherkasy_Oblast in 2021: [100.42921]\n",
      "Actual GDP for Cherkasy_Oblast in 2021: 105.5625\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - 0s 485ms/step - loss: 18.7844 - mae: 3.3895 - val_loss: 4398.6113 - val_mae: 49.2449\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 17.6236 - mae: 3.2702 - val_loss: 4441.4443 - val_mae: 49.2212\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 320ms/step - loss: 15.9816 - mae: 3.1338 - val_loss: 4496.1958 - val_mae: 49.1917\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 306ms/step - loss: 14.7223 - mae: 2.9844 - val_loss: 4554.0562 - val_mae: 49.1611\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 309ms/step - loss: 14.3666 - mae: 2.9833 - val_loss: 4605.9614 - val_mae: 49.1340\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 309ms/step - loss: 14.8599 - mae: 2.9818 - val_loss: 4644.0537 - val_mae: 49.1140\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 304ms/step - loss: 15.6877 - mae: 3.1075 - val_loss: 4663.1777 - val_mae: 49.1035\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 307ms/step - loss: 16.2395 - mae: 3.2178 - val_loss: 4661.8472 - val_mae: 49.1030\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 304ms/step - loss: 16.1784 - mae: 3.2079 - val_loss: 4642.3320 - val_mae: 49.1113\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 427ms/step - loss: 15.5877 - mae: 3.0914 - val_loss: 4609.8193 - val_mae: 49.1259\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 1s 519ms/step - loss: 14.8424 - mae: 2.9723 - val_loss: 4571.0273 - val_mae: 49.1437\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 402ms/step - loss: 14.3352 - mae: 2.9701 - val_loss: 4532.7505 - val_mae: 49.1614\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 361ms/step - loss: 14.2493 - mae: 2.9679 - val_loss: 4500.7310 - val_mae: 49.1761\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 342ms/step - loss: 14.4999 - mae: 2.9656 - val_loss: 4478.9575 - val_mae: 49.1858\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 345ms/step - loss: 14.8380 - mae: 2.9946 - val_loss: 4469.4502 - val_mae: 49.1893\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 345ms/step - loss: 15.0199 - mae: 3.0219 - val_loss: 4472.3052 - val_mae: 49.1866\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 376ms/step - loss: 14.9372 - mae: 3.0109 - val_loss: 4485.9214 - val_mae: 49.1784\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 434ms/step - loss: 14.6486 - mae: 2.9665 - val_loss: 4507.3574 - val_mae: 49.1663\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 358ms/step - loss: 14.3163 - mae: 2.9527 - val_loss: 4532.7896 - val_mae: 49.1523\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 367ms/step - loss: 14.0990 - mae: 2.9499 - val_loss: 4558.0537 - val_mae: 49.1384\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 416ms/step - loss: 14.0653 - mae: 2.9470 - val_loss: 4579.3037 - val_mae: 49.1266\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 392ms/step - loss: 14.1711 - mae: 2.9442 - val_loss: 4593.5786 - val_mae: 49.1182\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 362ms/step - loss: 14.3036 - mae: 2.9413 - val_loss: 4599.2729 - val_mae: 49.1140\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 316ms/step - loss: 14.3563 - mae: 2.9385 - val_loss: 4596.3267 - val_mae: 49.1140\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 330ms/step - loss: 14.2889 - mae: 2.9357 - val_loss: 4586.0938 - val_mae: 49.1175\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 330ms/step - loss: 14.1378 - mae: 2.9330 - val_loss: 4570.9702 - val_mae: 49.1234\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 315ms/step - loss: 13.9807 - mae: 2.9302 - val_loss: 4553.8594 - val_mae: 49.1303\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 311ms/step - loss: 13.8844 - mae: 2.9274 - val_loss: 4537.6299 - val_mae: 49.1368\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 409ms/step - loss: 13.8693 - mae: 2.9246 - val_loss: 4524.6768 - val_mae: 49.1416\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 315ms/step - loss: 13.9071 - mae: 2.9217 - val_loss: 4516.6187 - val_mae: 49.1441\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 319ms/step - loss: 13.9462 - mae: 2.9188 - val_loss: 4514.1685 - val_mae: 49.1436\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 322ms/step - loss: 13.9447 - mae: 2.9158 - val_loss: 4517.1470 - val_mae: 49.1405\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 329ms/step - loss: 13.8917 - mae: 2.9128 - val_loss: 4524.5923 - val_mae: 49.1350\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 336ms/step - loss: 13.8073 - mae: 2.9097 - val_loss: 4534.9717 - val_mae: 49.1281\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 332ms/step - loss: 13.7259 - mae: 2.9066 - val_loss: 4546.4399 - val_mae: 49.1206\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 327ms/step - loss: 13.6738 - mae: 2.9034 - val_loss: 4557.1519 - val_mae: 49.1134\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 322ms/step - loss: 13.6566 - mae: 2.9002 - val_loss: 4565.5229 - val_mae: 49.1074\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 419ms/step - loss: 13.6594 - mae: 2.8970 - val_loss: 4570.4824 - val_mae: 49.1030\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 323ms/step - loss: 13.6587 - mae: 2.8938 - val_loss: 4571.5864 - val_mae: 49.1005\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 319ms/step - loss: 13.6378 - mae: 2.8905 - val_loss: 4569.0518 - val_mae: 49.0998\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 357ms/step - loss: 13.5944 - mae: 2.8873 - val_loss: 4563.6440 - val_mae: 49.1004\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 312ms/step - loss: 13.5396 - mae: 2.8840 - val_loss: 4556.5103 - val_mae: 49.1018\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 313ms/step - loss: 13.4889 - mae: 2.8807 - val_loss: 4548.9385 - val_mae: 49.1034\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 405ms/step - loss: 13.4524 - mae: 2.8774 - val_loss: 4542.1504 - val_mae: 49.1045\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 307ms/step - loss: 13.4304 - mae: 2.8741 - val_loss: 4537.1069 - val_mae: 49.1047\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 322ms/step - loss: 13.4148 - mae: 2.8707 - val_loss: 4534.4019 - val_mae: 49.1037\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 323ms/step - loss: 13.3955 - mae: 2.8673 - val_loss: 4534.2148 - val_mae: 49.1013\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 310ms/step - loss: 13.3665 - mae: 2.8639 - val_loss: 4536.3164 - val_mae: 49.0976\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 310ms/step - loss: 13.3280 - mae: 2.8603 - val_loss: 4540.1543 - val_mae: 49.0930\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 313ms/step - loss: 13.2858 - mae: 2.8568 - val_loss: 4544.9629 - val_mae: 49.0878\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 332ms/step - loss: 13.2463 - mae: 2.8531 - val_loss: 4549.8940 - val_mae: 49.0825\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 356ms/step - loss: 13.2132 - mae: 2.8494 - val_loss: 4554.1523 - val_mae: 49.0774\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 393ms/step - loss: 13.1859 - mae: 2.8457 - val_loss: 4557.1328 - val_mae: 49.0729\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 455ms/step - loss: 13.1605 - mae: 2.8420 - val_loss: 4558.4683 - val_mae: 49.0692\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 415ms/step - loss: 13.1324 - mae: 2.8383 - val_loss: 4558.0977 - val_mae: 49.0662\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 371ms/step - loss: 13.0993 - mae: 2.8345 - val_loss: 4556.2280 - val_mae: 49.0639\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 317ms/step - loss: 13.0621 - mae: 2.8307 - val_loss: 4553.2847 - val_mae: 49.0621\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 316ms/step - loss: 13.0235 - mae: 2.8268 - val_loss: 4549.8110 - val_mae: 49.0604\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 388ms/step - loss: 12.9865 - mae: 2.8229 - val_loss: 4546.3813 - val_mae: 49.0587\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 343ms/step - loss: 12.9523 - mae: 2.8190 - val_loss: 4543.4995 - val_mae: 49.0567\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 319ms/step - loss: 12.9199 - mae: 2.8151 - val_loss: 4541.5337 - val_mae: 49.0540\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 329ms/step - loss: 12.8876 - mae: 2.8110 - val_loss: 4540.6655 - val_mae: 49.0507\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 328ms/step - loss: 12.8543 - mae: 2.8070 - val_loss: 4540.8931 - val_mae: 49.0468\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 310ms/step - loss: 12.8187 - mae: 2.8028 - val_loss: 4542.0356 - val_mae: 49.0423\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 306ms/step - loss: 12.7813 - mae: 2.7986 - val_loss: 4543.7866 - val_mae: 49.0374\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 340ms/step - loss: 12.7445 - mae: 2.7944 - val_loss: 4545.7744 - val_mae: 49.0324\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 346ms/step - loss: 12.7092 - mae: 2.7902 - val_loss: 4547.6235 - val_mae: 49.0273\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 352ms/step - loss: 12.6749 - mae: 2.7859 - val_loss: 4549.0225 - val_mae: 49.0224\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 360ms/step - loss: 12.6412 - mae: 2.7817 - val_loss: 4549.7515 - val_mae: 49.0179\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 320ms/step - loss: 12.6070 - mae: 2.7773 - val_loss: 4549.7192 - val_mae: 49.0137\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 317ms/step - loss: 12.5718 - mae: 2.7730 - val_loss: 4548.9702 - val_mae: 49.0098\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 326ms/step - loss: 12.5354 - mae: 2.7686 - val_loss: 4547.6543 - val_mae: 49.0062\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 303ms/step - loss: 12.4983 - mae: 2.7641 - val_loss: 4545.9956 - val_mae: 49.0027\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 12.4606 - mae: 2.7594 - val_loss: 4544.2524 - val_mae: 48.9993\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 399ms/step - loss: 12.4230 - mae: 2.7545 - val_loss: 4542.6748 - val_mae: 48.9956\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 298ms/step - loss: 12.3856 - mae: 2.7495 - val_loss: 4541.4570 - val_mae: 48.9917\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 303ms/step - loss: 12.3481 - mae: 2.7445 - val_loss: 4540.7197 - val_mae: 48.9875\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 296ms/step - loss: 12.3100 - mae: 2.7395 - val_loss: 4540.4937 - val_mae: 48.9829\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 319ms/step - loss: 12.2732 - mae: 2.7344 - val_loss: 4540.7246 - val_mae: 48.9780\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 296ms/step - loss: 12.2363 - mae: 2.7294 - val_loss: 4541.2944 - val_mae: 48.9728\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 12.1990 - mae: 2.7243 - val_loss: 4542.0400 - val_mae: 48.9674\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 344ms/step - loss: 12.1617 - mae: 2.7192 - val_loss: 4542.7803 - val_mae: 48.9619\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 353ms/step - loss: 12.1244 - mae: 2.7141 - val_loss: 4543.3511 - val_mae: 48.9564\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 304ms/step - loss: 12.0870 - mae: 2.7089 - val_loss: 4543.6343 - val_mae: 48.9510\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 320ms/step - loss: 12.0493 - mae: 2.7038 - val_loss: 4543.5630 - val_mae: 48.9457\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 299ms/step - loss: 12.0112 - mae: 2.6985 - val_loss: 4543.1323 - val_mae: 48.9405\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 11.9727 - mae: 2.6933 - val_loss: 4542.3901 - val_mae: 48.9355\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 312ms/step - loss: 11.9335 - mae: 2.6880 - val_loss: 4541.4321 - val_mae: 48.9305\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 313ms/step - loss: 11.8940 - mae: 2.6826 - val_loss: 4540.3760 - val_mae: 48.9254\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 377ms/step - loss: 11.8543 - mae: 2.6773 - val_loss: 4539.3320 - val_mae: 48.9202\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 296ms/step - loss: 11.8143 - mae: 2.6719 - val_loss: 4538.4126 - val_mae: 48.9149\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 313ms/step - loss: 11.7741 - mae: 2.6664 - val_loss: 4537.6807 - val_mae: 48.9094\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 304ms/step - loss: 11.7339 - mae: 2.6609 - val_loss: 4537.1699 - val_mae: 48.9037\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 295ms/step - loss: 11.6937 - mae: 2.6554 - val_loss: 4536.8701 - val_mae: 48.8978\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 295ms/step - loss: 11.6531 - mae: 2.6498 - val_loss: 4536.7412 - val_mae: 48.8916\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 309ms/step - loss: 11.6118 - mae: 2.6441 - val_loss: 4536.7090 - val_mae: 48.8853\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 11.5706 - mae: 2.6385 - val_loss: 4536.6895 - val_mae: 48.8788\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 11.5294 - mae: 2.6327 - val_loss: 4536.6099 - val_mae: 48.8723\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 306ms/step - loss: 11.4885 - mae: 2.6270 - val_loss: 4536.3975 - val_mae: 48.8658\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 297ms/step - loss: 11.4476 - mae: 2.6212 - val_loss: 4536.0195 - val_mae: 48.8593\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "Predicted GDP for Chernivtsi_Oblast in 2021: [99.00889]\n",
      "Actual GDP for Chernivtsi_Oblast in 2021: 104.875\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - 0s 424ms/step - loss: 20.2821 - mae: 3.1608 - val_loss: 4505.0923 - val_mae: 49.7251\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 303ms/step - loss: 17.9565 - mae: 2.9443 - val_loss: 4435.4727 - val_mae: 48.9928\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 306ms/step - loss: 14.8169 - mae: 2.6960 - val_loss: 4351.6294 - val_mae: 48.0955\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 332ms/step - loss: 12.4852 - mae: 2.6591 - val_loss: 4267.8955 - val_mae: 47.1822\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 309ms/step - loss: 11.7905 - mae: 2.8695 - val_loss: 4195.7651 - val_mae: 46.3812\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 335ms/step - loss: 12.5245 - mae: 3.2500 - val_loss: 4142.8887 - val_mae: 45.7862\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 386ms/step - loss: 13.8078 - mae: 3.5223 - val_loss: 4113.1089 - val_mae: 45.4499\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 309ms/step - loss: 14.6874 - mae: 3.6606 - val_loss: 4106.9658 - val_mae: 45.3844\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 314ms/step - loss: 14.6180 - mae: 3.6594 - val_loss: 4122.2900 - val_mae: 45.5661\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 318ms/step - loss: 13.6370 - mae: 3.5311 - val_loss: 4154.6797 - val_mae: 45.9417\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 305ms/step - loss: 12.2303 - mae: 3.3029 - val_loss: 4198.0122 - val_mae: 46.4375\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 317ms/step - loss: 11.0139 - mae: 3.0123 - val_loss: 4245.0479 - val_mae: 46.9689\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 305ms/step - loss: 10.4108 - mae: 2.7023 - val_loss: 4288.2925 - val_mae: 47.4525\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 312ms/step - loss: 10.4646 - mae: 2.4782 - val_loss: 4321.0923 - val_mae: 47.8171\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 378ms/step - loss: 10.8757 - mae: 2.3508 - val_loss: 4338.7300 - val_mae: 48.0144\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 326ms/step - loss: 11.2130 - mae: 2.3099 - val_loss: 4339.2319 - val_mae: 48.0250\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 311ms/step - loss: 11.1666 - mae: 2.2748 - val_loss: 4323.5513 - val_mae: 47.8595\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 10.6938 - mae: 2.2377 - val_loss: 4295.1694 - val_mae: 47.5541\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 315ms/step - loss: 9.9935 - mae: 2.2666 - val_loss: 4259.1367 - val_mae: 47.1619\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 306ms/step - loss: 9.3510 - mae: 2.3271 - val_loss: 4221.0093 - val_mae: 46.7426\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 298ms/step - loss: 8.9689 - mae: 2.4806 - val_loss: 4185.8813 - val_mae: 46.3528\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 348ms/step - loss: 8.8764 - mae: 2.6372 - val_loss: 4157.6460 - val_mae: 46.0379\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 365ms/step - loss: 8.9513 - mae: 2.7553 - val_loss: 4138.8018 - val_mae: 45.8279\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 319ms/step - loss: 9.0163 - mae: 2.8197 - val_loss: 4130.3105 - val_mae: 45.7354\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 8.9342 - mae: 2.8238 - val_loss: 4131.6191 - val_mae: 45.7551\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 321ms/step - loss: 8.6731 - mae: 2.7701 - val_loss: 4141.0508 - val_mae: 45.8676\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 8.3015 - mae: 2.6686 - val_loss: 4156.0283 - val_mae: 46.0430\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 303ms/step - loss: 7.9324 - mae: 2.5347 - val_loss: 4173.4448 - val_mae: 46.2453\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 391ms/step - loss: 7.6587 - mae: 2.3868 - val_loss: 4190.0977 - val_mae: 46.4381\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 340ms/step - loss: 7.5096 - mae: 2.2437 - val_loss: 4203.1562 - val_mae: 46.5897\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 315ms/step - loss: 7.4457 - mae: 2.1218 - val_loss: 4210.5420 - val_mae: 46.6745\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 315ms/step - loss: 7.3948 - mae: 2.0343 - val_loss: 4211.2563 - val_mae: 46.6813\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 321ms/step - loss: 7.2928 - mae: 1.9883 - val_loss: 4205.4429 - val_mae: 46.6142\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 305ms/step - loss: 7.1210 - mae: 1.9829 - val_loss: 4194.2808 - val_mae: 46.4864\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 310ms/step - loss: 6.9058 - mae: 2.0108 - val_loss: 4179.6719 - val_mae: 46.3188\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 348ms/step - loss: 6.7006 - mae: 2.0616 - val_loss: 4163.8257 - val_mae: 46.1364\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 6.5405 - mae: 2.1213 - val_loss: 4148.8965 - val_mae: 45.9635\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 341ms/step - loss: 6.4370 - mae: 2.1771 - val_loss: 4136.6787 - val_mae: 45.8210\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 367ms/step - loss: 6.3724 - mae: 2.2190 - val_loss: 4128.4209 - val_mae: 45.7235\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 327ms/step - loss: 6.3118 - mae: 2.2385 - val_loss: 4124.3931 - val_mae: 45.6764\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 415ms/step - loss: 6.2267 - mae: 2.2316 - val_loss: 4124.5801 - val_mae: 45.6788\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 443ms/step - loss: 6.1088 - mae: 2.1990 - val_loss: 4128.5439 - val_mae: 45.7239\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 345ms/step - loss: 5.9707 - mae: 2.1451 - val_loss: 4135.1240 - val_mae: 45.7986\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 344ms/step - loss: 5.8327 - mae: 2.0768 - val_loss: 4142.9146 - val_mae: 45.8866\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 325ms/step - loss: 5.7131 - mae: 2.0027 - val_loss: 4150.4873 - val_mae: 45.9714\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 388ms/step - loss: 5.6174 - mae: 1.9312 - val_loss: 4156.6001 - val_mae: 46.0390\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 320ms/step - loss: 5.5377 - mae: 1.8950 - val_loss: 4160.3701 - val_mae: 46.0794\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 331ms/step - loss: 5.4586 - mae: 1.8724 - val_loss: 4161.3799 - val_mae: 46.0880\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 374ms/step - loss: 5.3680 - mae: 1.8565 - val_loss: 4159.7026 - val_mae: 46.0654\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 443ms/step - loss: 5.2624 - mae: 1.8470 - val_loss: 4155.8501 - val_mae: 46.0175\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 373ms/step - loss: 5.1476 - mae: 1.8427 - val_loss: 4150.6396 - val_mae: 45.9535\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 347ms/step - loss: 5.0333 - mae: 1.8414 - val_loss: 4145.0386 - val_mae: 45.8844\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 358ms/step - loss: 4.9271 - mae: 1.8405 - val_loss: 4140.0103 - val_mae: 45.8212\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 392ms/step - loss: 4.8307 - mae: 1.8375 - val_loss: 4136.3491 - val_mae: 45.7731\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 335ms/step - loss: 4.7411 - mae: 1.8395 - val_loss: 4134.5918 - val_mae: 45.7463\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 327ms/step - loss: 4.6512 - mae: 1.8362 - val_loss: 4134.9756 - val_mae: 45.7437\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 331ms/step - loss: 4.5550 - mae: 1.8207 - val_loss: 4137.4077 - val_mae: 45.7640\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 342ms/step - loss: 4.4516 - mae: 1.7935 - val_loss: 4141.5298 - val_mae: 45.8034\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 331ms/step - loss: 4.3438 - mae: 1.7568 - val_loss: 4146.7891 - val_mae: 45.8553\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 333ms/step - loss: 4.2361 - mae: 1.7146 - val_loss: 4152.5361 - val_mae: 45.9122\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 339ms/step - loss: 4.1321 - mae: 1.6806 - val_loss: 4158.1265 - val_mae: 45.9668\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 390ms/step - loss: 4.0323 - mae: 1.6467 - val_loss: 4163.0239 - val_mae: 46.0127\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 345ms/step - loss: 3.9354 - mae: 1.6144 - val_loss: 4166.8809 - val_mae: 46.0458\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 354ms/step - loss: 3.8391 - mae: 1.5846 - val_loss: 4169.5659 - val_mae: 46.0645\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 338ms/step - loss: 3.7409 - mae: 1.5577 - val_loss: 4171.1655 - val_mae: 46.0701\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 325ms/step - loss: 3.6423 - mae: 1.5342 - val_loss: 4171.9634 - val_mae: 46.0659\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 329ms/step - loss: 3.5446 - mae: 1.5129 - val_loss: 4172.3677 - val_mae: 46.0568\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 315ms/step - loss: 3.4508 - mae: 1.4984 - val_loss: 4172.8271 - val_mae: 46.0478\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 326ms/step - loss: 3.3608 - mae: 1.4917 - val_loss: 4173.7495 - val_mae: 46.0439\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 390ms/step - loss: 3.2744 - mae: 1.4825 - val_loss: 4175.4507 - val_mae: 46.0487\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 377ms/step - loss: 3.1899 - mae: 1.4683 - val_loss: 4178.0913 - val_mae: 46.0641\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 340ms/step - loss: 3.1061 - mae: 1.4483 - val_loss: 4181.6758 - val_mae: 46.0902\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 402ms/step - loss: 3.0226 - mae: 1.4223 - val_loss: 4186.0601 - val_mae: 46.1254\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 1s 563ms/step - loss: 2.9400 - mae: 1.3913 - val_loss: 4190.9956 - val_mae: 46.1667\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 454ms/step - loss: 2.8592 - mae: 1.3574 - val_loss: 4196.1572 - val_mae: 46.2105\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 342ms/step - loss: 2.7812 - mae: 1.3369 - val_loss: 4201.2280 - val_mae: 46.2531\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 373ms/step - loss: 2.7062 - mae: 1.3165 - val_loss: 4205.9370 - val_mae: 46.2912\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 362ms/step - loss: 2.6341 - mae: 1.2972 - val_loss: 4210.0967 - val_mae: 46.3227\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 2.5645 - mae: 1.2795 - val_loss: 4213.6343 - val_mae: 46.3468\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 353ms/step - loss: 2.4969 - mae: 1.2639 - val_loss: 4216.5947 - val_mae: 46.3641\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 334ms/step - loss: 2.4313 - mae: 1.2500 - val_loss: 4219.1045 - val_mae: 46.3761\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 353ms/step - loss: 2.3675 - mae: 1.2375 - val_loss: 4221.3677 - val_mae: 46.3853\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 334ms/step - loss: 2.3059 - mae: 1.2255 - val_loss: 4223.6011 - val_mae: 46.3942\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 364ms/step - loss: 2.2468 - mae: 1.2133 - val_loss: 4226.0000 - val_mae: 46.4053\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 340ms/step - loss: 2.1903 - mae: 1.2002 - val_loss: 4228.7114 - val_mae: 46.4204\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 2.1360 - mae: 1.1858 - val_loss: 4231.7871 - val_mae: 46.4403\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 407ms/step - loss: 2.0833 - mae: 1.1698 - val_loss: 4235.2153 - val_mae: 46.4647\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 400ms/step - loss: 2.0320 - mae: 1.1522 - val_loss: 4238.9121 - val_mae: 46.4928\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 410ms/step - loss: 1.9822 - mae: 1.1334 - val_loss: 4242.7334 - val_mae: 46.5228\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 347ms/step - loss: 1.9341 - mae: 1.1138 - val_loss: 4246.5015 - val_mae: 46.5531\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 312ms/step - loss: 1.8877 - mae: 1.1054 - val_loss: 4250.0562 - val_mae: 46.5814\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 1.8430 - mae: 1.0969 - val_loss: 4253.2480 - val_mae: 46.6059\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 306ms/step - loss: 1.7997 - mae: 1.0882 - val_loss: 4255.9785 - val_mae: 46.6255\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 345ms/step - loss: 1.7574 - mae: 1.0796 - val_loss: 4258.2310 - val_mae: 46.6402\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 335ms/step - loss: 1.7161 - mae: 1.0709 - val_loss: 4260.0713 - val_mae: 46.6506\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 293ms/step - loss: 1.6756 - mae: 1.0622 - val_loss: 4261.6387 - val_mae: 46.6584\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 1.6363 - mae: 1.0535 - val_loss: 4263.1074 - val_mae: 46.6660\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 1.5978 - mae: 1.0446 - val_loss: 4264.6499 - val_mae: 46.6750\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 297ms/step - loss: 1.5602 - mae: 1.0356 - val_loss: 4266.3643 - val_mae: 46.6868\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 1.5233 - mae: 1.0264 - val_loss: 4268.2549 - val_mae: 46.7013\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "Predicted GDP for Chernihiv_Oblast in 2021: [94.67925]\n",
      "Actual GDP for Chernihiv_Oblast in 2021: 97.25\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - 0s 448ms/step - loss: 56.2642 - mae: 6.0559 - val_loss: 4400.3677 - val_mae: 51.9508\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 304ms/step - loss: 46.8169 - mae: 5.4014 - val_loss: 4515.3555 - val_mae: 51.8023\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 35.2673 - mae: 4.6300 - val_loss: 4661.4600 - val_mae: 51.6242\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 293ms/step - loss: 29.2437 - mae: 4.6222 - val_loss: 4809.4438 - val_mae: 51.4531\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 31.6829 - mae: 4.8402 - val_loss: 4920.7236 - val_mae: 51.3240\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 38.5378 - mae: 5.2492 - val_loss: 4967.2603 - val_mae: 51.2624\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 299ms/step - loss: 42.6396 - mae: 5.5507 - val_loss: 4944.5151 - val_mae: 51.2759\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 353ms/step - loss: 40.5985 - mae: 5.4074 - val_loss: 4867.2705 - val_mae: 51.3487\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 34.7297 - mae: 5.0236 - val_loss: 4762.2134 - val_mae: 51.4543\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 29.8028 - mae: 4.6644 - val_loss: 4657.0674 - val_mae: 51.5654\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 28.7133 - mae: 4.5857 - val_loss: 4572.4624 - val_mae: 51.6571\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 372ms/step - loss: 30.8914 - mae: 4.5827 - val_loss: 4520.2529 - val_mae: 51.7122\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 500ms/step - loss: 33.6662 - mae: 4.5784 - val_loss: 4504.5713 - val_mae: 51.7228\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 417ms/step - loss: 34.6300 - mae: 4.5724 - val_loss: 4523.2725 - val_mae: 51.6906\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 1s 513ms/step - loss: 33.1873 - mae: 4.5647 - val_loss: 4570.0493 - val_mae: 51.6243\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 435ms/step - loss: 30.4947 - mae: 4.5558 - val_loss: 4634.4814 - val_mae: 51.5377\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 467ms/step - loss: 28.3866 - mae: 4.5461 - val_loss: 4703.1035 - val_mae: 51.4477\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 362ms/step - loss: 28.0047 - mae: 4.5363 - val_loss: 4761.5591 - val_mae: 51.3707\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 374ms/step - loss: 29.0870 - mae: 4.6010 - val_loss: 4797.8306 - val_mae: 51.3196\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 367ms/step - loss: 30.3581 - mae: 4.7171 - val_loss: 4805.4907 - val_mae: 51.3007\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 30.6316 - mae: 4.7356 - val_loss: 4785.3682 - val_mae: 51.3130\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 357ms/step - loss: 29.6924 - mae: 4.6600 - val_loss: 4744.7314 - val_mae: 51.3485\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 337ms/step - loss: 28.2814 - mae: 4.5142 - val_loss: 4694.5278 - val_mae: 51.3955\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 343ms/step - loss: 27.3470 - mae: 4.4932 - val_loss: 4646.0342 - val_mae: 51.4412\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 354ms/step - loss: 27.3053 - mae: 4.4882 - val_loss: 4608.3457 - val_mae: 51.4746\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 358ms/step - loss: 27.8546 - mae: 4.4828 - val_loss: 4587.0894 - val_mae: 51.4886\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 354ms/step - loss: 28.3449 - mae: 4.4766 - val_loss: 4584.1660 - val_mae: 51.4804\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 366ms/step - loss: 28.3058 - mae: 4.4694 - val_loss: 4598.0781 - val_mae: 51.4518\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 346ms/step - loss: 27.7354 - mae: 4.4614 - val_loss: 4624.5010 - val_mae: 51.4083\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 338ms/step - loss: 27.0109 - mae: 4.4528 - val_loss: 4657.1221 - val_mae: 51.3575\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 372ms/step - loss: 26.5567 - mae: 4.4438 - val_loss: 4688.7744 - val_mae: 51.3082\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 26.5329 - mae: 4.4349 - val_loss: 4712.8389 - val_mae: 51.2677\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 326ms/step - loss: 26.7652 - mae: 4.4262 - val_loss: 4724.6807 - val_mae: 51.2415\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 321ms/step - loss: 26.9290 - mae: 4.4183 - val_loss: 4722.6719 - val_mae: 51.2312\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 343ms/step - loss: 26.8082 - mae: 4.4109 - val_loss: 4708.3555 - val_mae: 51.2350\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 323ms/step - loss: 26.4419 - mae: 4.4042 - val_loss: 4685.7798 - val_mae: 51.2484\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 26.0444 - mae: 4.3978 - val_loss: 4660.2432 - val_mae: 51.2653\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 1s 627ms/step - loss: 25.8133 - mae: 4.3916 - val_loss: 4636.9668 - val_mae: 51.2796\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 360ms/step - loss: 25.7882 - mae: 4.3853 - val_loss: 4620.0293 - val_mae: 51.2863\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 327ms/step - loss: 25.8551 - mae: 4.3787 - val_loss: 4611.7705 - val_mae: 51.2825\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 339ms/step - loss: 25.8569 - mae: 4.3716 - val_loss: 4612.6128 - val_mae: 51.2675\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 335ms/step - loss: 25.7149 - mae: 4.3638 - val_loss: 4621.2070 - val_mae: 51.2429\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 320ms/step - loss: 25.4700 - mae: 4.3555 - val_loss: 4634.8477 - val_mae: 51.2121\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 332ms/step - loss: 25.2311 - mae: 4.3469 - val_loss: 4650.0562 - val_mae: 51.1792\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 407ms/step - loss: 25.0836 - mae: 4.3382 - val_loss: 4663.3013 - val_mae: 51.1486\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 331ms/step - loss: 25.0317 - mae: 4.3295 - val_loss: 4671.7134 - val_mae: 51.1234\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 327ms/step - loss: 25.0101 - mae: 4.3210 - val_loss: 4673.6641 - val_mae: 51.1058\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 331ms/step - loss: 24.9427 - mae: 4.3128 - val_loss: 4669.0337 - val_mae: 51.0958\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 1s 524ms/step - loss: 24.8024 - mae: 4.3050 - val_loss: 4659.1060 - val_mae: 51.0919\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 368ms/step - loss: 24.6217 - mae: 4.2974 - val_loss: 4646.1250 - val_mae: 51.0916\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 343ms/step - loss: 24.4549 - mae: 4.2900 - val_loss: 4632.7168 - val_mae: 51.0916\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 352ms/step - loss: 24.3367 - mae: 4.2824 - val_loss: 4621.2754 - val_mae: 51.0890\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 328ms/step - loss: 24.2587 - mae: 4.2747 - val_loss: 4613.5151 - val_mae: 51.0818\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 320ms/step - loss: 24.1851 - mae: 4.2665 - val_loss: 4610.2378 - val_mae: 51.0688\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 328ms/step - loss: 24.0835 - mae: 4.2581 - val_loss: 4611.2578 - val_mae: 51.0502\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 353ms/step - loss: 23.9478 - mae: 4.2493 - val_loss: 4615.5518 - val_mae: 51.0273\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 357ms/step - loss: 23.7978 - mae: 4.2402 - val_loss: 4621.5396 - val_mae: 51.0020\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 327ms/step - loss: 23.6590 - mae: 4.2307 - val_loss: 4627.4189 - val_mae: 50.9766\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 325ms/step - loss: 23.5444 - mae: 4.2211 - val_loss: 4631.5781 - val_mae: 50.9531\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 323ms/step - loss: 23.4455 - mae: 4.2114 - val_loss: 4632.8892 - val_mae: 50.9329\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 336ms/step - loss: 23.3465 - mae: 4.2021 - val_loss: 4630.9033 - val_mae: 50.9164\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 315ms/step - loss: 23.2300 - mae: 4.1928 - val_loss: 4625.9224 - val_mae: 50.9033\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 397ms/step - loss: 23.0958 - mae: 4.1836 - val_loss: 4618.8291 - val_mae: 50.8928\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 335ms/step - loss: 22.9563 - mae: 4.1744 - val_loss: 4610.8447 - val_mae: 50.8831\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 316ms/step - loss: 22.8261 - mae: 4.1652 - val_loss: 4603.2417 - val_mae: 50.8727\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 314ms/step - loss: 22.7066 - mae: 4.1557 - val_loss: 4597.0635 - val_mae: 50.8602\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 319ms/step - loss: 22.5921 - mae: 4.1461 - val_loss: 4592.9429 - val_mae: 50.8448\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 330ms/step - loss: 22.4741 - mae: 4.1362 - val_loss: 4591.0205 - val_mae: 50.8262\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 316ms/step - loss: 22.3465 - mae: 4.1260 - val_loss: 4590.9712 - val_mae: 50.8047\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 315ms/step - loss: 22.2129 - mae: 4.1156 - val_loss: 4592.1279 - val_mae: 50.7823\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 372ms/step - loss: 22.0779 - mae: 4.1050 - val_loss: 4593.6201 - val_mae: 50.7590\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 485ms/step - loss: 21.9476 - mae: 4.0942 - val_loss: 4594.5840 - val_mae: 50.7360\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 21.8225 - mae: 4.0834 - val_loss: 4594.3330 - val_mae: 50.7141\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 340ms/step - loss: 21.6998 - mae: 4.0727 - val_loss: 4592.4980 - val_mae: 50.6945\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 360ms/step - loss: 21.5714 - mae: 4.0618 - val_loss: 4589.0664 - val_mae: 50.6772\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 330ms/step - loss: 21.4379 - mae: 4.0510 - val_loss: 4584.3667 - val_mae: 50.6614\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 353ms/step - loss: 21.3005 - mae: 4.0400 - val_loss: 4578.9448 - val_mae: 50.6461\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 335ms/step - loss: 21.1625 - mae: 4.0288 - val_loss: 4573.4209 - val_mae: 50.6304\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 416ms/step - loss: 21.0255 - mae: 4.0175 - val_loss: 4568.3599 - val_mae: 50.6135\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 342ms/step - loss: 20.8890 - mae: 4.0060 - val_loss: 4564.1611 - val_mae: 50.5949\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 336ms/step - loss: 20.7542 - mae: 3.9944 - val_loss: 4561.0015 - val_mae: 50.5751\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 324ms/step - loss: 20.6161 - mae: 3.9826 - val_loss: 4558.8223 - val_mae: 50.5540\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 342ms/step - loss: 20.4732 - mae: 3.9704 - val_loss: 4557.3716 - val_mae: 50.5321\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 333ms/step - loss: 20.3280 - mae: 3.9578 - val_loss: 4556.2705 - val_mae: 50.5098\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 326ms/step - loss: 20.1854 - mae: 3.9453 - val_loss: 4555.1021 - val_mae: 50.4869\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 389ms/step - loss: 20.0427 - mae: 3.9326 - val_loss: 4553.4961 - val_mae: 50.4639\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 353ms/step - loss: 19.8994 - mae: 3.9196 - val_loss: 4551.1885 - val_mae: 50.4414\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 322ms/step - loss: 19.7523 - mae: 3.9060 - val_loss: 4548.0542 - val_mae: 50.4193\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 360ms/step - loss: 19.6003 - mae: 3.8920 - val_loss: 4544.1484 - val_mae: 50.3979\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 326ms/step - loss: 19.4431 - mae: 3.8773 - val_loss: 4539.6470 - val_mae: 50.3769\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 330ms/step - loss: 19.2858 - mae: 3.8626 - val_loss: 4534.8179 - val_mae: 50.3566\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 422ms/step - loss: 19.1122 - mae: 3.8464 - val_loss: 4529.9712 - val_mae: 50.3370\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 323ms/step - loss: 18.9066 - mae: 3.8274 - val_loss: 4525.3374 - val_mae: 50.3175\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 316ms/step - loss: 18.6817 - mae: 3.8060 - val_loss: 4521.0435 - val_mae: 50.2960\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 410ms/step - loss: 18.4455 - mae: 3.7835 - val_loss: 4517.0771 - val_mae: 50.2721\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 468ms/step - loss: 18.1829 - mae: 3.7588 - val_loss: 4513.2490 - val_mae: 50.2470\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 345ms/step - loss: 17.8781 - mae: 3.7295 - val_loss: 4509.2144 - val_mae: 50.2210\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 334ms/step - loss: 17.5463 - mae: 3.6964 - val_loss: 4504.6050 - val_mae: 50.1960\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 17.2152 - mae: 3.6656 - val_loss: 4499.0439 - val_mae: 50.1723\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 374ms/step - loss: 16.8835 - mae: 3.6362 - val_loss: 4492.2676 - val_mae: 50.1494\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "Predicted GDP for Kyiv in 2021: [106.971794]\n",
      "Actual GDP for Kyiv in 2021: 114.75\n"
     ]
    }
   ],
   "source": [
    "# load clean gdp data\n",
    "gdp = pd.read_csv(\"data/clean_ukr_gdp.csv\")\n",
    "\n",
    "# initialise a dictionary to store the results\n",
    "results = {\"region\": [], \"predicted_gdp\": [], \"actual_gdp\": [], \"mae\": [], \"percentage_error\": []}\n",
    "\n",
    "# delete observations with year > 2021\n",
    "gdp = gdp[gdp[\"year\"].astype(int) <= 2021]\n",
    "\n",
    "# get unique regions and years\n",
    "regions = gdp[\"region\"].unique()\n",
    "# regions = ['Cherkasy_Oblast']\n",
    "years = gdp[\"year\"].unique()\n",
    "\n",
    "# load the snow covered and snow free images, add them together and append to the list\n",
    "for region in regions:\n",
    "\n",
    "    # initialise data\n",
    "    X = np.zeros((len(years), 765, 1076, 2))\n",
    "    X_test = np.zeros((1, 765, 1076, 2))\n",
    "    y = np.zeros(len(years))\n",
    "\n",
    "    for i in range(len(years)):\n",
    "\n",
    "        year = years[i]\n",
    "        gdp_value = gdp[(gdp[\"region\"] == region) & (gdp[\"year\"] == year)][\"real_gdp\"].values[0]\n",
    "\n",
    "        # get the file name\n",
    "        file_name = f\"{year}_{region}.h5\"\n",
    "\n",
    "        # load the image\n",
    "        file_path = f\"data/annual_region_images/{file_name}\"\n",
    "    \n",
    "        with h5py.File(file_path, 'r') as annual_region:\n",
    "            # nearnad_snow_cov = annual_region[\"NearNadir_Composite_Snow_Covered\"][:]\n",
    "            nearnad_snow_free = annual_region[\"NearNadir_Composite_Snow_Free\"][:]\n",
    "            # offnad_snow_cov = annual_region[\"OffNadir_Composite_Snow_Covered\"][:]\n",
    "            offnad_snow_free = annual_region[\"OffNadir_Composite_Snow_Free\"][:]\n",
    "            # allangle_snow_cov = annual_region[\"AllAngle_Composite_Snow_Covered\"][:]\n",
    "            # allangle_snow_free = annual_region[\"AllAngle_Composite_Snow_Free\"][:]\n",
    "\n",
    "            # add the two images together\n",
    "            # combined = snow_covered + snow_free\n",
    "\n",
    "        # add the gdp value to y\n",
    "\n",
    "        if year != 2021:\n",
    "            y[i] = gdp_value\n",
    "\n",
    "            # append both images as two channels to to X\n",
    "            # X[i, :, :, 0] = allangle_snow_cov\n",
    "            # X[i, :, :, 1] = allangle_snow_free\n",
    "            # X[i, :, :, 2] = offnad_snow_cov\n",
    "            X[i, :, :, 0] = offnad_snow_free \n",
    "            # X[i, :, :, 4] = nearnad_snow_cov\n",
    "            X[i, :, :, 1] = nearnad_snow_free \n",
    "        else:\n",
    "            X_test[0, :, :, 0] = offnad_snow_free\n",
    "            X_test[0, :, :, 1] = nearnad_snow_free\n",
    "\n",
    "            y_test = gdp_value\n",
    "\n",
    "        # # Normalise the images\n",
    "        # X[:, :, :, 0] = X[:, :, :, 0] / X[:, :, :, 0].max()\n",
    "        # X[:, :, :, 1] = X[:, :, :, 1] / X[:, :, :, 1].max()\n",
    "\n",
    "        # X_test[:, :, :, 0] = X_test[:, :, :, 0] / X[:, :, :, 0].max()\n",
    "        # X_test[:, :, :, 1] = X_test[:, :, :, 1] / X[:, :, :, 1].max()\n",
    "\n",
    "        # # normalise GDP values\n",
    "        # y = y / y.max()\n",
    "        # y_test = y_test / y.max()\n",
    "\n",
    "\n",
    "    # change the format to a float16\n",
    "    X = X.astype(np.float16)\n",
    "    y = y.astype(np.float16)\n",
    "    X_test = X_test.astype(np.float16)\n",
    "    y_test = y_test.astype(np.float16)\n",
    "\n",
    "    # fit the model on the data\n",
    "    model.fit(X, y, epochs=100, batch_size=16, validation_split=0.2)  # Assuming you have a validation split of 20%\n",
    "\n",
    "    # predict the gdp for 2021\n",
    "    y_hat = model.predict(X_test).flatten()\n",
    "\n",
    "    print(f\"Predicted GDP for {region} in 2021: {y_hat}\")\n",
    "    print(f\"Actual GDP for {region} in 2021: {y_test}\")\n",
    "\n",
    "    # add the results to the dictionary\n",
    "    mae = np.mean(np.abs(y_test - y_hat))\n",
    "    percentage_error = np.mean(100*np.abs((y_test - y_hat) / y_test))\n",
    "\n",
    "    results[\"region\"].append(region)\n",
    "    results[\"predicted_gdp\"].append(y_hat)\n",
    "    results[\"actual_gdp\"].append(y_test)\n",
    "    results[\"mae\"].append(mae)\n",
    "    results[\"percentage_error\"].append(percentage_error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[9, :, :, 0].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.291629"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percentage_error = results[\"percentage_error\"]\n",
    "np.mean(percentage_error) # 7.3% mean percentage error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "\n",
    "# load clean gdp data\n",
    "gdp = pd.read_csv(\"data/clean_gdp.csv\")\n",
    "\n",
    "# Initialise a three dimensional array to store the images with the shape (number of images, height, width, channels)\n",
    "X = np.zeros((len(gdp), 765, 1076, 4))\n",
    "y = np.zeros(len(gdp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest and other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the tabular data\n",
    "data = pd.read_csv(\"data/tabular_data_ukraine.csv\")\n",
    "\n",
    "# turn the region column into a categorical variable using one hot encoding\n",
    "data = pd.get_dummies(data, columns=[\"region\"])\n",
    "\n",
    "# training data contains years until 2021\n",
    "train_data = data[data[\"year\"] <= 2021]\n",
    "test_data = data[data[\"year\"] >= 2022]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.013127711837708958\n",
      "9.505047946852148\n"
     ]
    }
   ],
   "source": [
    "# train a random forest model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# drop year and gdp columns\n",
    "X = train_data.drop(columns=[\"year\", \"gdp\"])\n",
    "y = train_data[\"gdp\"]\n",
    "\n",
    "# standardise gdp\n",
    "# y_mean = y.mean()\n",
    "# y_std = y.std()\n",
    "# y = (y - y_mean) / y_std\n",
    "\n",
    "y_max = y.max()\n",
    "y = y / y_max\n",
    "\n",
    "# take randomly 80% of the data for training\n",
    "train_size = int(0.8 * len(train_data))\n",
    "test_size = len(train_data) - train_size\n",
    "\n",
    "train_indices = np.random.choice(len(train_data), train_size, replace=False)\n",
    "test_indices = np.setdiff1d(np.arange(len(train_data)), train_indices)\n",
    "\n",
    "X_train = X.iloc[train_indices]\n",
    "y_train = y.iloc[train_indices]\n",
    "X_test = X.iloc[test_indices]\n",
    "y_test = y.iloc[test_indices]\n",
    "\n",
    "# train the model\n",
    "rf_model = RandomForestRegressor(n_estimators=1000, random_state = 0)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# get the predictions\n",
    "y_hat = rf_model.predict(X_test)\n",
    "\n",
    "# compute the mae\n",
    "mae = np.mean(np.abs(y_test - y_hat))\n",
    "\n",
    "# compute the mean percentage error\n",
    "percentage_error = np.mean(100*np.abs((y_test - y_hat) / y_test))\n",
    "\n",
    "print(mae)\n",
    "print(percentage_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05457855129920569\n",
      "9.11219542637664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_8528/1922239168.py:6: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  model.fit(X_train, y_train)\n",
      "c:\\Users\\jakub\\anaconda\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:530: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\jakub\\anaconda\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.3410191651259114, tolerance: 0.0003713225983503325\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "# fit a lasso regression model \n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "model = Lasso(alpha=0)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# get the predictions\n",
    "y_hat = model.predict(X_test)\n",
    "\n",
    "# compute the mae\n",
    "mae = np.mean(np.abs(y_test - y_hat))\n",
    "\n",
    "# compute the mean percentage error\n",
    "percentage_error = np.mean(100*np.abs((y_test - y_hat) / y_test))\n",
    "\n",
    "print(mae)\n",
    "print(percentage_error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.013926091130794049\n",
      "8.009536583409178\n"
     ]
    }
   ],
   "source": [
    "# fit a XGBoost model\n",
    "import xgboost as xgb\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(objective=\"reg:squarederror\", random_state=0)\n",
    "\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# get the predictions\n",
    "y_hat = xgb_model.predict(X_test)\n",
    "\n",
    "# compute the mae\n",
    "mae = np.mean(np.abs(y_test - y_hat))\n",
    "\n",
    "# compute the mean percentage error\n",
    "percentage_error = np.mean(100*np.abs((y_test - y_hat) / y_test))\n",
    "\n",
    "print(mae)\n",
    "print(percentage_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[col] = X_train[col].astype(int)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[col] = X_train[col].astype(int)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[col] = X_train[col].astype(int)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[col] = X_train[col].astype(int)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[col] = X_train[col].astype(int)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[col] = X_train[col].astype(int)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[col] = X_train[col].astype(int)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[col] = X_train[col].astype(int)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[col] = X_train[col].astype(int)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[col] = X_train[col].astype(int)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[col] = X_train[col].astype(int)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[col] = X_train[col].astype(int)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[col] = X_train[col].astype(int)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[col] = X_train[col].astype(int)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[col] = X_train[col].astype(int)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[col] = X_train[col].astype(int)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[col] = X_train[col].astype(int)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[col] = X_train[col].astype(int)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[col] = X_train[col].astype(int)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[col] = X_train[col].astype(int)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[col] = X_train[col].astype(int)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[col] = X_train[col].astype(int)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[col] = X_train[col].astype(int)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[col] = X_train[col].astype(int)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[col] = X_train[col].astype(int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:From c:\\Users\\jakub\\anaconda\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "5/5 [==============================] - 2s 73ms/step - loss: 61967528099840.0000 - mae: 5316903.0000 - val_loss: 70135830282240.0000 - val_mae: 5151166.0000\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 61967184166912.0000 - mae: 5316874.5000 - val_loss: 70135326965760.0000 - val_mae: 5151121.5000\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 61966458552320.0000 - mae: 5316811.0000 - val_loss: 70134311944192.0000 - val_mae: 5151032.5000\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 61965162512384.0000 - mae: 5316689.0000 - val_loss: 70132260929536.0000 - val_mae: 5150853.0000\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 61962226499584.0000 - mae: 5316441.5000 - val_loss: 70128213426176.0000 - val_mae: 5150499.0000\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 61957075894272.0000 - mae: 5315957.0000 - val_loss: 70120185528320.0000 - val_mae: 5149804.0000\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 61946640465920.0000 - mae: 5315005.0000 - val_loss: 70104469471232.0000 - val_mae: 5148452.0000\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 61926910459904.0000 - mae: 5313211.0000 - val_loss: 70074853490688.0000 - val_mae: 5145915.5000\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 61889350467584.0000 - mae: 5309818.5000 - val_loss: 70021342560256.0000 - val_mae: 5141353.0000\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 61824644939776.0000 - mae: 5303982.0000 - val_loss: 69927771832320.0000 - val_mae: 5133415.5000\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 61705908387840.0000 - mae: 5293576.0000 - val_loss: 69768765767680.0000 - val_mae: 5119957.0000\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 61510298632192.0000 - mae: 5276333.5000 - val_loss: 69504658833408.0000 - val_mae: 5097642.5000\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 61213497098240.0000 - mae: 5249571.0000 - val_loss: 69077515108352.0000 - val_mae: 5061492.0000\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 60708465147904.0000 - mae: 5205500.0000 - val_loss: 68425049178112.0000 - val_mae: 5005767.0000\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 60017457758208.0000 - mae: 5134803.0000 - val_loss: 67455036686336.0000 - val_mae: 4921823.0000\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 58958853177344.0000 - mae: 5035533.0000 - val_loss: 66020983177216.0000 - val_mae: 4795290.5000\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 57394507808768.0000 - mae: 4880711.5000 - val_loss: 63917468418048.0000 - val_mae: 4603982.5000\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 55065092030464.0000 - mae: 4661265.0000 - val_loss: 61045720743936.0000 - val_mae: 4329786.5000\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 52275330416640.0000 - mae: 4364453.5000 - val_loss: 57265751064576.0000 - val_mae: 3941495.2500\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 48183493263360.0000 - mae: 3930869.2500 - val_loss: 52543010897920.0000 - val_mae: 3418556.7500\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 43460757291008.0000 - mae: 3499256.7500 - val_loss: 47276227559424.0000 - val_mae: 2985466.5000\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 38957257588736.0000 - mae: 3169831.5000 - val_loss: 42398533025792.0000 - val_mae: 2773229.5000\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 35193217875968.0000 - mae: 3126442.5000 - val_loss: 38898101125120.0000 - val_mae: 2929644.2500\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 34060258770944.0000 - mae: 3444804.0000 - val_loss: 37351988396032.0000 - val_mae: 3359900.5000\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 33696572768256.0000 - mae: 3629973.2500 - val_loss: 36701938384896.0000 - val_mae: 3408358.5000\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 32962032697344.0000 - mae: 3695509.5000 - val_loss: 36014022197248.0000 - val_mae: 3290199.2500\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 31580531720192.0000 - mae: 3449490.0000 - val_loss: 35592490450944.0000 - val_mae: 3096213.5000\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 31063833313280.0000 - mae: 3336971.2500 - val_loss: 35190822928384.0000 - val_mae: 3017992.0000\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 32354844278784.0000 - mae: 3349326.0000 - val_loss: 34867091865600.0000 - val_mae: 2958705.0000\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 30585126912000.0000 - mae: 3248770.5000 - val_loss: 34324804009984.0000 - val_mae: 2946596.5000\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 30136439144448.0000 - mae: 3205174.5000 - val_loss: 33630841733120.0000 - val_mae: 2957802.5000\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 29654444408832.0000 - mae: 3257899.0000 - val_loss: 33151380357120.0000 - val_mae: 2939147.0000\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 30052100079616.0000 - mae: 3261328.5000 - val_loss: 32589849034752.0000 - val_mae: 2934494.0000\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 29801201008640.0000 - mae: 3315169.2500 - val_loss: 32109175504896.0000 - val_mae: 2918576.5000\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 27664328949760.0000 - mae: 3213127.5000 - val_loss: 31609042501632.0000 - val_mae: 2906968.5000\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 27342957182976.0000 - mae: 3167449.2500 - val_loss: 31232033292288.0000 - val_mae: 2877702.2500\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 28762771030016.0000 - mae: 3254063.0000 - val_loss: 30440335343616.0000 - val_mae: 2893129.0000\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 27481660719104.0000 - mae: 3224133.2500 - val_loss: 29884176924672.0000 - val_mae: 2881265.5000\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 27991790845952.0000 - mae: 3202549.2500 - val_loss: 29330369413120.0000 - val_mae: 2868136.5000\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 26984964947968.0000 - mae: 3239939.5000 - val_loss: 28980744814592.0000 - val_mae: 2837043.2500\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 24606450647040.0000 - mae: 3055275.0000 - val_loss: 28557522763776.0000 - val_mae: 2811614.0000\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 26112449052672.0000 - mae: 3094432.5000 - val_loss: 28177132945408.0000 - val_mae: 2784619.5000\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 26000905732096.0000 - mae: 3075671.2500 - val_loss: 27679292129280.0000 - val_mae: 2772408.2500\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 24759228170240.0000 - mae: 3070260.0000 - val_loss: 26890203037696.0000 - val_mae: 2771435.2500\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 25010494242816.0000 - mae: 3099732.5000 - val_loss: 26489194020864.0000 - val_mae: 2756533.7500\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 25423438151680.0000 - mae: 3015110.5000 - val_loss: 26186050699264.0000 - val_mae: 2736633.2500\n",
      "Epoch 47/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 23503684239360.0000 - mae: 3006075.5000 - val_loss: 25714040504320.0000 - val_mae: 2722254.0000\n",
      "Epoch 48/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 25030140362752.0000 - mae: 3040248.0000 - val_loss: 25088248250368.0000 - val_mae: 2712660.0000\n",
      "Epoch 49/100\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 22834298486784.0000 - mae: 3031668.7500 - val_loss: 24485866504192.0000 - val_mae: 2701776.7500\n",
      "Epoch 50/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 22718940446720.0000 - mae: 3027556.0000 - val_loss: 24352573620224.0000 - val_mae: 2690159.7500\n",
      "Epoch 51/100\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 22043705737216.0000 - mae: 2886051.5000 - val_loss: 23842533670912.0000 - val_mae: 2676057.0000\n",
      "Epoch 52/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 22445318733824.0000 - mae: 2945980.7500 - val_loss: 23186380947456.0000 - val_mae: 2661384.7500\n",
      "Epoch 53/100\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 22200895668224.0000 - mae: 2975536.2500 - val_loss: 22459512258560.0000 - val_mae: 2657821.2500\n",
      "Epoch 54/100\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 22407502888960.0000 - mae: 2957141.0000 - val_loss: 22151656636416.0000 - val_mae: 2641429.5000\n",
      "Epoch 55/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 20293382832128.0000 - mae: 2902677.5000 - val_loss: 22087303430144.0000 - val_mae: 2617665.5000\n",
      "Epoch 56/100\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 22978272165888.0000 - mae: 2981542.5000 - val_loss: 21899669143552.0000 - val_mae: 2602391.5000\n",
      "Epoch 57/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 22571441455104.0000 - mae: 2938034.0000 - val_loss: 21805158891520.0000 - val_mae: 2588491.2500\n",
      "Epoch 58/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 20920584372224.0000 - mae: 2825619.7500 - val_loss: 21395547357184.0000 - val_mae: 2576968.5000\n",
      "Epoch 59/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 19933121478656.0000 - mae: 2800611.5000 - val_loss: 20734543921152.0000 - val_mae: 2566286.5000\n",
      "Epoch 60/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 20422131187712.0000 - mae: 2845140.7500 - val_loss: 20185610190848.0000 - val_mae: 2552767.5000\n",
      "Epoch 61/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 18968666439680.0000 - mae: 2816516.5000 - val_loss: 19789820985344.0000 - val_mae: 2537590.0000\n",
      "Epoch 62/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 19026564612096.0000 - mae: 2769060.7500 - val_loss: 19557968248832.0000 - val_mae: 2520814.5000\n",
      "Epoch 63/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 19283394428928.0000 - mae: 2820034.5000 - val_loss: 19112023556096.0000 - val_mae: 2505041.5000\n",
      "Epoch 64/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 17989648777216.0000 - mae: 2720193.7500 - val_loss: 18767316779008.0000 - val_mae: 2485657.5000\n",
      "Epoch 65/100\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 17444758355968.0000 - mae: 2683651.2500 - val_loss: 18311028932608.0000 - val_mae: 2468916.2500\n",
      "Epoch 66/100\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 18693301993472.0000 - mae: 2825991.5000 - val_loss: 17632979845120.0000 - val_mae: 2451691.5000\n",
      "Epoch 67/100\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 16757167226880.0000 - mae: 2702068.2500 - val_loss: 17440517914624.0000 - val_mae: 2431668.7500\n",
      "Epoch 68/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 16746511597568.0000 - mae: 2684744.7500 - val_loss: 17093326012416.0000 - val_mae: 2413734.5000\n",
      "Epoch 69/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 17126544900096.0000 - mae: 2689137.2500 - val_loss: 16687578480640.0000 - val_mae: 2394496.5000\n",
      "Epoch 70/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 17304425332736.0000 - mae: 2689955.2500 - val_loss: 16472653955072.0000 - val_mae: 2377190.0000\n",
      "Epoch 71/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 16697893322752.0000 - mae: 2707360.7500 - val_loss: 16255162515456.0000 - val_mae: 2358299.7500\n",
      "Epoch 72/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 16112673619968.0000 - mae: 2653412.0000 - val_loss: 16133031723008.0000 - val_mae: 2338552.5000\n",
      "Epoch 73/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 16220027879424.0000 - mae: 2653209.5000 - val_loss: 16247576068096.0000 - val_mae: 2327660.0000\n",
      "Epoch 74/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 16558731558912.0000 - mae: 2626236.5000 - val_loss: 15582239916032.0000 - val_mae: 2302967.2500\n",
      "Epoch 75/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 14872072945664.0000 - mae: 2502864.7500 - val_loss: 15230932353024.0000 - val_mae: 2280333.5000\n",
      "Epoch 76/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 16111057764352.0000 - mae: 2558505.2500 - val_loss: 14890838261760.0000 - val_mae: 2257928.2500\n",
      "Epoch 77/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 14995277479936.0000 - mae: 2552911.7500 - val_loss: 14821979324416.0000 - val_mae: 2240116.5000\n",
      "Epoch 78/100\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 16272500719616.0000 - mae: 2532241.7500 - val_loss: 14716453781504.0000 - val_mae: 2221859.0000\n",
      "Epoch 79/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 15601320853504.0000 - mae: 2604241.7500 - val_loss: 14659322118144.0000 - val_mae: 2206184.5000\n",
      "Epoch 80/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 15771260420096.0000 - mae: 2497339.0000 - val_loss: 14026554736640.0000 - val_mae: 2177935.5000\n",
      "Epoch 81/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 15252108345344.0000 - mae: 2518715.5000 - val_loss: 13386005872640.0000 - val_mae: 2149033.5000\n",
      "Epoch 82/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 14588681650176.0000 - mae: 2481270.5000 - val_loss: 12921814908928.0000 - val_mae: 2123688.2500\n",
      "Epoch 83/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 14198940631040.0000 - mae: 2501149.5000 - val_loss: 12699636334592.0000 - val_mae: 2102179.2500\n",
      "Epoch 84/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 13328113991680.0000 - mae: 2387065.2500 - val_loss: 12369113645056.0000 - val_mae: 2077580.6250\n",
      "Epoch 85/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 12483813179392.0000 - mae: 2336126.2500 - val_loss: 11995669594112.0000 - val_mae: 2051392.0000\n",
      "Epoch 86/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 13234544312320.0000 - mae: 2424830.0000 - val_loss: 11953789468672.0000 - val_mae: 2033024.7500\n",
      "Epoch 87/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 14334372610048.0000 - mae: 2414911.5000 - val_loss: 11700206043136.0000 - val_mae: 2007441.0000\n",
      "Epoch 88/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 10976083574784.0000 - mae: 2259399.0000 - val_loss: 11552383041536.0000 - val_mae: 1989954.2500\n",
      "Epoch 89/100\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 12023264968704.0000 - mae: 2292919.5000 - val_loss: 11626314989568.0000 - val_mae: 1987970.0000\n",
      "Epoch 90/100\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 12626639716352.0000 - mae: 2290527.5000 - val_loss: 11375087714304.0000 - val_mae: 1968959.6250\n",
      "Epoch 91/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 10162164203520.0000 - mae: 2125319.2500 - val_loss: 10796884033536.0000 - val_mae: 1928000.2500\n",
      "Epoch 92/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 11524256038912.0000 - mae: 2227295.0000 - val_loss: 10545914707968.0000 - val_mae: 1905091.2500\n",
      "Epoch 93/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 10784746766336.0000 - mae: 2195225.5000 - val_loss: 10216077787136.0000 - val_mae: 1882451.2500\n",
      "Epoch 94/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 11691471405056.0000 - mae: 2198781.5000 - val_loss: 10173043179520.0000 - val_mae: 1872127.6250\n",
      "Epoch 95/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 10473355345920.0000 - mae: 2162363.2500 - val_loss: 9654604136448.0000 - val_mae: 1848404.0000\n",
      "Epoch 96/100\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 10955121491968.0000 - mae: 2145363.5000 - val_loss: 9264418521088.0000 - val_mae: 1824381.6250\n",
      "Epoch 97/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 11056406593536.0000 - mae: 2188752.5000 - val_loss: 9022032838656.0000 - val_mae: 1809183.0000\n",
      "Epoch 98/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 8954667073536.0000 - mae: 2045728.6250 - val_loss: 9252488871936.0000 - val_mae: 1814025.3750\n",
      "Epoch 99/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 9320868610048.0000 - mae: 2016403.0000 - val_loss: 9330285871104.0000 - val_mae: 1811033.0000\n",
      "Epoch 100/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 9616468475904.0000 - mae: 1965337.6250 - val_loss: 8748979978240.0000 - val_mae: 1783583.6250\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2007276.4348418936\n",
      "61.515446011157735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test[col] = X_test[col].astype(int)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test[col] = X_test[col].astype(int)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test[col] = X_test[col].astype(int)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test[col] = X_test[col].astype(int)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test[col] = X_test[col].astype(int)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test[col] = X_test[col].astype(int)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test[col] = X_test[col].astype(int)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test[col] = X_test[col].astype(int)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test[col] = X_test[col].astype(int)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test[col] = X_test[col].astype(int)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test[col] = X_test[col].astype(int)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test[col] = X_test[col].astype(int)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test[col] = X_test[col].astype(int)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test[col] = X_test[col].astype(int)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test[col] = X_test[col].astype(int)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test[col] = X_test[col].astype(int)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test[col] = X_test[col].astype(int)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test[col] = X_test[col].astype(int)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test[col] = X_test[col].astype(int)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test[col] = X_test[col].astype(int)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test[col] = X_test[col].astype(int)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test[col] = X_test[col].astype(int)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test[col] = X_test[col].astype(int)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test[col] = X_test[col].astype(int)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test[col] = X_test[col].astype(int)\n"
     ]
    }
   ],
   "source": [
    "# fit a neural network model\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Resizing, Dropout, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# turn X_train bool to integers\n",
    "boolean_columns = [col for col in X_train.columns if X_train[col].dtype == bool]\n",
    "\n",
    "for col in boolean_columns:\n",
    "    X_train[col] = X_train[col].astype(int) \n",
    " \n",
    "\n",
    "# fit the model on the X_train and y_train\n",
    "model = Sequential()\n",
    "model.add(Dense(256, activation=\"relu\", input_dim=X_train.shape[1]))  # More complex first layer\n",
    "model.add(Dropout(0.3))  # Dropout to reduce overfitting\n",
    "model.add(Dense(128, activation=\"relu\"))  # Second layer\n",
    "model.add(Dropout(0.2))  # Additional Dropout layer\n",
    "model.add(Dense(64, activation=\"relu\"))  # Third layer\n",
    "model.add(Dense(32, activation=\"relu\"))  # Fourth layer\n",
    "model.add(Dropout(0.1))  # Dropout layer\n",
    "model.add(Dense(16, activation=\"relu\"))  # New additional layer\n",
    "model.add(Dense(1)) \n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"mean_squared_error\", metrics=[\"mae\"])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# get the predictions\n",
    "boolean_columns = [col for col in X_test.columns if X_test[col].dtype == bool]\n",
    "for col in boolean_columns:\n",
    "    X_test[col] = X_test[col].astype(int)\n",
    "\n",
    "y_hat = model.predict(X_test).flatten()\n",
    "\n",
    "# compute the mae\n",
    "mae = np.mean(np.abs(y_test - y_hat))\n",
    "\n",
    "# compute the mean percentage error\n",
    "percentage_error = np.mean(100*np.abs((y_test - y_hat) / y_test))\n",
    "\n",
    "print(mae)\n",
    "print(percentage_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model on the entire dataset, predict for year 2023\n",
    "best_model = xgb_model\n",
    "\n",
    "# read the data\n",
    "data = pd.read_csv(\"data/tabular_data_ukraine.csv\")\n",
    "data = pd.get_dummies(data, columns=[\"region\"])\n",
    "\n",
    "# get train and test\n",
    "train_data = data[data[\"year\"] <= 2021]\n",
    "test_data = data[data[\"year\"] == 2022]\n",
    "\n",
    "X_train = train_data.drop(columns=[\"year\", \"gdp\"])\n",
    "y_train = train_data[\"gdp\"]\n",
    "\n",
    "X_test = test_data.drop(columns=[\"year\", \"gdp\"])\n",
    "\n",
    "# train \n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# predict\n",
    "y_hat = best_model.predict(X_test)\n",
    "\n",
    "# add the predictions to the test_data\n",
    "test_data = pd.read_csv(\"data/tabular_data_ukraine.csv\")\n",
    "test_data = test_data[test_data[\"year\"] == 2022]\n",
    "\n",
    "test_data[\"gdp_prediction\"] = y_hat\n",
    "\n",
    "results = test_data[[\"region\", \"gdp_prediction\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>region</th>\n",
       "      <th>gdp_prediction</th>\n",
       "      <th>gdp</th>\n",
       "      <th>change</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Vinnytsia_Oblast</td>\n",
       "      <td>3.291774e+06</td>\n",
       "      <td>4.337287e+06</td>\n",
       "      <td>-0.241052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Volyn_Oblast</td>\n",
       "      <td>2.008938e+06</td>\n",
       "      <td>1.915275e+06</td>\n",
       "      <td>0.048903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dnipropetrovsk_Oblast</td>\n",
       "      <td>2.551748e+06</td>\n",
       "      <td>1.284456e+07</td>\n",
       "      <td>-0.801336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Donetsk_Oblast</td>\n",
       "      <td>3.255840e+06</td>\n",
       "      <td>6.400417e+06</td>\n",
       "      <td>-0.491308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Zhytomyr_Oblast</td>\n",
       "      <td>2.294772e+06</td>\n",
       "      <td>3.007165e+06</td>\n",
       "      <td>-0.236898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Zakarpattia_Oblast</td>\n",
       "      <td>2.112678e+06</td>\n",
       "      <td>2.096846e+06</td>\n",
       "      <td>0.007550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Zaporizhia_Oblast</td>\n",
       "      <td>2.279829e+06</td>\n",
       "      <td>5.481753e+06</td>\n",
       "      <td>-0.584106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Ivano-Frankivsk_Oblast</td>\n",
       "      <td>2.872956e+06</td>\n",
       "      <td>3.312617e+06</td>\n",
       "      <td>-0.132723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Kyiv_Oblast</td>\n",
       "      <td>4.273730e+06</td>\n",
       "      <td>7.395152e+06</td>\n",
       "      <td>-0.422090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Kirovohrad_Oblast</td>\n",
       "      <td>2.275934e+06</td>\n",
       "      <td>2.512884e+06</td>\n",
       "      <td>-0.094294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Luhansk_Oblast</td>\n",
       "      <td>1.779990e+06</td>\n",
       "      <td>1.587944e+06</td>\n",
       "      <td>0.120940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Lviv_Oblast</td>\n",
       "      <td>5.997118e+06</td>\n",
       "      <td>7.122045e+06</td>\n",
       "      <td>-0.157950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Mykolaiv_Oblast</td>\n",
       "      <td>2.156018e+06</td>\n",
       "      <td>3.256755e+06</td>\n",
       "      <td>-0.337986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Odessa_Oblast</td>\n",
       "      <td>4.764006e+06</td>\n",
       "      <td>7.541502e+06</td>\n",
       "      <td>-0.368295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Poltava_Oblast</td>\n",
       "      <td>2.116752e+06</td>\n",
       "      <td>4.591725e+06</td>\n",
       "      <td>-0.539007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Rivne_Oblast</td>\n",
       "      <td>2.089803e+06</td>\n",
       "      <td>2.289795e+06</td>\n",
       "      <td>-0.087340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Sumy_Oblast</td>\n",
       "      <td>2.164331e+06</td>\n",
       "      <td>2.509821e+06</td>\n",
       "      <td>-0.137655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Ternopil_Oblast</td>\n",
       "      <td>1.944809e+06</td>\n",
       "      <td>2.016336e+06</td>\n",
       "      <td>-0.035474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Kharkiv_Oblast</td>\n",
       "      <td>2.198483e+06</td>\n",
       "      <td>7.574767e+06</td>\n",
       "      <td>-0.709762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Kherson_Oblast</td>\n",
       "      <td>2.006044e+06</td>\n",
       "      <td>2.063189e+06</td>\n",
       "      <td>-0.027697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Khmelnytskyi_Oblast</td>\n",
       "      <td>2.576037e+06</td>\n",
       "      <td>2.891974e+06</td>\n",
       "      <td>-0.109246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Cherkasy_Oblast</td>\n",
       "      <td>2.324630e+06</td>\n",
       "      <td>3.300715e+06</td>\n",
       "      <td>-0.295719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Chernivtsi_Oblast</td>\n",
       "      <td>1.437781e+06</td>\n",
       "      <td>1.381072e+06</td>\n",
       "      <td>0.041062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Chernihiv_Oblast</td>\n",
       "      <td>2.263214e+06</td>\n",
       "      <td>2.327018e+06</td>\n",
       "      <td>-0.027419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Kyiv</td>\n",
       "      <td>2.654508e+07</td>\n",
       "      <td>3.163222e+07</td>\n",
       "      <td>-0.160821</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    region  gdp_prediction           gdp    change\n",
       "0         Vinnytsia_Oblast    3.291774e+06  4.337287e+06 -0.241052\n",
       "1             Volyn_Oblast    2.008938e+06  1.915275e+06  0.048903\n",
       "2    Dnipropetrovsk_Oblast    2.551748e+06  1.284456e+07 -0.801336\n",
       "3           Donetsk_Oblast    3.255840e+06  6.400417e+06 -0.491308\n",
       "4          Zhytomyr_Oblast    2.294772e+06  3.007165e+06 -0.236898\n",
       "5       Zakarpattia_Oblast    2.112678e+06  2.096846e+06  0.007550\n",
       "6        Zaporizhia_Oblast    2.279829e+06  5.481753e+06 -0.584106\n",
       "7   Ivano-Frankivsk_Oblast    2.872956e+06  3.312617e+06 -0.132723\n",
       "8              Kyiv_Oblast    4.273730e+06  7.395152e+06 -0.422090\n",
       "9        Kirovohrad_Oblast    2.275934e+06  2.512884e+06 -0.094294\n",
       "10          Luhansk_Oblast    1.779990e+06  1.587944e+06  0.120940\n",
       "11             Lviv_Oblast    5.997118e+06  7.122045e+06 -0.157950\n",
       "12         Mykolaiv_Oblast    2.156018e+06  3.256755e+06 -0.337986\n",
       "13           Odessa_Oblast    4.764006e+06  7.541502e+06 -0.368295\n",
       "14          Poltava_Oblast    2.116752e+06  4.591725e+06 -0.539007\n",
       "15            Rivne_Oblast    2.089803e+06  2.289795e+06 -0.087340\n",
       "16             Sumy_Oblast    2.164331e+06  2.509821e+06 -0.137655\n",
       "17         Ternopil_Oblast    1.944809e+06  2.016336e+06 -0.035474\n",
       "18          Kharkiv_Oblast    2.198483e+06  7.574767e+06 -0.709762\n",
       "19          Kherson_Oblast    2.006044e+06  2.063189e+06 -0.027697\n",
       "20     Khmelnytskyi_Oblast    2.576037e+06  2.891974e+06 -0.109246\n",
       "21         Cherkasy_Oblast    2.324630e+06  3.300715e+06 -0.295719\n",
       "22       Chernivtsi_Oblast    1.437781e+06  1.381072e+06  0.041062\n",
       "23        Chernihiv_Oblast    2.263214e+06  2.327018e+06 -0.027419\n",
       "24                    Kyiv    2.654508e+07  3.163222e+07 -0.160821"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compare results with gdp from the previous year\n",
    "previous_year = pd.read_csv(\"data/tabular_data_ukraine.csv\")\n",
    "previous_year = previous_year[previous_year[\"year\"] == 2021]\n",
    "previous_year = previous_year[[\"region\", \"gdp\"]]\n",
    "\n",
    "# merge on region\n",
    "results = results.merge(previous_year, on=\"region\")\n",
    "\n",
    "# compute change in percentages\n",
    "results[\"change\"] = (results[\"gdp_prediction\"] - results[\"gdp\"]) / results[\"gdp\"]\n",
    "\n",
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.31819984624562825"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(results[\"change\"]) # 23% drop\n",
    "\n",
    "(results[\"gdp_prediction\"].sum() - results[\"gdp\"].sum()) / results[\"gdp\"].sum() # 31.8% drop, World Bank esimates 29.1%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>gdp</th>\n",
       "      <th>nearnad_snow_cov_mean</th>\n",
       "      <th>nearnad_snow_cov_median</th>\n",
       "      <th>nearnad_snow_cov_std</th>\n",
       "      <th>nearnad_snow_free_mean</th>\n",
       "      <th>nearnad_snow_free_median</th>\n",
       "      <th>nearnad_snow_free_std</th>\n",
       "      <th>offnad_snow_cov_mean</th>\n",
       "      <th>offnad_snow_cov_median</th>\n",
       "      <th>...</th>\n",
       "      <th>region_Odessa_Oblast</th>\n",
       "      <th>region_Poltava_Oblast</th>\n",
       "      <th>region_Rivne_Oblast</th>\n",
       "      <th>region_Sumy_Oblast</th>\n",
       "      <th>region_Ternopil_Oblast</th>\n",
       "      <th>region_Vinnytsia_Oblast</th>\n",
       "      <th>region_Volyn_Oblast</th>\n",
       "      <th>region_Zakarpattia_Oblast</th>\n",
       "      <th>region_Zaporizhia_Oblast</th>\n",
       "      <th>region_Zhytomyr_Oblast</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>2.647959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>46.242333</td>\n",
       "      <td>1.184331</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43.000168</td>\n",
       "      <td>2.546648</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2013</td>\n",
       "      <td>99.300000</td>\n",
       "      <td>2.343649</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.583948</td>\n",
       "      <td>0.961109</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.325981</td>\n",
       "      <td>1.299532</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>2014</td>\n",
       "      <td>94.434300</td>\n",
       "      <td>3.100126</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.716915</td>\n",
       "      <td>0.974721</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.166740</td>\n",
       "      <td>2.828148</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>2015</td>\n",
       "      <td>85.274173</td>\n",
       "      <td>2.467506</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.238729</td>\n",
       "      <td>0.930540</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.208755</td>\n",
       "      <td>2.504813</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>2016</td>\n",
       "      <td>83.909786</td>\n",
       "      <td>3.240427</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.904961</td>\n",
       "      <td>1.048079</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.651880</td>\n",
       "      <td>1.986214</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>2017</td>\n",
       "      <td>85.587982</td>\n",
       "      <td>2.544386</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.548876</td>\n",
       "      <td>1.243090</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52.883580</td>\n",
       "      <td>2.503581</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>2018</td>\n",
       "      <td>87.727681</td>\n",
       "      <td>3.188704</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55.825475</td>\n",
       "      <td>1.250778</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.561350</td>\n",
       "      <td>2.398645</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>2019</td>\n",
       "      <td>90.973606</td>\n",
       "      <td>3.057841</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.197801</td>\n",
       "      <td>1.254185</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.082681</td>\n",
       "      <td>2.261230</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>2020</td>\n",
       "      <td>84.605453</td>\n",
       "      <td>3.204058</td>\n",
       "      <td>0.0</td>\n",
       "      <td>67.320126</td>\n",
       "      <td>1.225739</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.918232</td>\n",
       "      <td>1.514519</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>2021</td>\n",
       "      <td>86.805195</td>\n",
       "      <td>4.045430</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.211584</td>\n",
       "      <td>1.338012</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.280639</td>\n",
       "      <td>3.359127</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>2022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.532210</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.691905</td>\n",
       "      <td>0.067109</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.584661</td>\n",
       "      <td>0.433461</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>2023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.711638</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.289871</td>\n",
       "      <td>0.193923</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.717158</td>\n",
       "      <td>0.232041</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     year         gdp  nearnad_snow_cov_mean  nearnad_snow_cov_median  \\\n",
       "2    2012  100.000000               2.647959                      0.0   \n",
       "27   2013   99.300000               2.343649                      0.0   \n",
       "52   2014   94.434300               3.100126                      0.0   \n",
       "77   2015   85.274173               2.467506                      0.0   \n",
       "102  2016   83.909786               3.240427                      0.0   \n",
       "127  2017   85.587982               2.544386                      0.0   \n",
       "152  2018   87.727681               3.188704                      0.0   \n",
       "177  2019   90.973606               3.057841                      0.0   \n",
       "202  2020   84.605453               3.204058                      0.0   \n",
       "227  2021   86.805195               4.045430                      0.0   \n",
       "252  2022         NaN               1.532210                      0.0   \n",
       "277  2023         NaN               0.711638                      0.0   \n",
       "\n",
       "     nearnad_snow_cov_std  nearnad_snow_free_mean  nearnad_snow_free_median  \\\n",
       "2               46.242333                1.184331                       0.0   \n",
       "27              33.583948                0.961109                       0.0   \n",
       "52              40.716915                0.974721                       0.0   \n",
       "77              50.238729                0.930540                       0.0   \n",
       "102             31.904961                1.048079                       0.0   \n",
       "127             39.548876                1.243090                       0.0   \n",
       "152             55.825475                1.250778                       0.0   \n",
       "177             51.197801                1.254185                       0.0   \n",
       "202             67.320126                1.225739                       0.0   \n",
       "227             45.211584                1.338012                       0.0   \n",
       "252             30.691905                0.067109                       0.0   \n",
       "277             13.289871                0.193923                       0.0   \n",
       "\n",
       "     nearnad_snow_free_std  offnad_snow_cov_mean  offnad_snow_cov_median  ...  \\\n",
       "2                43.000168              2.546648                     0.0  ...   \n",
       "27               30.325981              1.299532                     0.0  ...   \n",
       "52               25.166740              2.828148                     0.0  ...   \n",
       "77               26.208755              2.504813                     0.0  ...   \n",
       "102              41.651880              1.986214                     0.0  ...   \n",
       "127              52.883580              2.503581                     0.0  ...   \n",
       "152              50.561350              2.398645                     0.0  ...   \n",
       "177              41.082681              2.261230                     0.0  ...   \n",
       "202              30.918232              1.514519                     0.0  ...   \n",
       "227              31.280639              3.359127                     0.0  ...   \n",
       "252               3.584661              0.433461                     0.0  ...   \n",
       "277              13.717158              0.232041                     0.0  ...   \n",
       "\n",
       "     region_Odessa_Oblast  region_Poltava_Oblast  region_Rivne_Oblast  \\\n",
       "2                   False                  False                False   \n",
       "27                  False                  False                False   \n",
       "52                  False                  False                False   \n",
       "77                  False                  False                False   \n",
       "102                 False                  False                False   \n",
       "127                 False                  False                False   \n",
       "152                 False                  False                False   \n",
       "177                 False                  False                False   \n",
       "202                 False                  False                False   \n",
       "227                 False                  False                False   \n",
       "252                 False                  False                False   \n",
       "277                 False                  False                False   \n",
       "\n",
       "     region_Sumy_Oblast  region_Ternopil_Oblast  region_Vinnytsia_Oblast  \\\n",
       "2                 False                   False                    False   \n",
       "27                False                   False                    False   \n",
       "52                False                   False                    False   \n",
       "77                False                   False                    False   \n",
       "102               False                   False                    False   \n",
       "127               False                   False                    False   \n",
       "152               False                   False                    False   \n",
       "177               False                   False                    False   \n",
       "202               False                   False                    False   \n",
       "227               False                   False                    False   \n",
       "252               False                   False                    False   \n",
       "277               False                   False                    False   \n",
       "\n",
       "     region_Volyn_Oblast  region_Zakarpattia_Oblast  region_Zaporizhia_Oblast  \\\n",
       "2                  False                      False                     False   \n",
       "27                 False                      False                     False   \n",
       "52                 False                      False                     False   \n",
       "77                 False                      False                     False   \n",
       "102                False                      False                     False   \n",
       "127                False                      False                     False   \n",
       "152                False                      False                     False   \n",
       "177                False                      False                     False   \n",
       "202                False                      False                     False   \n",
       "227                False                      False                     False   \n",
       "252                False                      False                     False   \n",
       "277                False                      False                     False   \n",
       "\n",
       "     region_Zhytomyr_Oblast  \n",
       "2                     False  \n",
       "27                    False  \n",
       "52                    False  \n",
       "77                    False  \n",
       "102                   False  \n",
       "127                   False  \n",
       "152                   False  \n",
       "177                   False  \n",
       "202                   False  \n",
       "227                   False  \n",
       "252                   False  \n",
       "277                   False  \n",
       "\n",
       "[12 rows x 45 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select region_Dnipropetrovsk_Oblast == 1\n",
    "subset = data[data[\"region_Dnipropetrovsk_Oblast\"] == 1]\n",
    "subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>gdp</th>\n",
       "      <th>nearnad_snow_cov_mean</th>\n",
       "      <th>nearnad_snow_cov_median</th>\n",
       "      <th>nearnad_snow_cov_std</th>\n",
       "      <th>nearnad_snow_free_mean</th>\n",
       "      <th>nearnad_snow_free_median</th>\n",
       "      <th>nearnad_snow_free_std</th>\n",
       "      <th>offnad_snow_cov_mean</th>\n",
       "      <th>offnad_snow_cov_median</th>\n",
       "      <th>...</th>\n",
       "      <th>region_Odessa_Oblast</th>\n",
       "      <th>region_Poltava_Oblast</th>\n",
       "      <th>region_Rivne_Oblast</th>\n",
       "      <th>region_Sumy_Oblast</th>\n",
       "      <th>region_Ternopil_Oblast</th>\n",
       "      <th>region_Vinnytsia_Oblast</th>\n",
       "      <th>region_Volyn_Oblast</th>\n",
       "      <th>region_Zakarpattia_Oblast</th>\n",
       "      <th>region_Zaporizhia_Oblast</th>\n",
       "      <th>region_Zhytomyr_Oblast</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.945420</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.503158</td>\n",
       "      <td>0.301091</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.865808</td>\n",
       "      <td>1.307311</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.767207</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.420684</td>\n",
       "      <td>0.102880</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.273735</td>\n",
       "      <td>0.682606</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>2.647959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>46.242333</td>\n",
       "      <td>1.184331</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43.000168</td>\n",
       "      <td>2.546648</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>3.566429</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.045082</td>\n",
       "      <td>1.317057</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.056379</td>\n",
       "      <td>3.413734</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.717199</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.332430</td>\n",
       "      <td>0.201979</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.190241</td>\n",
       "      <td>1.358591</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>2021</td>\n",
       "      <td>110.225016</td>\n",
       "      <td>0.622373</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.658013</td>\n",
       "      <td>0.196125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.930659</td>\n",
       "      <td>0.655630</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>2021</td>\n",
       "      <td>105.572198</td>\n",
       "      <td>1.308531</td>\n",
       "      <td>0.0</td>\n",
       "      <td>73.238358</td>\n",
       "      <td>0.349940</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.114947</td>\n",
       "      <td>1.509102</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>2021</td>\n",
       "      <td>104.896846</td>\n",
       "      <td>0.363287</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.754161</td>\n",
       "      <td>0.155258</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.675048</td>\n",
       "      <td>0.342800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>2021</td>\n",
       "      <td>97.226457</td>\n",
       "      <td>0.404712</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.745696</td>\n",
       "      <td>0.107963</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.970843</td>\n",
       "      <td>0.414831</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>2021</td>\n",
       "      <td>114.740464</td>\n",
       "      <td>2.208380</td>\n",
       "      <td>0.0</td>\n",
       "      <td>64.008319</td>\n",
       "      <td>0.955585</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.319962</td>\n",
       "      <td>2.248791</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     year         gdp  nearnad_snow_cov_mean  nearnad_snow_cov_median  \\\n",
       "0    2012  100.000000               0.945420                      0.0   \n",
       "1    2012  100.000000               0.767207                      0.0   \n",
       "2    2012  100.000000               2.647959                      0.0   \n",
       "3    2012  100.000000               3.566429                      0.0   \n",
       "4    2012  100.000000               0.717199                      0.0   \n",
       "..    ...         ...                    ...                      ...   \n",
       "245  2021  110.225016               0.622373                      0.0   \n",
       "246  2021  105.572198               1.308531                      0.0   \n",
       "247  2021  104.896846               0.363287                      0.0   \n",
       "248  2021   97.226457               0.404712                      0.0   \n",
       "249  2021  114.740464               2.208380                      0.0   \n",
       "\n",
       "     nearnad_snow_cov_std  nearnad_snow_free_mean  nearnad_snow_free_median  \\\n",
       "0               13.503158                0.301091                       0.0   \n",
       "1                6.420684                0.102880                       0.0   \n",
       "2               46.242333                1.184331                       0.0   \n",
       "3               39.045082                1.317057                       0.0   \n",
       "4               11.332430                0.201979                       0.0   \n",
       "..                    ...                     ...                       ...   \n",
       "245              8.658013                0.196125                       0.0   \n",
       "246             73.238358                0.349940                       0.0   \n",
       "247              4.754161                0.155258                       0.0   \n",
       "248              7.745696                0.107963                       0.0   \n",
       "249             64.008319                0.955585                       0.0   \n",
       "\n",
       "     nearnad_snow_free_std  offnad_snow_cov_mean  offnad_snow_cov_median  ...  \\\n",
       "0                 3.865808              1.307311                     0.0  ...   \n",
       "1                 2.273735              0.682606                     0.0  ...   \n",
       "2                43.000168              2.546648                     0.0  ...   \n",
       "3                14.056379              3.413734                     0.0  ...   \n",
       "4                 3.190241              1.358591                     0.0  ...   \n",
       "..                     ...                   ...                     ...  ...   \n",
       "245               3.930659              0.655630                     0.0  ...   \n",
       "246              38.114947              1.509102                     0.0  ...   \n",
       "247               3.675048              0.342800                     0.0  ...   \n",
       "248               3.970843              0.414831                     0.0  ...   \n",
       "249              18.319962              2.248791                     0.0  ...   \n",
       "\n",
       "     region_Odessa_Oblast  region_Poltava_Oblast  region_Rivne_Oblast  \\\n",
       "0                   False                  False                False   \n",
       "1                   False                  False                False   \n",
       "2                   False                  False                False   \n",
       "3                   False                  False                False   \n",
       "4                   False                  False                False   \n",
       "..                    ...                    ...                  ...   \n",
       "245                 False                  False                False   \n",
       "246                 False                  False                False   \n",
       "247                 False                  False                False   \n",
       "248                 False                  False                False   \n",
       "249                 False                  False                False   \n",
       "\n",
       "     region_Sumy_Oblast  region_Ternopil_Oblast  region_Vinnytsia_Oblast  \\\n",
       "0                 False                   False                     True   \n",
       "1                 False                   False                    False   \n",
       "2                 False                   False                    False   \n",
       "3                 False                   False                    False   \n",
       "4                 False                   False                    False   \n",
       "..                  ...                     ...                      ...   \n",
       "245               False                   False                    False   \n",
       "246               False                   False                    False   \n",
       "247               False                   False                    False   \n",
       "248               False                   False                    False   \n",
       "249               False                   False                    False   \n",
       "\n",
       "     region_Volyn_Oblast  region_Zakarpattia_Oblast  region_Zaporizhia_Oblast  \\\n",
       "0                  False                      False                     False   \n",
       "1                   True                      False                     False   \n",
       "2                  False                      False                     False   \n",
       "3                  False                      False                     False   \n",
       "4                  False                      False                     False   \n",
       "..                   ...                        ...                       ...   \n",
       "245                False                      False                     False   \n",
       "246                False                      False                     False   \n",
       "247                False                      False                     False   \n",
       "248                False                      False                     False   \n",
       "249                False                      False                     False   \n",
       "\n",
       "     region_Zhytomyr_Oblast  \n",
       "0                     False  \n",
       "1                     False  \n",
       "2                     False  \n",
       "3                     False  \n",
       "4                      True  \n",
       "..                      ...  \n",
       "245                   False  \n",
       "246                   False  \n",
       "247                   False  \n",
       "248                   False  \n",
       "249                   False  \n",
       "\n",
       "[250 rows x 45 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
