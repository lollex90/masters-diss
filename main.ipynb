{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jakub\\anaconda\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\jakub\\anaconda\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import rasterio\n",
    "from rasterio.mask import mask\n",
    "from rasterio.transform import from_origin\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd \n",
    "import pandas as pd\n",
    "from dnb_annual import *\n",
    "from variables import years, composites, ukr_region_map, pol_region_map\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Resizing, Dropout, BatchNormalization, Activation, Add, GlobalAveragePooling2D, Input, Reshape, Conv2DTranspose, Cropping2D\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Resizing, Dropout, BatchNormalization, Activation, Add, GlobalAveragePooling2D, Input, Reshape, Conv2DTranspose, Cropping2D\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_polygons = gpd.read_file('poland_boundaries.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_polygons\n",
    "\n",
    "# change names according to pol_region_map\n",
    "country_polygons['name'] = country_polygons['name'].apply(lambda x: pol_region_map[x])\n",
    "\n",
    "# rename name to shapeName\n",
    "country_polygons.rename(columns={'name': 'shapeName'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>shapeName</th>\n",
       "      <th>density</th>\n",
       "      <th>path</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5307</td>\n",
       "      <td>Dolnoslaskie</td>\n",
       "      <td>0</td>\n",
       "      <td>/world/Poland/Dolnośląskie</td>\n",
       "      <td>MULTIPOLYGON (((16.16883 51.66089, 16.19236 51...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5304</td>\n",
       "      <td>Kujawsko-Pomorskie</td>\n",
       "      <td>0</td>\n",
       "      <td>/world/Poland/Kujawsko-Pomorskie</td>\n",
       "      <td>MULTIPOLYGON (((18.25566 53.69423, 18.27436 53...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5315</td>\n",
       "      <td>Lodzkie</td>\n",
       "      <td>0</td>\n",
       "      <td>/world/Poland/Łódzkie</td>\n",
       "      <td>MULTIPOLYGON (((19.34231 52.30918, 19.36254 52...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5305</td>\n",
       "      <td>Lubelskie</td>\n",
       "      <td>0</td>\n",
       "      <td>/world/Poland/Lubelskie</td>\n",
       "      <td>MULTIPOLYGON (((23.19990 52.29715, 23.19235 52...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5308</td>\n",
       "      <td>Lubuskie</td>\n",
       "      <td>0</td>\n",
       "      <td>/world/Poland/Lubuskie</td>\n",
       "      <td>MULTIPOLYGON (((16.02720 53.10739, 16.03948 53...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5306</td>\n",
       "      <td>Malopolskie</td>\n",
       "      <td>0</td>\n",
       "      <td>/world/Poland/Małopolskie</td>\n",
       "      <td>MULTIPOLYGON (((20.15906 50.43463, 20.17673 50...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5309</td>\n",
       "      <td>Mazowieckie</td>\n",
       "      <td>0</td>\n",
       "      <td>/world/Poland/Mazowieckie</td>\n",
       "      <td>MULTIPOLYGON (((21.56137 53.37360, 21.56842 53...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5310</td>\n",
       "      <td>Opolskie</td>\n",
       "      <td>0</td>\n",
       "      <td>/world/Poland/Opolskie</td>\n",
       "      <td>MULTIPOLYGON (((17.79365 51.13264, 17.79802 51...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5313</td>\n",
       "      <td>Podkarpackie</td>\n",
       "      <td>0</td>\n",
       "      <td>/world/Poland/Podkarpackie</td>\n",
       "      <td>MULTIPOLYGON (((22.00627 50.76723, 22.03055 50...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5311</td>\n",
       "      <td>Podlaskie</td>\n",
       "      <td>0</td>\n",
       "      <td>/world/Poland/Podlaskie</td>\n",
       "      <td>MULTIPOLYGON (((22.88482 54.42379, 22.88450 54...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5312</td>\n",
       "      <td>Pomorskie</td>\n",
       "      <td>0</td>\n",
       "      <td>/world/Poland/Pomorskie</td>\n",
       "      <td>MULTIPOLYGON (((18.79694 54.36570, 18.79694 54...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5316</td>\n",
       "      <td>Slaskie</td>\n",
       "      <td>0</td>\n",
       "      <td>/world/Poland/Śląskie</td>\n",
       "      <td>MULTIPOLYGON (((18.97734 51.01314, 19.00442 51...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5302</td>\n",
       "      <td>Swietokrzyskie</td>\n",
       "      <td>0</td>\n",
       "      <td>/world/Poland/Świętokrzyskie</td>\n",
       "      <td>MULTIPOLYGON (((20.70706 51.14639, 20.71060 51...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5317</td>\n",
       "      <td>Warminsko-Mazurskie</td>\n",
       "      <td>0</td>\n",
       "      <td>/world/Poland/Warmińsko-Mazurskie</td>\n",
       "      <td>MULTIPOLYGON (((19.80316 54.43536, 19.86333 54...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5303</td>\n",
       "      <td>Wielkopolskie</td>\n",
       "      <td>0</td>\n",
       "      <td>/world/Poland/Wielkopolskie</td>\n",
       "      <td>MULTIPOLYGON (((16.92182 53.59652, 16.94772 53...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5314</td>\n",
       "      <td>Zachodniopomorskie</td>\n",
       "      <td>0</td>\n",
       "      <td>/world/Poland/Zachodniopomorskie</td>\n",
       "      <td>MULTIPOLYGON (((14.59639 53.63792, 14.59639 53...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id            shapeName  density                               path  \\\n",
       "0   5307         Dolnoslaskie        0         /world/Poland/Dolnośląskie   \n",
       "1   5304   Kujawsko-Pomorskie        0   /world/Poland/Kujawsko-Pomorskie   \n",
       "2   5315              Lodzkie        0              /world/Poland/Łódzkie   \n",
       "3   5305            Lubelskie        0            /world/Poland/Lubelskie   \n",
       "4   5308             Lubuskie        0             /world/Poland/Lubuskie   \n",
       "5   5306          Malopolskie        0          /world/Poland/Małopolskie   \n",
       "6   5309          Mazowieckie        0          /world/Poland/Mazowieckie   \n",
       "7   5310             Opolskie        0             /world/Poland/Opolskie   \n",
       "8   5313         Podkarpackie        0         /world/Poland/Podkarpackie   \n",
       "9   5311            Podlaskie        0            /world/Poland/Podlaskie   \n",
       "10  5312            Pomorskie        0            /world/Poland/Pomorskie   \n",
       "11  5316              Slaskie        0              /world/Poland/Śląskie   \n",
       "12  5302       Swietokrzyskie        0       /world/Poland/Świętokrzyskie   \n",
       "13  5317  Warminsko-Mazurskie        0  /world/Poland/Warmińsko-Mazurskie   \n",
       "14  5303        Wielkopolskie        0        /world/Poland/Wielkopolskie   \n",
       "15  5314   Zachodniopomorskie        0   /world/Poland/Zachodniopomorskie   \n",
       "\n",
       "                                             geometry  \n",
       "0   MULTIPOLYGON (((16.16883 51.66089, 16.19236 51...  \n",
       "1   MULTIPOLYGON (((18.25566 53.69423, 18.27436 53...  \n",
       "2   MULTIPOLYGON (((19.34231 52.30918, 19.36254 52...  \n",
       "3   MULTIPOLYGON (((23.19990 52.29715, 23.19235 52...  \n",
       "4   MULTIPOLYGON (((16.02720 53.10739, 16.03948 53...  \n",
       "5   MULTIPOLYGON (((20.15906 50.43463, 20.17673 50...  \n",
       "6   MULTIPOLYGON (((21.56137 53.37360, 21.56842 53...  \n",
       "7   MULTIPOLYGON (((17.79365 51.13264, 17.79802 51...  \n",
       "8   MULTIPOLYGON (((22.00627 50.76723, 22.03055 50...  \n",
       "9   MULTIPOLYGON (((22.88482 54.42379, 22.88450 54...  \n",
       "10  MULTIPOLYGON (((18.79694 54.36570, 18.79694 54...  \n",
       "11  MULTIPOLYGON (((18.97734 51.01314, 19.00442 51...  \n",
       "12  MULTIPOLYGON (((20.70706 51.14639, 20.71060 51...  \n",
       "13  MULTIPOLYGON (((19.80316 54.43536, 19.86333 54...  \n",
       "14  MULTIPOLYGON (((16.92182 53.59652, 16.94772 53...  \n",
       "15  MULTIPOLYGON (((14.59639 53.63792, 14.59639 53...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "country_polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this script is used only once to generate the regional images for each year\n",
    "# Ukraine\n",
    "ukr_polygons = gpd.read_file(\"geoBoundaries-UKR-ADM1.geojson\")\n",
    "# source: https://cartographyvectors.com/\n",
    "years = [2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023]\n",
    "for year in years:\n",
    "    dnb = dnb_annual(year, composites, ukr_polygons, \"ukr\")\n",
    "    dnb.load_all_data()\n",
    "    dnb.filter_data()\n",
    "    dnb.save_rasters()\n",
    "    dnb.load_rasters()\n",
    "    dnb.build_regional_images()\n",
    "    dnb.add_padding()\n",
    "    dnb.save_regional_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_pol_polygons(pol_polygons):\n",
    "\n",
    "    pol_polygons['name'] = pol_polygons['name'].apply(lambda x: pol_region_map[x])\n",
    "    pol_polygons.rename(columns={'name': 'shapeName'}, inplace=True)\n",
    "\n",
    "    return pol_polygons\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Poland\n",
    "pol_polygons = gpd.read_file(\"poland_boundaries.geojson\")\n",
    "pol_polygons = clean_pol_polygons(pol_polygons)\n",
    "years = [2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023]\n",
    "\n",
    "for year in years:\n",
    "    dnb = dnb_annual(year, composites, pol_polygons, \"pol\")\n",
    "    dnb.load_all_data()\n",
    "    dnb.filter_data()\n",
    "    dnb.save_rasters()\n",
    "    dnb.load_rasters()\n",
    "    dnb.build_regional_images()\n",
    "    dnb.add_padding()\n",
    "    dnb.save_regional_images()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this script is used to clean gdp data\n",
    "\n",
    "# Inflation data\n",
    "# inflation = pd.read_excel(\"data/isc_reg.xls\", skiprows=2, header=1)\n",
    "# inflation = inflation.drop(columns=inflation.columns[0])\n",
    "# inflation = inflation.rename(columns={inflation.columns[-1]: \"region\"})\n",
    "# inflation = inflation[~inflation[\"region\"].isin([\"Ukraine\", \"oblasts\"])]\n",
    "# inflation = inflation.dropna()\n",
    "# inflation[\"region\"] = inflation[\"region\"].map(region_map)\n",
    "# inflation.columns = inflation.columns.astype(str)\n",
    "# inflation = inflation.melt(id_vars=\"region\", var_name=\"year\", value_name=\"inflation\")\n",
    "# inflation.to_csv(\"data/inflation.csv\", index=False)\n",
    "\n",
    "# GDP data\n",
    "# gdp = pd.read_excel(\"data/ukr_reg_gdp.xls\", skiprows=3, header=1)\n",
    "# gdp = gdp.drop(columns=gdp.columns[0])\n",
    "# gdp = gdp.iloc[:, np.r_[18:36, -1]]\n",
    "# gdp = gdp.rename(columns={gdp.columns[-1]: \"region\"})\n",
    "# gdp = gdp[~gdp[\"region\"].isin([\"Ukrane\", \"oblasts\"])]\n",
    "# gdp = gdp.dropna()\n",
    "# gdp[\"region\"] = gdp[\"region\"].map(region_map)\n",
    "# gdp[\"region\"] = gdp[\"region\"].fillna(\"Sevastopol\")\n",
    "# gdp.columns = gdp.columns.astype(str)\n",
    "# gdp = gdp.rename(columns={gdp.columns[i]: gdp.columns[i][:4] for i in range(18)})\n",
    "# gdp = gdp.melt(id_vars=\"region\", var_name=\"year\", value_name=\"real_gdp_change\")\n",
    "\n",
    "# # include only years from 2012 inclusive, exclude Sevastopol and the Autonomous Republic of Crimea\n",
    "# gdp = gdp[gdp[\"year\"].astype(int) >= 2012]\n",
    "# gdp = gdp[~gdp[\"region\"].isin([\"Sevastopol\", \"Autonomous Republic of Crimea\"])]\n",
    "\n",
    "# # set the value for the starting year to 100 (2012), NaN for the rest\n",
    "# gdp.loc[gdp[\"year\"] == \"2012\", \"real_gdp\"] = 100\n",
    "# gdp = gdp.sort_values(by=[\"region\", \"year\"])\n",
    "# gdp[\"real_gdp_change\"] = gdp[\"real_gdp_change\"] / 100\n",
    "\n",
    "# # reste the index\n",
    "# gdp = gdp.reset_index(drop=True)\n",
    "\n",
    "# # # calculate the real gdp\n",
    "# for i in range(1, gdp.shape[0]):\n",
    "\n",
    "#     # skip if the year is 2012\n",
    "#     if gdp.loc[gdp.index[i], \"year\"] == \"2012\":\n",
    "#         continue\n",
    "#     else:\n",
    "#         gdp.loc[gdp.index[i], \"real_gdp\"] = gdp.loc[gdp.index[i-1], \"real_gdp\"] * (gdp.loc[gdp.index[i], \"real_gdp_change\"])\n",
    "\n",
    "# # delete the real_gdp_change column\n",
    "# gdp = gdp.drop(columns=\"real_gdp_change\")\n",
    "\n",
    "# # get the nominal gdp\n",
    "# gdp_nominal = pd.read_excel(\"data/ukr_reg_gdp.xls\", skiprows=3, header=1)\n",
    "# gdp_nominal = gdp_nominal.iloc[:, np.r_[9, -1]]\n",
    "# gdp_nominal.columns = [\"gdp_nominal\", \"region\"]\n",
    "# gdp_nominal = gdp_nominal[~gdp_nominal[\"region\"].isin([\"Ukrane\", \"oblasts\"])]\n",
    "# gdp_nominal = gdp_nominal.dropna()\n",
    "# gdp_nominal[\"region\"] = gdp_nominal[\"region\"].map(region_map)\n",
    "# gdp_nominal[\"region\"] = gdp_nominal[\"region\"].fillna(\"Sevastopol\")\n",
    "\n",
    "# # merge nominal gdp to real gdp by region\n",
    "# gdp = gdp.merge(gdp_nominal, on=\"region\")\n",
    "\n",
    "# # multiple the real gdp by the nominal gdp\n",
    "# gdp[\"real_gdp\"] = gdp[\"real_gdp\"] * gdp[\"gdp_nominal\"]\n",
    "\n",
    "# # drop the nominal gdp column\n",
    "# gdp = gdp.drop(columns=\"gdp_nominal\")\n",
    "\n",
    "# # for the region column, change all spaces to _\n",
    "# gdp[\"region\"] = gdp[\"region\"].str.replace(\" \", \"_\")\n",
    "\n",
    "# # save the data\n",
    "# gdp.to_csv(\"data/clean_ukr_gdp.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare gdp data, Ukraine\n",
    "gdp_ukr = pd.read_excel(\"data/ukr_reg_gdp.xls\", skiprows=3, header=1)\n",
    "gdp_ukr = gdp_ukr.drop(columns=gdp_ukr.columns[0])\n",
    "gdp_ukr = gdp_ukr.iloc[:, np.r_[18:36, -1]]\n",
    "gdp_ukr = gdp_ukr.rename(columns={gdp_ukr.columns[-1]: \"region\"})\n",
    "gdp_ukr = gdp_ukr[~gdp_ukr[\"region\"].isin([\"Ukrane\", \"oblasts\"])]\n",
    "gdp_ukr = gdp_ukr.dropna()\n",
    "gdp_ukr[\"region\"] = gdp_ukr[\"region\"].map(region_map)\n",
    "gdp_ukr[\"region\"] = gdp_ukr[\"region\"].fillna(\"Sevastopol\")\n",
    "gdp_ukr.columns = gdp_ukr.columns.astype(str)\n",
    "gdp_ukr = gdp_ukr.rename(columns={gdp_ukr.columns[i]: gdp_ukr.columns[i][:4] for i in range(18)})\n",
    "gdp_ukr = gdp_ukr[~gdp_ukr[\"region\"].isin([\"Sevastopol\", \"Autonomous Republic of Crimea\"])]\n",
    "\n",
    "# gdp_ukr_nominal = pd.read_excel(\"data/ukr_reg_gdp.xls\", skiprows=3, header=1)\n",
    "# gdp_ukr = gdp_ukr.drop(columns=gdp_ukr.columns[0])\n",
    "# gdp_ukr = gdp_ukr.iloc[:, np.r_[18:36, -1]]\n",
    "\n",
    "# select years from 2012 inclusive and the region column\n",
    "gdp_ukr = gdp_ukr[[gdp_ukr.columns[i] for i in range(18) if int(gdp_ukr.columns[i]) >= 2012] + [\"region\"]]\n",
    "\n",
    "# Poland\n",
    "gdp_pol = pd.read_excel(\"data/pol_reg_gdp.xlsx\", header=1, sheet_name=1)\n",
    "gdp_pol = gdp_pol.drop(columns=gdp_pol.columns[0])\n",
    "gdp_pol = gdp_pol.drop(0)\n",
    "gdp_pol = gdp_pol.rename(columns={gdp_pol.columns[0]: \"region\"})\n",
    "# gdp_pol = gdp_pol.drop(columns=\"2022\")\n",
    "\n",
    "def clean_gdp_data(data, country):\n",
    "\n",
    "    # set the 2012 first column to 100\n",
    "    data[\"2012\"] = 100\n",
    "\n",
    "    # divide columns from 2013 to 2021 by 100\n",
    "    for year in range(2013, 2022):\n",
    "        data[str(year)] = data[str(year)] / 100\n",
    "\n",
    "    # calculate the real gdp\n",
    "    for i in range(2013, 2022):\n",
    "        data[str(i)] = data[str(i)] * data[str(i-1)]\n",
    "\n",
    "    # add a column 2022 and 2023 with NaN\n",
    "    if country == \"ukr\":\n",
    "        data[\"2022\"] = np.nan\n",
    "        data[\"2023\"] = np.nan\n",
    "\n",
    "    # format to long\n",
    "    data = data.melt(id_vars=\"region\", var_name=\"year\", value_name=\"real_gdp\")\n",
    "    data[\"region\"] = data[\"region\"].str.replace(\" \", \"_\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "gdp_pol = clean_gdp_data(gdp_pol, country = \"pol\")\n",
    "gdp_ukr = clean_gdp_data(gdp_ukr, country = \"ukr\")\n",
    "\n",
    "# read nominal gdp data for Ukraine\n",
    "gdp_ukr_nominal = pd.read_excel(\"data/ukr_reg_gdp.xls\", skiprows=3, header=1)\n",
    "gdp_ukr_nominal = gdp_ukr_nominal.drop(columns=gdp_ukr_nominal.columns[0])\n",
    "gdp_ukr_nominal = gdp_ukr_nominal.iloc[:, np.r_[8, -1]]\n",
    "\n",
    "gdp_ukr_nominal = gdp_ukr_nominal.rename(columns={gdp_ukr_nominal.columns[-1]: \"region\"})\n",
    "gdp_ukr_nominal = gdp_ukr_nominal[~gdp_ukr_nominal[\"region\"].isin([\"Ukrane\", \"oblasts\"])]\n",
    "gdp_ukr_nominal = gdp_ukr_nominal.dropna()\n",
    "gdp_ukr_nominal[\"region\"] = gdp_ukr_nominal[\"region\"].map(region_map)\n",
    "gdp_ukr_nominal[\"region\"] = gdp_ukr_nominal[\"region\"].fillna(\"Sevastopol\")\n",
    "gdp_ukr_nominal.columns = gdp_ukr_nominal.columns.astype(str)\n",
    "gdp_ukr_nominal = gdp_ukr_nominal[~gdp_ukr_nominal[\"region\"].isin([\"Sevastopol\", \"Autonomous Republic of Crimea\"])]\n",
    "gdp_ukr_nominal[\"region\"] = gdp_ukr_nominal[\"region\"].str.replace(\" \", \"_\")\n",
    "\n",
    "# merge nominal gdp to real gdp by region\n",
    "gdp_ukr = gdp_ukr.merge(gdp_ukr_nominal, on=\"region\")\n",
    "\n",
    "# multiple the real gdp by the nominal gdp\n",
    "gdp_ukr[\"real_gdp\"] = gdp_ukr[\"real_gdp\"] * gdp_ukr[\"2012\"]\n",
    "\n",
    "# drop the nominal gdp column\n",
    "gdp_ukr = gdp_ukr.drop(columns=\"2012\")\n",
    "\n",
    "# save the data\n",
    "gdp_pol.to_csv(\"data/clean_pol_gdp.csv\", index=False)\n",
    "gdp_ukr.to_csv(\"data/clean_ukr_gdp.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>region</th>\n",
       "      <th>year</th>\n",
       "      <th>real_gdp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Vinnytsia_Oblast</td>\n",
       "      <td>2012</td>\n",
       "      <td>3302400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Volyn_Oblast</td>\n",
       "      <td>2012</td>\n",
       "      <td>2000500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dnipropetrovsk_Oblast</td>\n",
       "      <td>2012</td>\n",
       "      <td>14797000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Donetsk_Oblast</td>\n",
       "      <td>2012</td>\n",
       "      <td>17077500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Zhytomyr_Oblast</td>\n",
       "      <td>2012</td>\n",
       "      <td>2484900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>Khmelnytskyi_Oblast</td>\n",
       "      <td>2023</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>Cherkasy_Oblast</td>\n",
       "      <td>2023</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>Chernivtsi_Oblast</td>\n",
       "      <td>2023</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>Chernihiv_Oblast</td>\n",
       "      <td>2023</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>Kyiv</td>\n",
       "      <td>2023</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    region  year    real_gdp\n",
       "0         Vinnytsia_Oblast  2012   3302400.0\n",
       "1             Volyn_Oblast  2012   2000500.0\n",
       "2    Dnipropetrovsk_Oblast  2012  14797000.0\n",
       "3           Donetsk_Oblast  2012  17077500.0\n",
       "4          Zhytomyr_Oblast  2012   2484900.0\n",
       "..                     ...   ...         ...\n",
       "295    Khmelnytskyi_Oblast  2023         NaN\n",
       "296        Cherkasy_Oblast  2023         NaN\n",
       "297      Chernivtsi_Oblast  2023         NaN\n",
       "298       Chernihiv_Oblast  2023         NaN\n",
       "299                   Kyiv  2023         NaN\n",
       "\n",
       "[300 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdp_ukr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2012</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33024.0</td>\n",
       "      <td>Vinnytsia_Oblast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20005.0</td>\n",
       "      <td>Volyn_Oblast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>147970.0</td>\n",
       "      <td>Dnipropetrovsk_Oblast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>170775.0</td>\n",
       "      <td>Donetsk_Oblast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>24849.0</td>\n",
       "      <td>Zhytomyr_Oblast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>21404.0</td>\n",
       "      <td>Zakarpattia_Oblast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>54828.0</td>\n",
       "      <td>Zaporizhia_Oblast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>32286.0</td>\n",
       "      <td>Ivano-Frankivsk_Oblast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>69663.0</td>\n",
       "      <td>Kyiv_Oblast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>22056.0</td>\n",
       "      <td>Kirovohrad_Oblast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>58767.0</td>\n",
       "      <td>Luhansk_Oblast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>61962.0</td>\n",
       "      <td>Lviv_Oblast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>29205.0</td>\n",
       "      <td>Mykolaiv_Oblast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>64743.0</td>\n",
       "      <td>Odessa_Oblast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>56580.0</td>\n",
       "      <td>Poltava_Oblast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>21795.0</td>\n",
       "      <td>Rivne_Oblast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>24933.0</td>\n",
       "      <td>Sumy_Oblast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>17957.0</td>\n",
       "      <td>Ternopil_Oblast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>82223.0</td>\n",
       "      <td>Kharkiv_Oblast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>19357.0</td>\n",
       "      <td>Kherson_Oblast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>26237.0</td>\n",
       "      <td>Khmelnytskyi_Oblast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>31265.0</td>\n",
       "      <td>Cherkasy_Oblast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>13166.0</td>\n",
       "      <td>Chernivtsi_Oblast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>23934.0</td>\n",
       "      <td>Chernihiv_Oblast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>275685.0</td>\n",
       "      <td>Kyiv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        2012                  region\n",
       "3    33024.0        Vinnytsia_Oblast\n",
       "4    20005.0            Volyn_Oblast\n",
       "5   147970.0   Dnipropetrovsk_Oblast\n",
       "6   170775.0          Donetsk_Oblast\n",
       "7    24849.0         Zhytomyr_Oblast\n",
       "8    21404.0      Zakarpattia_Oblast\n",
       "9    54828.0       Zaporizhia_Oblast\n",
       "10   32286.0  Ivano-Frankivsk_Oblast\n",
       "11   69663.0             Kyiv_Oblast\n",
       "12   22056.0       Kirovohrad_Oblast\n",
       "13   58767.0          Luhansk_Oblast\n",
       "14   61962.0             Lviv_Oblast\n",
       "15   29205.0         Mykolaiv_Oblast\n",
       "16   64743.0           Odessa_Oblast\n",
       "17   56580.0          Poltava_Oblast\n",
       "18   21795.0            Rivne_Oblast\n",
       "19   24933.0             Sumy_Oblast\n",
       "20   17957.0         Ternopil_Oblast\n",
       "21   82223.0          Kharkiv_Oblast\n",
       "22   19357.0          Kherson_Oblast\n",
       "23   26237.0     Khmelnytskyi_Oblast\n",
       "24   31265.0         Cherkasy_Oblast\n",
       "25   13166.0       Chernivtsi_Oblast\n",
       "26   23934.0        Chernihiv_Oblast\n",
       "27  275685.0                    Kyiv"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "gdp_ukr_nominal = pd.read_excel(\"data/ukr_reg_gdp.xls\", skiprows=3, header=1)\n",
    "gdp_ukr_nominal = gdp_ukr_nominal.drop(columns=gdp_ukr_nominal.columns[0])\n",
    "gdp_ukr_nominal = gdp_ukr_nominal.iloc[:, np.r_[8, -1]]\n",
    "\n",
    "gdp_ukr_nominal = gdp_ukr_nominal.rename(columns={gdp_ukr_nominal.columns[-1]: \"region\"})\n",
    "gdp_ukr_nominal = gdp_ukr_nominal[~gdp_ukr_nominal[\"region\"].isin([\"Ukrane\", \"oblasts\"])]\n",
    "gdp_ukr_nominal = gdp_ukr_nominal.dropna()\n",
    "gdp_ukr_nominal[\"region\"] = gdp_ukr_nominal[\"region\"].map(region_map)\n",
    "gdp_ukr_nominal[\"region\"] = gdp_ukr_nominal[\"region\"].fillna(\"Sevastopol\")\n",
    "gdp_ukr_nominal.columns = gdp_ukr_nominal.columns.astype(str)\n",
    "gdp_ukr_nominal = gdp_ukr_nominal[~gdp_ukr_nominal[\"region\"].isin([\"Sevastopol\", \"Autonomous Republic of Crimea\"])]\n",
    "gdp_ukr_nominal[\"region\"] = gdp_ukr_nominal[\"region\"].str.replace(\" \", \"_\")\n",
    "gdp_ukr_nominal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Loading the MNIST dataset\n",
    "# from keras.datasets import mnist\n",
    "# (train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load clean gdp data, keep only year, region and real_gdp columns\n",
    "ukraine = pd.read_csv(\"data/tabular_data_ukraine.csv\")\n",
    "\n",
    "# delete observations with year > 2021, reset index\n",
    "ukraine = ukraine[ukraine[\"year\"].astype(int) <= 2021]\n",
    "ukraine.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Initialise a three dimensional array to store the images with the shape (number of images, height, width, channels)\n",
    "X = np.zeros((len(ukraine), 765, 1076, 1))\n",
    "y = np.zeros(len(ukraine))\n",
    "\n",
    "# load the images\n",
    "for i in range(len(ukraine)):\n",
    "\n",
    "    # get year, region, and gdp\n",
    "    year = ukraine[\"year\"][i]\n",
    "    region = ukraine[\"region\"][i]\n",
    "    gdp_value = ukraine[\"real_gdp\"][i]\n",
    "\n",
    "    # get the file name\n",
    "    file_name = f\"{year}_{region}_hq.h5\"\n",
    "\n",
    "    # load the image\n",
    "    file_path = f\"data/annual_region_images/{file_name}\"\n",
    "    \n",
    "    with h5py.File(file_path, 'r') as annual_region:\n",
    "        # nearnad_snow_free = annual_region[\"NearNadir_Composite_Snow_Free\"][:]\n",
    "        # offnad_snow_free = annual_region[\"OffNadir_Composite_Snow_Free\"][:]\n",
    "        allangle_snow_free = annual_region[\"AllAngle_Composite_Snow_Free\"][:]\n",
    "\n",
    "        # add the two images together\n",
    "        # combined = snow_covered + snow_free\n",
    "\n",
    "    # add the gdp value to y\n",
    "    y[i] = gdp_value\n",
    "\n",
    "    # append images to X\n",
    "    X[i, :, :, 0] = allangle_snow_free\n",
    "    # X[i, :, :, 0] = offnad_snow_free\n",
    "    # X[i, :, :, 0] = nearnad_snow_free\n",
    "\n",
    "# normalise the images\n",
    "# maximum = X.max()\n",
    "# X = X / maximum\n",
    "\n",
    "# get indices for observations with  year = 2021: this is the test set\n",
    "test_indices = np.where(ukraine[\"year\"].astype(int) == 2021)[0]\n",
    "train_indices = np.where(ukraine[\"year\"].astype(int) != 2021)[0]\n",
    "\n",
    "# get the train and test sets\n",
    "X_train = X[train_indices]\n",
    "y_train = y[train_indices]\n",
    "X_test = X[test_indices]\n",
    "y_test = y[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\jakub\\anaconda\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\jakub\\anaconda\\lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "# Resizing the images\n",
    "model.add(Resizing(300, 440, input_shape=(765, 1076, 1)))\n",
    "# Start with Convolutional layers\n",
    "model.add(Conv2D(16, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "# model.add(Conv2D(64, (3, 3), activation='relu'))  # Additional Conv layer\n",
    "# model.add(MaxPooling2D((2, 2)))\n",
    "# model.add(Conv2D(128, (3, 3), activation='relu'))  # Additional Conv layer\n",
    "# model.add(MaxPooling2D((2, 2)))\n",
    "# Flatten the results to feed into a dense layer\n",
    "model.add(Flatten())\n",
    "# Add dense layers (hidden layers)\n",
    "model.add(Dense(64, activation='relu'))  # Upscaled dense layer\n",
    "model.add(Dense(32, activation='relu'))   # Additional dense layer\n",
    "# Output layer\n",
    "model.add(Dense(1))\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(), loss='mean_absolute_error', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " resizing (Resizing)         (None, 300, 440, 1)       0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 298, 438, 16)      160       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 149, 219, 16)      0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 147, 217, 32)      4640      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 73, 108, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 252288)            0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                8073248   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8078081 (30.82 MB)\n",
      "Trainable params: 8078081 (30.82 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)        [(None, 765, 1076, 1)]       0         []                            \n",
      "                                                                                                  \n",
      " conv2d_24 (Conv2D)          (None, 383, 538, 64)         3200      ['input_2[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_13 (Ba  (None, 383, 538, 64)         256       ['conv2d_24[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_13 (Activation)  (None, 383, 538, 64)         0         ['batch_normalization_13[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " max_pooling2d_9 (MaxPoolin  (None, 192, 269, 64)         0         ['activation_13[0][0]']       \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv2d_25 (Conv2D)          (None, 192, 269, 32)         18464     ['max_pooling2d_9[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_14 (Ba  (None, 192, 269, 32)         128       ['conv2d_25[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_14 (Activation)  (None, 192, 269, 32)         0         ['batch_normalization_14[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_26 (Conv2D)          (None, 192, 269, 32)         9248      ['activation_14[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_27 (Conv2D)          (None, 192, 269, 32)         2080      ['max_pooling2d_9[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_15 (Ba  (None, 192, 269, 32)         128       ['conv2d_26[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " add_6 (Add)                 (None, 192, 269, 32)         0         ['conv2d_27[0][0]',           \n",
      "                                                                     'batch_normalization_15[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_15 (Activation)  (None, 192, 269, 32)         0         ['add_6[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_28 (Conv2D)          (None, 192, 269, 32)         9248      ['activation_15[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_16 (Ba  (None, 192, 269, 32)         128       ['conv2d_28[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_16 (Activation)  (None, 192, 269, 32)         0         ['batch_normalization_16[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_29 (Conv2D)          (None, 192, 269, 32)         9248      ['activation_16[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_17 (Ba  (None, 192, 269, 32)         128       ['conv2d_29[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " add_7 (Add)                 (None, 192, 269, 32)         0         ['activation_15[0][0]',       \n",
      "                                                                     'batch_normalization_17[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_17 (Activation)  (None, 192, 269, 32)         0         ['add_7[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_30 (Conv2D)          (None, 96, 135, 64)          18496     ['activation_17[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_18 (Ba  (None, 96, 135, 64)          256       ['conv2d_30[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_18 (Activation)  (None, 96, 135, 64)          0         ['batch_normalization_18[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_31 (Conv2D)          (None, 96, 135, 64)          36928     ['activation_18[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_32 (Conv2D)          (None, 96, 135, 64)          2112      ['activation_17[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_19 (Ba  (None, 96, 135, 64)          256       ['conv2d_31[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " add_8 (Add)                 (None, 96, 135, 64)          0         ['conv2d_32[0][0]',           \n",
      "                                                                     'batch_normalization_19[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_19 (Activation)  (None, 96, 135, 64)          0         ['add_8[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_33 (Conv2D)          (None, 96, 135, 64)          36928     ['activation_19[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_20 (Ba  (None, 96, 135, 64)          256       ['conv2d_33[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_20 (Activation)  (None, 96, 135, 64)          0         ['batch_normalization_20[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_34 (Conv2D)          (None, 96, 135, 64)          36928     ['activation_20[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_21 (Ba  (None, 96, 135, 64)          256       ['conv2d_34[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " add_9 (Add)                 (None, 96, 135, 64)          0         ['activation_19[0][0]',       \n",
      "                                                                     'batch_normalization_21[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_21 (Activation)  (None, 96, 135, 64)          0         ['add_9[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_35 (Conv2D)          (None, 48, 68, 128)          73856     ['activation_21[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_22 (Ba  (None, 48, 68, 128)          512       ['conv2d_35[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_22 (Activation)  (None, 48, 68, 128)          0         ['batch_normalization_22[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_36 (Conv2D)          (None, 48, 68, 128)          147584    ['activation_22[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_37 (Conv2D)          (None, 48, 68, 128)          8320      ['activation_21[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_23 (Ba  (None, 48, 68, 128)          512       ['conv2d_36[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " add_10 (Add)                (None, 48, 68, 128)          0         ['conv2d_37[0][0]',           \n",
      "                                                                     'batch_normalization_23[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_23 (Activation)  (None, 48, 68, 128)          0         ['add_10[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_38 (Conv2D)          (None, 48, 68, 128)          147584    ['activation_23[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_24 (Ba  (None, 48, 68, 128)          512       ['conv2d_38[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_24 (Activation)  (None, 48, 68, 128)          0         ['batch_normalization_24[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_39 (Conv2D)          (None, 48, 68, 128)          147584    ['activation_24[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_25 (Ba  (None, 48, 68, 128)          512       ['conv2d_39[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " add_11 (Add)                (None, 48, 68, 128)          0         ['activation_23[0][0]',       \n",
      "                                                                     'batch_normalization_25[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_25 (Activation)  (None, 48, 68, 128)          0         ['add_11[0][0]']              \n",
      "                                                                                                  \n",
      " global_average_pooling2d_1  (None, 128)                  0         ['activation_25[0][0]']       \n",
      "  (GlobalAveragePooling2D)                                                                        \n",
      "                                                                                                  \n",
      " dense_8 (Dense)             (None, 1)                    129       ['global_average_pooling2d_1[0\n",
      "                                                                    ][0]']                        \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 711777 (2.72 MB)\n",
      "Trainable params: 709857 (2.71 MB)\n",
      "Non-trainable params: 1920 (7.50 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def basic_block(x, filters, stride=1):\n",
    "    y = Conv2D(filters, (3, 3), strides=stride, padding='same')(x)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation('relu')(y)\n",
    "\n",
    "    y = Conv2D(filters, (3, 3), padding='same')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "\n",
    "    if stride != 1 or x.shape[-1] != filters:\n",
    "        x = Conv2D(filters, (1, 1), strides=stride, padding='same')(x)\n",
    "    y = Add()([x, y])\n",
    "    y = Activation('relu')(y)\n",
    "    return y\n",
    "\n",
    "def ResNet18(input_shape=(765, 1076, 1)):\n",
    "    input = Input(shape=input_shape)\n",
    "\n",
    "    x = Resizing(300, 440, input_shape=(765, 1076, 1))\n",
    "\n",
    "    x = Conv2D(64, (7, 7), strides=(2, 2), padding='same')(input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
    "\n",
    "    x = basic_block(x, 32, stride=1)\n",
    "    x = basic_block(x, 32, stride=1)\n",
    "    x = basic_block(x, 64, stride=2)\n",
    "    x = basic_block(x, 64, stride=1)\n",
    "    x = basic_block(x, 128, stride=2)\n",
    "    x = basic_block(x, 128, stride=1)\n",
    "    # x = basic_block(x, 512, stride=2)\n",
    "    # x = basic_block(x, 512, stride=1)\n",
    "\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(1, activation='linear')(x)\n",
    "\n",
    "    model = Model(inputs=input, outputs=x)\n",
    "    return model\n",
    "\n",
    "# Example usage\n",
    "model = ResNet18()\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=Adam(), loss='mean_squared_error', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# Resizing the images\n",
    "model.add(Resizing(300, 440, input_shape=(765, 1076, 1)))\n",
    "\n",
    "# Start with Convolutional layers\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))  # Increased number of filters\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))  # Increased number of filters\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))  # Increased number of filters\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(Conv2D(256, (3, 3), activation='relu'))  # Increased number of filters\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "# Flatten the results to feed into a dense layer\n",
    "model.add(Flatten())\n",
    "\n",
    "# Add dense layers (hidden layers)\n",
    "model.add(Dense(256, activation='relu'))  # Increased the number of neurons\n",
    "model.add(Dense(128, activation='relu'))  # Increased the number of neurons\n",
    "model.add(Dense(64, activation='relu'))   # Kept as is for detailed feature extraction\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(), loss='mean_squared_error', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 765, 1076, 1)\n",
      "0.01435\n"
     ]
    }
   ],
   "source": [
    "# check the size of X_train and y_train\n",
    "print(X_train.shape)\n",
    "print(X_train[1, :, :, 0].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:From c:\\Users\\jakub\\anaconda\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\jakub\\anaconda\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "3/3 [==============================] - 27s 7s/step - loss: 2986162432.0000 - mae: 44922.8594 - val_loss: 16875475968.0000 - val_mae: 73818.3594\n",
      "Epoch 2/50\n",
      "3/3 [==============================] - 22s 7s/step - loss: 1649803392.0000 - mae: 31756.7383 - val_loss: 12461476864.0000 - val_mae: 61813.4570\n",
      "Epoch 3/50\n",
      "3/3 [==============================] - 25s 9s/step - loss: 852806400.0000 - mae: 16027.0098 - val_loss: 14683069440.0000 - val_mae: 62618.0234\n",
      "Epoch 4/50\n",
      "3/3 [==============================] - 21s 7s/step - loss: 585213440.0000 - mae: 17655.7324 - val_loss: 15069169664.0000 - val_mae: 63303.2773\n",
      "Epoch 5/50\n",
      "3/3 [==============================] - 26s 8s/step - loss: 338988576.0000 - mae: 12590.5137 - val_loss: 14138784768.0000 - val_mae: 64194.4492\n",
      "Epoch 6/50\n",
      "3/3 [==============================] - 19s 6s/step - loss: 403277824.0000 - mae: 14660.1377 - val_loss: 14464156672.0000 - val_mae: 63644.5938\n",
      "Epoch 7/50\n",
      "3/3 [==============================] - 24s 8s/step - loss: 207753536.0000 - mae: 9760.4053 - val_loss: 14989082624.0000 - val_mae: 63552.6328\n",
      "Epoch 8/50\n",
      "3/3 [==============================] - 24s 8s/step - loss: 271813792.0000 - mae: 11658.7363 - val_loss: 14483767296.0000 - val_mae: 64155.3906\n",
      "Epoch 9/50\n",
      "3/3 [==============================] - 21s 7s/step - loss: 139644592.0000 - mae: 8345.0039 - val_loss: 14087676928.0000 - val_mae: 67707.7812\n",
      "Epoch 10/50\n",
      "3/3 [==============================] - 26s 9s/step - loss: 141979328.0000 - mae: 8488.1494 - val_loss: 14393092096.0000 - val_mae: 64375.2617\n",
      "Epoch 11/50\n",
      "3/3 [==============================] - 26s 9s/step - loss: 149477984.0000 - mae: 8755.2832 - val_loss: 14426728448.0000 - val_mae: 64196.0547\n",
      "Epoch 12/50\n",
      "3/3 [==============================] - 26s 9s/step - loss: 121430440.0000 - mae: 7358.9380 - val_loss: 14054098944.0000 - val_mae: 66915.7578\n",
      "Epoch 13/50\n",
      "3/3 [==============================] - 29s 10s/step - loss: 116273832.0000 - mae: 7239.5889 - val_loss: 14123052032.0000 - val_mae: 65972.8438\n",
      "Epoch 14/50\n",
      "3/3 [==============================] - 24s 7s/step - loss: 90426144.0000 - mae: 6475.6162 - val_loss: 14320833536.0000 - val_mae: 64530.9297\n",
      "Epoch 15/50\n",
      "3/3 [==============================] - 20s 7s/step - loss: 89921864.0000 - mae: 6668.4380 - val_loss: 14147234816.0000 - val_mae: 65973.1328\n",
      "Epoch 16/50\n",
      "3/3 [==============================] - 23s 8s/step - loss: 91090160.0000 - mae: 6527.0981 - val_loss: 14028036096.0000 - val_mae: 67038.0703\n",
      "Epoch 17/50\n",
      "3/3 [==============================] - 22s 7s/step - loss: 83255024.0000 - mae: 6232.9873 - val_loss: 14214036480.0000 - val_mae: 64766.5664\n",
      "Epoch 18/50\n",
      "3/3 [==============================] - 26s 9s/step - loss: 79502560.0000 - mae: 5740.1533 - val_loss: 14035523584.0000 - val_mae: 65664.0703\n",
      "Epoch 19/50\n",
      "3/3 [==============================] - 24s 8s/step - loss: 60879360.0000 - mae: 5156.5356 - val_loss: 13929814016.0000 - val_mae: 66243.6250\n",
      "Epoch 20/50\n",
      "3/3 [==============================] - 27s 9s/step - loss: 61305708.0000 - mae: 5068.0884 - val_loss: 13942918144.0000 - val_mae: 65545.0469\n",
      "Epoch 21/50\n",
      "3/3 [==============================] - 26s 9s/step - loss: 57207960.0000 - mae: 4837.2866 - val_loss: 13873385472.0000 - val_mae: 65708.8984\n",
      "Epoch 22/50\n",
      "3/3 [==============================] - 25s 9s/step - loss: 58127036.0000 - mae: 4862.8350 - val_loss: 13830704128.0000 - val_mae: 65718.1875\n",
      "Epoch 23/50\n",
      "3/3 [==============================] - 22s 7s/step - loss: 52429688.0000 - mae: 4661.7427 - val_loss: 13798240256.0000 - val_mae: 65669.6719\n",
      "Epoch 24/50\n",
      "3/3 [==============================] - 26s 9s/step - loss: 50820356.0000 - mae: 4679.8091 - val_loss: 13737108480.0000 - val_mae: 65740.6719\n",
      "Epoch 25/50\n",
      "3/3 [==============================] - 23s 8s/step - loss: 46843964.0000 - mae: 4436.9990 - val_loss: 13743265792.0000 - val_mae: 65147.7070\n",
      "Epoch 26/50\n",
      "3/3 [==============================] - 25s 8s/step - loss: 49351892.0000 - mae: 4420.5557 - val_loss: 13655797760.0000 - val_mae: 65323.6797\n",
      "Epoch 27/50\n",
      "3/3 [==============================] - 24s 8s/step - loss: 45636488.0000 - mae: 4359.8628 - val_loss: 13547956224.0000 - val_mae: 65207.1953\n",
      "Epoch 28/50\n",
      "3/3 [==============================] - 26s 9s/step - loss: 41972528.0000 - mae: 4165.1831 - val_loss: 13470858240.0000 - val_mae: 64665.9453\n",
      "Epoch 29/50\n",
      "3/3 [==============================] - 24s 8s/step - loss: 42043608.0000 - mae: 4114.2700 - val_loss: 13361973248.0000 - val_mae: 64427.1562\n",
      "Epoch 30/50\n",
      "1/3 [=========>....................] - ETA: 14s - loss: 40258120.0000 - mae: 3948.4165"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10260/3538720438.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Assuming you have a validation split of 20%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\jakub\\anaconda\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jakub\\anaconda\\lib\\site-packages\\keras\\src\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1805\u001b[0m                         ):\n\u001b[0;32m   1806\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1807\u001b[1;33m                             \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1808\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1809\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jakub\\anaconda\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jakub\\anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    831\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 832\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    833\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    834\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jakub\\anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    866\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    867\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 868\u001b[1;33m       return tracing_compilation.call_function(\n\u001b[0m\u001b[0;32m    869\u001b[0m           \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_no_variable_creation_config\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    870\u001b[0m       )\n",
      "\u001b[1;32mc:\\Users\\jakub\\anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m   )\n",
      "\u001b[1;32mc:\\Users\\jakub\\anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1321\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1322\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1323\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1324\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1325\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jakub\\anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m     \u001b[1;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 216\u001b[1;33m     \u001b[0mflat_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    217\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jakub\\anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 251\u001b[1;33m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[0;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jakub\\anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\context.py\u001b[0m in \u001b[0;36mcall_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1484\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1485\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1486\u001b[1;33m       outputs = execute.execute(\n\u001b[0m\u001b[0;32m   1487\u001b[0m           \u001b[0mname\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1488\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jakub\\anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=50, batch_size=64, validation_split=0.2)  # Assuming you have a validation split of 20%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 292ms/step - loss: 13331.5254 - mae: 13331.5254\n",
      "Test MAE: 13331.525390625\n",
      "Test Loss: 13331.525390625\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Evaluate your model on the testing data\n",
    "test_loss, test_mae = model.evaluate(X_test, y_test)\n",
    "print('Test MAE:', test_mae) # mean absolute error\n",
    "print('Test Loss:', test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 541ms/step\n",
      "15.93812160316615\n"
     ]
    }
   ],
   "source": [
    "y_hat = model.predict(X_test).flatten()\n",
    "\n",
    "# print the mean absolute percentage error\n",
    "print(np.mean(100*np.abs((y_test - y_hat) / y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 288ms/step\n",
      "0.038987137\n",
      "27.844175\n"
     ]
    }
   ],
   "source": [
    "# get the predictions from X_test\n",
    "y_hat = model.predict(X_test).flatten()\n",
    "\n",
    "# convert the predictions back to the original scale\n",
    "# y_hat = y_hat * y_std + y_mean\n",
    "# y_test = y_test * y_std + y_mean\n",
    "\n",
    "# compute the mae\n",
    "mae = np.mean(np.abs(y_test - y_hat))\n",
    "print(mae)\n",
    "\n",
    "# compute the mean percentage error\n",
    "percentage_error = np.mean(100*np.abs((y_test - y_hat) / y_test))\n",
    "print(percentage_error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "gdp = pd.read_csv(\"data/clean_ukr_gdp.csv\")\n",
    "\n",
    "\n",
    "# Initialise a three dimensional array to store the images with the shape (number of images, height, width, channels)\n",
    "X = np.zeros((len(gdp), 765, 1076))\n",
    "\n",
    "# load the snow covered and snow free images, add them together and append to the list\n",
    "for i in range(len(gdp)):\n",
    "\n",
    "    # get year, region, and gdp\n",
    "    year = gdp[\"year\"][i]\n",
    "    region = gdp[\"region\"][i]\n",
    "    gdp_value = gdp[\"real_gdp\"][i]\n",
    "\n",
    "    # get the file name\n",
    "    file_name = f\"{year}_{region}_hq.h5\"\n",
    "\n",
    "    # load the image\n",
    "    file_path = f\"data/annual_region_images/{file_name}\"\n",
    "    \n",
    "    with h5py.File(file_path, 'r') as annual_region:\n",
    "        # nearnad_snow_cov = annual_region[\"NearNadir_Composite_Snow_Covered\"][:]\n",
    "        # nearnad_snow_free = annual_region[\"NearNadir_Composite_Snow_Free\"][:]\n",
    "        # offnad_snow_cov = annual_region[\"OffNadir_Composite_Snow_Covered\"][:]\n",
    "        offnad_snow_free = annual_region[\"OffNadir_Composite_Snow_Free\"][:]\n",
    "        # allangle_snow_cov = annual_region[\"AllAngle_Composite_Snow_Covered\"][:]\n",
    "        # allangle_snow_free = annual_region[\"AllAngle_Composite_Snow_Free\"][:]\n",
    "\n",
    "        # add the two images together\n",
    "        # combined = snow_covered + snow_free\n",
    "\n",
    "    # append both images as two channels to to X\n",
    "    # X[i, :, :]= allangle_snow_cov\n",
    "    # X[i, :, :] = allangle_snow_free\n",
    "    # X[i, :, :] = offnad_snow_cov\n",
    "    X[i, :, :] = offnad_snow_free\n",
    "    # X[i, :, :] = nearnad_snow_cov\n",
    "    # X[i, :, :] = nearnad_snow_free\n",
    "\n",
    "X = X.astype(np.float32)\n",
    "\n",
    "# take the log\n",
    "X = np.log(X + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jakub\\anaconda\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\jakub\\anaconda\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "(300, 765, 1076)\n",
      "(300, 382, 538, 1)\n"
     ]
    }
   ],
   "source": [
    "# resize images \n",
    "import tensorflow as tf\n",
    "print(X.shape)\n",
    "\n",
    "h, w = 382, 538\n",
    "if X.ndim == 3:  # If images have shape (300, 765, 1076)\n",
    "    X = np.expand_dims(X, axis=-1)\n",
    "images_resized = tf.image.resize(X, (h, w)).numpy()\n",
    "\n",
    "print(images_resized.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\jakub\\anaconda\\lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\jakub\\anaconda\\lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\jakub\\anaconda\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 382, 538, 1)]     0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 382, 538, 16)      160       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 191, 269, 16)      0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 191, 269, 32)      4640      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 96, 135, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 414720)            0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                4147210   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 407360)            4480960   \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 95, 134, 32)       0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 95, 134, 32)       9248      \n",
      "                                                                 \n",
      " up_sampling2d (UpSampling2  (None, 190, 268, 32)      0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 190, 268, 16)      4624      \n",
      "                                                                 \n",
      " up_sampling2d_1 (UpSamplin  (None, 380, 536, 16)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " zero_padding2d (ZeroPaddin  (None, 382, 538, 16)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 382, 538, 1)       145       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8646987 (32.99 MB)\n",
      "Trainable params: 8646987 (32.99 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Dimensions\n",
    "input_shape = (h, w, 1)\n",
    "\n",
    "# Length of the compressed vector\n",
    "k = 10\n",
    "\n",
    "# Resize the data\n",
    "images_resized = tf.image.resize(X, (h, w)).numpy()\n",
    "\n",
    "# normalize the data\n",
    "# images_resized = images_resized / images_resized.max()\n",
    "\n",
    "# Define the autoencoder\n",
    "input_img = layers.Input(shape=input_shape)\n",
    "\n",
    "# Encoder\n",
    "x = layers.Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\n",
    "x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "# Flatten and add the bottleneck layer\n",
    "x = layers.Flatten()(x)\n",
    "encoded = layers.Dense(k, activation='relu')(x)\n",
    "\n",
    "# Decoder\n",
    "x = layers.Dense((h // 4) * (w// 4) * 32, activation='relu')(encoded)\n",
    "x = layers.Reshape((h // 4, w // 4, 32))(x)\n",
    "x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "x = layers.UpSampling2D((2, 2))(x)\n",
    "x = layers.Conv2D(16, (3, 3), activation='relu', padding='same')(x)\n",
    "x = layers.UpSampling2D((2, 2))(x)\n",
    "x = layers.ZeroPadding2D(padding=((1, 1), (1, 1)))(x)\n",
    "\n",
    "decoded = layers.Conv2D(1, (3, 3), activation='linear', padding='same')(x)\n",
    "# Create the autoencoder model\n",
    "autoencoder = models.Model(input_img, decoded)\n",
    "\n",
    "# Compile the model\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Print the model summary\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "WARNING:tensorflow:From c:\\Users\\jakub\\anaconda\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "10/10 [==============================] - 29s 2s/step - loss: 0.1712\n",
      "Epoch 2/20\n",
      "10/10 [==============================] - 19s 2s/step - loss: 0.1492\n",
      "Epoch 3/20\n",
      "10/10 [==============================] - 26s 3s/step - loss: 0.1378\n",
      "Epoch 4/20\n",
      "10/10 [==============================] - 29s 3s/step - loss: 0.1248\n",
      "Epoch 5/20\n",
      "10/10 [==============================] - 24s 2s/step - loss: 0.1127\n",
      "Epoch 6/20\n",
      "10/10 [==============================] - 26s 2s/step - loss: 0.1010\n",
      "Epoch 7/20\n",
      "10/10 [==============================] - 21s 2s/step - loss: 0.0895\n",
      "Epoch 8/20\n",
      "10/10 [==============================] - 19s 2s/step - loss: 0.0803\n",
      "Epoch 9/20\n",
      "10/10 [==============================] - 20s 2s/step - loss: 0.0730\n",
      "Epoch 10/20\n",
      "10/10 [==============================] - 21s 2s/step - loss: 0.0673\n",
      "Epoch 11/20\n",
      "10/10 [==============================] - 21s 2s/step - loss: 0.0638\n",
      "Epoch 12/20\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.0602\n",
      "Epoch 13/20\n",
      "10/10 [==============================] - 21s 2s/step - loss: 0.0571\n",
      "Epoch 14/20\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.0547\n",
      "Epoch 15/20\n",
      "10/10 [==============================] - 21s 2s/step - loss: 0.0532\n",
      "Epoch 16/20\n",
      "10/10 [==============================] - 21s 2s/step - loss: 0.0538\n",
      "Epoch 17/20\n",
      "10/10 [==============================] - 21s 2s/step - loss: 0.0517\n",
      "Epoch 18/20\n",
      "10/10 [==============================] - 23s 2s/step - loss: 0.0492\n",
      "Epoch 19/20\n",
      "10/10 [==============================] - 28s 3s/step - loss: 0.0475\n",
      "Epoch 20/20\n",
      "10/10 [==============================] - 19s 2s/step - loss: 0.0461\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1592d464c40>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming you have training data loaded in `train_images`\n",
    "autoencoder.fit(images_resized, images_resized, epochs=20, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jakub\\anaconda\\lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "autoencoder.save('simple_autoencoder.h5')\n",
    "encoder = models.Model(input_img, encoded)\n",
    "encoder.save('encoder.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 3s 165ms/step\n"
     ]
    }
   ],
   "source": [
    "encoded_imgs = encoder.predict(images_resized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# turn the encoded images to a pandas dataframe\n",
    "encoded_imgs = pd.DataFrame(encoded_imgs)\n",
    "gdp = pd.read_csv(\"data/clean_ukr_gdp.csv\")\n",
    "\n",
    "\n",
    "# change column names to encoded_1, encoded_2, ...\n",
    "encoded_imgs.columns = [f\"encoded_{i}\" for i in range(encoded_imgs.shape[1])]\n",
    "\n",
    "# # add encoded images to the gdp data as new columns\n",
    "gdp = pd.concat([gdp, encoded_imgs], axis=1)\n",
    "\n",
    "data = pd.get_dummies(gdp, columns=[\"region\"])\n",
    "\n",
    "# training data contains years until 2021\n",
    "train_data = data[data[\"year\"] <= 2021]\n",
    "test_data = data[data[\"year\"] >= 2022]\n",
    "\n",
    "X = train_data.drop(columns=[\"year\", \"real_gdp\"])\n",
    "y = train_data[\"real_gdp\"]\n",
    "\n",
    "# take randomly 80% of the data for training\n",
    "train_size = int(0.8 * len(train_data))\n",
    "test_size = len(train_data) - train_size\n",
    "\n",
    "train_indices = np.random.choice(len(train_data), train_size, replace=False)\n",
    "test_indices = np.setdiff1d(np.arange(len(train_data)), train_indices)\n",
    "\n",
    "X_train = X.iloc[train_indices]\n",
    "y_train = y.iloc[train_indices]\n",
    "X_test = X.iloc[test_indices]\n",
    "y_test = y.iloc[test_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Please reshape the input data into 2-dimensional matrix.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18652/2252530174.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mxgb_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mXGBRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"reg:squarederror\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mxgb_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# get the predictions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jakub\\anaconda\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    728\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    729\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 730\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    731\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    732\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jakub\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mverbosity\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbosity\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1054\u001b[0m             \u001b[0mevals_result\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTrainingCallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEvalsLog\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1055\u001b[1;33m             train_dmatrix, evals = _wrap_evaluation_matrices(\n\u001b[0m\u001b[0;32m   1056\u001b[0m                 \u001b[0mmissing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1057\u001b[0m                 \u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jakub\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py\u001b[0m in \u001b[0;36m_wrap_evaluation_matrices\u001b[1;34m(missing, X, y, group, qid, sample_weight, base_margin, feature_weights, eval_set, sample_weight_eval_set, base_margin_eval_set, eval_group, eval_qid, create_dmatrix, enable_categorical, feature_types)\u001b[0m\n\u001b[0;32m    519\u001b[0m     \"\"\"Convert array_like evaluation matrices into DMatrix.  Perform validation on the\n\u001b[0;32m    520\u001b[0m     way.\"\"\"\n\u001b[1;32m--> 521\u001b[1;33m     train_dmatrix = create_dmatrix(\n\u001b[0m\u001b[0;32m    522\u001b[0m         \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m         \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jakub\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py\u001b[0m in \u001b[0;36m_create_dmatrix\u001b[1;34m(self, ref, **kwargs)\u001b[0m\n\u001b[0;32m    956\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_can_use_qdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_method\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbooster\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"gblinear\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    957\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 958\u001b[1;33m                 return QuantileDMatrix(\n\u001b[0m\u001b[0;32m    959\u001b[0m                     \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnthread\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_bin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_bin\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    960\u001b[0m                 )\n",
      "\u001b[1;32mc:\\Users\\jakub\\anaconda\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    728\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    729\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 730\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    731\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    732\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jakub\\anaconda\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, label, weight, base_margin, missing, silent, feature_names, feature_types, nthread, max_bin, ref, group, qid, label_lower_bound, label_upper_bound, feature_weights, enable_categorical, data_split_mode)\u001b[0m\n\u001b[0;32m   1527\u001b[0m                 )\n\u001b[0;32m   1528\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1529\u001b[1;33m         self._init(\n\u001b[0m\u001b[0;32m   1530\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1531\u001b[0m             \u001b[0mref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jakub\\anaconda\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36m_init\u001b[1;34m(self, data, ref, enable_categorical, **meta)\u001b[0m\n\u001b[0;32m   1586\u001b[0m             \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbyref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1587\u001b[0m         )\n\u001b[1;32m-> 1588\u001b[1;33m         \u001b[0mit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1589\u001b[0m         \u001b[1;31m# delay check_call to throw intermediate exception first\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1590\u001b[0m         \u001b[0m_check_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jakub\\anaconda\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36mreraise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    574\u001b[0m             \u001b[0mexc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    575\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exception\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 576\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mexc\u001b[0m  \u001b[1;31m# pylint: disable=raising-bad-type\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    577\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__del__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jakub\\anaconda\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36m_handle_exception\u001b[1;34m(self, fn, dft_ret)\u001b[0m\n\u001b[0;32m    555\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    556\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 557\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    558\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m             \u001b[1;31m# Defer the exception in order to return 0 and stop the iteration.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jakub\\anaconda\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    639\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    640\u001b[0m         \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 641\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    642\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    643\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jakub\\anaconda\\lib\\site-packages\\xgboost\\data.py\u001b[0m in \u001b[0;36mnext\u001b[1;34m(self, input_data)\u001b[0m\n\u001b[0;32m   1278\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1279\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mit\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1280\u001b[1;33m         \u001b[0minput_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1281\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1282\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jakub\\anaconda\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    728\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    729\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 730\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    731\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    732\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jakub\\anaconda\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36minput_data\u001b[1;34m(data, feature_names, feature_types, **kwargs)\u001b[0m\n\u001b[0;32m    630\u001b[0m             \u001b[1;31m# Stage the data, meta info are copied inside C++ MetaInfo.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    631\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_temporary_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcat_codes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 632\u001b[1;33m             \u001b[0mdispatch_proxy_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mproxy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcat_codes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_allow_host\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    633\u001b[0m             self.proxy.set_info(\n\u001b[0;32m    634\u001b[0m                 \u001b[0mfeature_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jakub\\anaconda\\lib\\site-packages\\xgboost\\data.py\u001b[0m in \u001b[0;36mdispatch_proxy_set_data\u001b[1;34m(proxy, data, cat_codes, allow_host)\u001b[0m\n\u001b[0;32m   1329\u001b[0m     \u001b[1;34m\"\"\"Dispatch for QuantileDMatrix.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0m_is_cudf_ser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0m_is_pandas_series\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1331\u001b[1;33m         \u001b[0m_check_data_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1332\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_is_cudf_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jakub\\anaconda\\lib\\site-packages\\xgboost\\data.py\u001b[0m in \u001b[0;36m_check_data_shape\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_check_data_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mDataType\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"shape\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Please reshape the input data into 2-dimensional matrix.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Please reshape the input data into 2-dimensional matrix."
     ]
    }
   ],
   "source": [
    "# fit a XGBoost model\n",
    "import xgboost as xgb\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(objective=\"reg:squarederror\", random_state=0)\n",
    "\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# get the predictions\n",
    "y_hat = xgb_model.predict(X_test)\n",
    "\n",
    "# compute the mae\n",
    "mae = np.mean(np.abs(y_test - y_hat))\n",
    "\n",
    "# compute the mean percentage error\n",
    "percentage_error = np.mean(100*np.abs((y_test - y_hat) / y_test))\n",
    "\n",
    "print(mae)\n",
    "print(percentage_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/4118865579.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  results[\"gdp_prediction\"] = y_hat\n"
     ]
    }
   ],
   "source": [
    "# predict on full data\n",
    "best_model = xgb.XGBRegressor(objective=\"reg:squarederror\", random_state=0)\n",
    "\n",
    "best_model.fit(X, y)\n",
    "\n",
    "results = test_data\n",
    "test_data = test_data.drop(columns=[\"year\", \"real_gdp\"])\n",
    "y_hat = best_model.predict(test_data)\n",
    "\n",
    "results[\"gdp_prediction\"] = y_hat\n",
    "\n",
    "# results = results[[\"region\", \"gdp_prediction\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>real_gdp</th>\n",
       "      <th>encoded_0</th>\n",
       "      <th>encoded_1</th>\n",
       "      <th>encoded_2</th>\n",
       "      <th>encoded_3</th>\n",
       "      <th>encoded_4</th>\n",
       "      <th>encoded_5</th>\n",
       "      <th>encoded_6</th>\n",
       "      <th>encoded_7</th>\n",
       "      <th>...</th>\n",
       "      <th>region_Poltava_Oblast</th>\n",
       "      <th>region_Rivne_Oblast</th>\n",
       "      <th>region_Sumy_Oblast</th>\n",
       "      <th>region_Ternopil_Oblast</th>\n",
       "      <th>region_Vinnytsia_Oblast</th>\n",
       "      <th>region_Volyn_Oblast</th>\n",
       "      <th>region_Zakarpattia_Oblast</th>\n",
       "      <th>region_Zaporizhia_Oblast</th>\n",
       "      <th>region_Zhytomyr_Oblast</th>\n",
       "      <th>gdp_prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>2022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>32.356968</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>43.147476</td>\n",
       "      <td>6.614171</td>\n",
       "      <td>57.424915</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2.901053e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>2022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>65.775826</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42.208607</td>\n",
       "      <td>28.832651</td>\n",
       "      <td>20.495667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>19.882025</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2.009610e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>2022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>28.939548</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.387489</td>\n",
       "      <td>51.399616</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.370453e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>2022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>85.156311</td>\n",
       "      <td>2.947619</td>\n",
       "      <td>62.387512</td>\n",
       "      <td>32.447998</td>\n",
       "      <td>23.472427</td>\n",
       "      <td>31.117155</td>\n",
       "      <td>4.903654</td>\n",
       "      <td>44.713188</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3.709054e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>2022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.884701</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>38.491554</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.141434</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.568389</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>2.948006e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>2022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>132.615051</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.576831</td>\n",
       "      <td>13.221070</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.538671</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2.111304e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>2022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.066391</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>69.055626</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2.851961e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>2022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24.315319</td>\n",
       "      <td>12.990799</td>\n",
       "      <td>23.690477</td>\n",
       "      <td>40.577160</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.253977</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3.480044e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>2022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24.124853</td>\n",
       "      <td>11.860563</td>\n",
       "      <td>3.499140</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>83.198761</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42.292961</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2.524573e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>2022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>49.317535</td>\n",
       "      <td>23.669285</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2.799378e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>2022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>64.932922</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>62.514519</td>\n",
       "      <td>85.151123</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.974605e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>2022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.377668</td>\n",
       "      <td>105.309410</td>\n",
       "      <td>51.549778</td>\n",
       "      <td>2.520474</td>\n",
       "      <td>39.702934</td>\n",
       "      <td>20.234741</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>99.735855</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>5.917208e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>2022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.117034</td>\n",
       "      <td>9.557559</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3.109010e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>2022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>32.279461</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>5.157749e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>2022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.620899</td>\n",
       "      <td>29.988794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3.881390e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>2022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>47.804504</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.081977</td>\n",
       "      <td>24.307182</td>\n",
       "      <td>2.423903</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.453278</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2.231310e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>2022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>23.274954</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27.327993</td>\n",
       "      <td>1.941042</td>\n",
       "      <td>4.701285</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098328</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2.496220e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>2022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.404328</td>\n",
       "      <td>34.771671</td>\n",
       "      <td>36.263599</td>\n",
       "      <td>26.087931</td>\n",
       "      <td>17.159975</td>\n",
       "      <td>43.585247</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2.251190e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>2022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.531701</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.862323</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4.148731e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>2022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>86.954224</td>\n",
       "      <td>84.024147</td>\n",
       "      <td>50.510502</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.939266e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>2022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.881707</td>\n",
       "      <td>1.879484</td>\n",
       "      <td>8.300534</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>63.572739</td>\n",
       "      <td>70.749725</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2.624105e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>2022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>35.141968</td>\n",
       "      <td>26.455742</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.144335</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2.961603e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>2022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.895641</td>\n",
       "      <td>49.103500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.424743</td>\n",
       "      <td>49.028187</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.219572</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.645569e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>2022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.357098</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.728305</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2.571784e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>2022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>58.731270</td>\n",
       "      <td>46.541512</td>\n",
       "      <td>51.322849</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.283657</td>\n",
       "      <td>17.720144</td>\n",
       "      <td>12.576164</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2.572198e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>2023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>75.949532</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>131.039780</td>\n",
       "      <td>64.321556</td>\n",
       "      <td>54.991585</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>94.787010</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4.900883e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>2023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>64.977066</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>81.399017</td>\n",
       "      <td>52.826775</td>\n",
       "      <td>17.857889</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>71.891693</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>6.023840e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>2023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>51.968273</td>\n",
       "      <td>20.797262</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>88.438080</td>\n",
       "      <td>15.208241</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.374459e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>2023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>79.791435</td>\n",
       "      <td>8.541647</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>95.033958</td>\n",
       "      <td>68.514725</td>\n",
       "      <td>33.335163</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>102.231369</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>6.869778e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>2023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>35.114967</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.980209</td>\n",
       "      <td>31.953747</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>79.424248</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>5.935744e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>2023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>166.415970</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42.227905</td>\n",
       "      <td>39.235859</td>\n",
       "      <td>3.090797</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>76.413551</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>6.494650e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>2023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21.286146</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.071556</td>\n",
       "      <td>116.726830</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>58.657703</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2.723546e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>2023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.700587</td>\n",
       "      <td>18.709120</td>\n",
       "      <td>68.790398</td>\n",
       "      <td>64.153214</td>\n",
       "      <td>59.984715</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>108.449814</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3.440170e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>2023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.468575</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>86.899544</td>\n",
       "      <td>23.521624</td>\n",
       "      <td>73.919250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>113.595375</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>6.475463e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>2023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>85.617317</td>\n",
       "      <td>110.655190</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.273093</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2.490796e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>2023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.193028</td>\n",
       "      <td>79.083710</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>123.386757</td>\n",
       "      <td>132.203903</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>82.014114</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4.168402e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>2023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>104.092178</td>\n",
       "      <td>53.096714</td>\n",
       "      <td>66.645973</td>\n",
       "      <td>43.919735</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>164.768494</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>6.925840e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>2023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>64.378624</td>\n",
       "      <td>53.028915</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>28.328146</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3.098534e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>2023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>118.833672</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>35.174244</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>5.554954e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>2023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>89.932915</td>\n",
       "      <td>79.645287</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>43.839787</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4.597882e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>2023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>58.596031</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>75.023285</td>\n",
       "      <td>38.211773</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>81.722946</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>6.112166e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>2023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>52.441467</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>56.185223</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>44.379845</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2.347504e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>2023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.049595</td>\n",
       "      <td>79.127190</td>\n",
       "      <td>40.940548</td>\n",
       "      <td>89.539642</td>\n",
       "      <td>75.145302</td>\n",
       "      <td>77.351135</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>86.608978</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>5.879978e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>2023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>35.120060</td>\n",
       "      <td>44.417534</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>58.568123</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4.573593e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>2023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>91.466980</td>\n",
       "      <td>95.484573</td>\n",
       "      <td>107.734558</td>\n",
       "      <td>0.114152</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.519308</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.982434e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>2023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.501080</td>\n",
       "      <td>16.011730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>34.552292</td>\n",
       "      <td>129.523178</td>\n",
       "      <td>79.853905</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>75.113213</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>6.201698e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>2023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.381772</td>\n",
       "      <td>47.776096</td>\n",
       "      <td>107.813789</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>38.612297</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3.188360e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>2023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>79.026489</td>\n",
       "      <td>82.651367</td>\n",
       "      <td>1.452701</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>54.113617</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>59.449387</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2.449324e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>2023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>23.723021</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>63.326340</td>\n",
       "      <td>6.445199</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>47.565456</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2.358227e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>2023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>58.493618</td>\n",
       "      <td>46.457417</td>\n",
       "      <td>51.225571</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>39.787750</td>\n",
       "      <td>16.869678</td>\n",
       "      <td>11.558626</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2.572198e+07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     year  real_gdp  encoded_0   encoded_1  encoded_2   encoded_3   encoded_4  \\\n",
       "250  2022       NaN   0.000000   32.356968   0.000000   43.147476    6.614171   \n",
       "251  2022       NaN   0.000000   65.775826   0.000000   42.208607   28.832651   \n",
       "252  2022       NaN   0.000000    0.000000  28.939548    0.000000    0.000000   \n",
       "253  2022       NaN  85.156311    2.947619  62.387512   32.447998   23.472427   \n",
       "254  2022       NaN   0.000000    6.884701   0.000000   38.491554    0.000000   \n",
       "255  2022       NaN   0.000000  132.615051   0.000000    5.576831   13.221070   \n",
       "256  2022       NaN   0.000000   16.066391   0.000000    0.000000   69.055626   \n",
       "257  2022       NaN   0.000000    0.000000  24.315319   12.990799   23.690477   \n",
       "258  2022       NaN   0.000000   24.124853  11.860563    3.499140    0.000000   \n",
       "259  2022       NaN   0.000000    0.000000  49.317535   23.669285    0.000000   \n",
       "260  2022       NaN   0.000000   64.932922   0.000000   62.514519   85.151123   \n",
       "261  2022       NaN  14.377668  105.309410  51.549778    2.520474   39.702934   \n",
       "262  2022       NaN   0.000000    0.000000   0.000000   12.117034    9.557559   \n",
       "263  2022       NaN   0.000000    0.000000   0.000000    0.000000    0.000000   \n",
       "264  2022       NaN   0.000000    0.000000   0.000000   40.620899   29.988794   \n",
       "265  2022       NaN   0.000000   47.804504   0.000000    5.081977   24.307182   \n",
       "266  2022       NaN   0.000000   23.274954   0.000000   27.327993    1.941042   \n",
       "267  2022       NaN   1.404328   34.771671  36.263599   26.087931   17.159975   \n",
       "268  2022       NaN   0.000000    0.000000   1.531701    0.000000    7.862323   \n",
       "269  2022       NaN   0.000000   86.954224  84.024147   50.510502    0.000000   \n",
       "270  2022       NaN   1.881707    1.879484   8.300534    0.000000   63.572739   \n",
       "271  2022       NaN   0.000000    0.000000  35.141968   26.455742    0.000000   \n",
       "272  2022       NaN   0.000000   50.895641  49.103500    0.000000    3.424743   \n",
       "273  2022       NaN   0.000000    1.357098   0.000000    6.728305    0.000000   \n",
       "274  2022       NaN  58.731270   46.541512  51.322849    0.000000    0.000000   \n",
       "275  2023       NaN   0.000000   75.949532   0.000000  131.039780   64.321556   \n",
       "276  2023       NaN   0.000000   64.977066   0.000000   81.399017   52.826775   \n",
       "277  2023       NaN   0.000000    0.000000  51.968273   20.797262    0.000000   \n",
       "278  2023       NaN  79.791435    8.541647   0.000000   95.033958   68.514725   \n",
       "279  2023       NaN   0.000000   35.114967   0.000000  100.980209   31.953747   \n",
       "280  2023       NaN   0.000000  166.415970   0.000000   42.227905   39.235859   \n",
       "281  2023       NaN   0.000000   21.286146   0.000000    9.071556  116.726830   \n",
       "282  2023       NaN   0.000000   14.700587  18.709120   68.790398   64.153214   \n",
       "283  2023       NaN   0.000000    5.468575   0.000000   86.899544   23.521624   \n",
       "284  2023       NaN   0.000000    0.000000  85.617317  110.655190    0.000000   \n",
       "285  2023       NaN   4.193028   79.083710   0.000000  123.386757  132.203903   \n",
       "286  2023       NaN   0.000000  104.092178  53.096714   66.645973   43.919735   \n",
       "287  2023       NaN   0.000000    0.000000   0.000000   64.378624   53.028915   \n",
       "288  2023       NaN   0.000000    0.000000   0.000000  118.833672    0.000000   \n",
       "289  2023       NaN   0.000000    0.000000   0.000000   89.932915   79.645287   \n",
       "290  2023       NaN   0.000000   58.596031   0.000000   75.023285   38.211773   \n",
       "291  2023       NaN   0.000000   52.441467   0.000000   56.185223    0.000000   \n",
       "292  2023       NaN  30.049595   79.127190  40.940548   89.539642   75.145302   \n",
       "293  2023       NaN   0.000000    0.000000   0.000000   35.120060   44.417534   \n",
       "294  2023       NaN   0.000000   91.466980  95.484573  107.734558    0.114152   \n",
       "295  2023       NaN  32.501080   16.011730   0.000000   34.552292  129.523178   \n",
       "296  2023       NaN   0.000000   15.381772  47.776096  107.813789    0.000000   \n",
       "297  2023       NaN   0.000000   79.026489  82.651367    1.452701    0.000000   \n",
       "298  2023       NaN   0.000000   23.723021   0.000000   63.326340    6.445199   \n",
       "299  2023       NaN  58.493618   46.457417  51.225571    0.000000    0.000000   \n",
       "\n",
       "     encoded_5  encoded_6   encoded_7  ...  region_Poltava_Oblast  \\\n",
       "250  57.424915   0.000000    0.000000  ...                  False   \n",
       "251  20.495667   0.000000   19.882025  ...                  False   \n",
       "252  33.387489  51.399616    0.000000  ...                  False   \n",
       "253  31.117155   4.903654   44.713188  ...                  False   \n",
       "254   7.141434   0.000000    0.568389  ...                  False   \n",
       "255   0.000000   0.000000   13.538671  ...                  False   \n",
       "256   0.000000   0.000000    0.000000  ...                  False   \n",
       "257  40.577160   0.000000   30.253977  ...                  False   \n",
       "258  83.198761   0.000000   42.292961  ...                  False   \n",
       "259   0.000000   0.000000    0.000000  ...                  False   \n",
       "260   0.000000   0.000000    0.000000  ...                  False   \n",
       "261  20.234741   0.000000   99.735855  ...                  False   \n",
       "262   0.000000   0.000000    0.000000  ...                  False   \n",
       "263  32.279461   0.000000    0.000000  ...                  False   \n",
       "264   0.000000   0.000000    0.000000  ...                   True   \n",
       "265   2.423903   0.000000    3.453278  ...                  False   \n",
       "266   4.701285   0.000000    0.098328  ...                  False   \n",
       "267  43.585247   0.000000    0.000000  ...                  False   \n",
       "268   0.000000   0.000000    0.000000  ...                  False   \n",
       "269   0.000000   0.000000    0.000000  ...                  False   \n",
       "270  70.749725   0.000000    0.000000  ...                  False   \n",
       "271   0.000000   0.000000    1.144335  ...                  False   \n",
       "272  49.028187   0.000000   14.219572  ...                  False   \n",
       "273   0.000000   0.000000    0.000000  ...                  False   \n",
       "274  40.283657  17.720144   12.576164  ...                  False   \n",
       "275  54.991585   0.000000   94.787010  ...                  False   \n",
       "276  17.857889   0.000000   71.891693  ...                  False   \n",
       "277   0.000000  88.438080   15.208241  ...                  False   \n",
       "278  33.335163   0.000000  102.231369  ...                  False   \n",
       "279   0.000000   0.000000   79.424248  ...                  False   \n",
       "280   3.090797   0.000000   76.413551  ...                  False   \n",
       "281   0.000000   0.000000   58.657703  ...                  False   \n",
       "282  59.984715   0.000000  108.449814  ...                  False   \n",
       "283  73.919250   0.000000  113.595375  ...                  False   \n",
       "284   0.000000   0.000000    5.273093  ...                  False   \n",
       "285   0.000000   0.000000   82.014114  ...                  False   \n",
       "286   0.000000   0.000000  164.768494  ...                  False   \n",
       "287   0.000000   0.000000   28.328146  ...                  False   \n",
       "288   0.000000   0.000000   35.174244  ...                  False   \n",
       "289   0.000000   0.000000   43.839787  ...                   True   \n",
       "290   0.000000   0.000000   81.722946  ...                  False   \n",
       "291   0.000000   0.000000   44.379845  ...                  False   \n",
       "292  77.351135   0.000000   86.608978  ...                  False   \n",
       "293   0.000000   0.000000   58.568123  ...                  False   \n",
       "294   0.000000   0.000000   16.519308  ...                  False   \n",
       "295  79.853905   0.000000   75.113213  ...                  False   \n",
       "296   0.000000   0.000000   38.612297  ...                  False   \n",
       "297  54.113617   0.000000   59.449387  ...                  False   \n",
       "298   0.000000   0.000000   47.565456  ...                  False   \n",
       "299  39.787750  16.869678   11.558626  ...                  False   \n",
       "\n",
       "     region_Rivne_Oblast  region_Sumy_Oblast  region_Ternopil_Oblast  \\\n",
       "250                False               False                   False   \n",
       "251                False               False                   False   \n",
       "252                False               False                   False   \n",
       "253                False               False                   False   \n",
       "254                False               False                   False   \n",
       "255                False               False                   False   \n",
       "256                False               False                   False   \n",
       "257                False               False                   False   \n",
       "258                False               False                   False   \n",
       "259                False               False                   False   \n",
       "260                False               False                   False   \n",
       "261                False               False                   False   \n",
       "262                False               False                   False   \n",
       "263                False               False                   False   \n",
       "264                False               False                   False   \n",
       "265                 True               False                   False   \n",
       "266                False                True                   False   \n",
       "267                False               False                    True   \n",
       "268                False               False                   False   \n",
       "269                False               False                   False   \n",
       "270                False               False                   False   \n",
       "271                False               False                   False   \n",
       "272                False               False                   False   \n",
       "273                False               False                   False   \n",
       "274                False               False                   False   \n",
       "275                False               False                   False   \n",
       "276                False               False                   False   \n",
       "277                False               False                   False   \n",
       "278                False               False                   False   \n",
       "279                False               False                   False   \n",
       "280                False               False                   False   \n",
       "281                False               False                   False   \n",
       "282                False               False                   False   \n",
       "283                False               False                   False   \n",
       "284                False               False                   False   \n",
       "285                False               False                   False   \n",
       "286                False               False                   False   \n",
       "287                False               False                   False   \n",
       "288                False               False                   False   \n",
       "289                False               False                   False   \n",
       "290                 True               False                   False   \n",
       "291                False                True                   False   \n",
       "292                False               False                    True   \n",
       "293                False               False                   False   \n",
       "294                False               False                   False   \n",
       "295                False               False                   False   \n",
       "296                False               False                   False   \n",
       "297                False               False                   False   \n",
       "298                False               False                   False   \n",
       "299                False               False                   False   \n",
       "\n",
       "     region_Vinnytsia_Oblast  region_Volyn_Oblast  region_Zakarpattia_Oblast  \\\n",
       "250                     True                False                      False   \n",
       "251                    False                 True                      False   \n",
       "252                    False                False                      False   \n",
       "253                    False                False                      False   \n",
       "254                    False                False                      False   \n",
       "255                    False                False                       True   \n",
       "256                    False                False                      False   \n",
       "257                    False                False                      False   \n",
       "258                    False                False                      False   \n",
       "259                    False                False                      False   \n",
       "260                    False                False                      False   \n",
       "261                    False                False                      False   \n",
       "262                    False                False                      False   \n",
       "263                    False                False                      False   \n",
       "264                    False                False                      False   \n",
       "265                    False                False                      False   \n",
       "266                    False                False                      False   \n",
       "267                    False                False                      False   \n",
       "268                    False                False                      False   \n",
       "269                    False                False                      False   \n",
       "270                    False                False                      False   \n",
       "271                    False                False                      False   \n",
       "272                    False                False                      False   \n",
       "273                    False                False                      False   \n",
       "274                    False                False                      False   \n",
       "275                     True                False                      False   \n",
       "276                    False                 True                      False   \n",
       "277                    False                False                      False   \n",
       "278                    False                False                      False   \n",
       "279                    False                False                      False   \n",
       "280                    False                False                       True   \n",
       "281                    False                False                      False   \n",
       "282                    False                False                      False   \n",
       "283                    False                False                      False   \n",
       "284                    False                False                      False   \n",
       "285                    False                False                      False   \n",
       "286                    False                False                      False   \n",
       "287                    False                False                      False   \n",
       "288                    False                False                      False   \n",
       "289                    False                False                      False   \n",
       "290                    False                False                      False   \n",
       "291                    False                False                      False   \n",
       "292                    False                False                      False   \n",
       "293                    False                False                      False   \n",
       "294                    False                False                      False   \n",
       "295                    False                False                      False   \n",
       "296                    False                False                      False   \n",
       "297                    False                False                      False   \n",
       "298                    False                False                      False   \n",
       "299                    False                False                      False   \n",
       "\n",
       "     region_Zaporizhia_Oblast  region_Zhytomyr_Oblast  gdp_prediction  \n",
       "250                     False                   False    2.901053e+06  \n",
       "251                     False                   False    2.009610e+06  \n",
       "252                     False                   False    1.370453e+07  \n",
       "253                     False                   False    3.709054e+06  \n",
       "254                     False                    True    2.948006e+06  \n",
       "255                     False                   False    2.111304e+06  \n",
       "256                      True                   False    2.851961e+06  \n",
       "257                     False                   False    3.480044e+06  \n",
       "258                     False                   False    2.524573e+06  \n",
       "259                     False                   False    2.799378e+06  \n",
       "260                     False                   False    1.974605e+06  \n",
       "261                     False                   False    5.917208e+06  \n",
       "262                     False                   False    3.109010e+06  \n",
       "263                     False                   False    5.157749e+06  \n",
       "264                     False                   False    3.881390e+06  \n",
       "265                     False                   False    2.231310e+06  \n",
       "266                     False                   False    2.496220e+06  \n",
       "267                     False                   False    2.251190e+06  \n",
       "268                     False                   False    4.148731e+06  \n",
       "269                     False                   False    1.939266e+06  \n",
       "270                     False                   False    2.624105e+06  \n",
       "271                     False                   False    2.961603e+06  \n",
       "272                     False                   False    1.645569e+06  \n",
       "273                     False                   False    2.571784e+06  \n",
       "274                     False                   False    2.572198e+07  \n",
       "275                     False                   False    4.900883e+06  \n",
       "276                     False                   False    6.023840e+06  \n",
       "277                     False                   False    1.374459e+07  \n",
       "278                     False                   False    6.869778e+06  \n",
       "279                     False                    True    5.935744e+06  \n",
       "280                     False                   False    6.494650e+06  \n",
       "281                      True                   False    2.723546e+06  \n",
       "282                     False                   False    3.440170e+06  \n",
       "283                     False                   False    6.475463e+06  \n",
       "284                     False                   False    2.490796e+06  \n",
       "285                     False                   False    4.168402e+06  \n",
       "286                     False                   False    6.925840e+06  \n",
       "287                     False                   False    3.098534e+06  \n",
       "288                     False                   False    5.554954e+06  \n",
       "289                     False                   False    4.597882e+06  \n",
       "290                     False                   False    6.112166e+06  \n",
       "291                     False                   False    2.347504e+06  \n",
       "292                     False                   False    5.879978e+06  \n",
       "293                     False                   False    4.573593e+06  \n",
       "294                     False                   False    1.982434e+06  \n",
       "295                     False                   False    6.201698e+06  \n",
       "296                     False                   False    3.188360e+06  \n",
       "297                     False                   False    2.449324e+06  \n",
       "298                     False                   False    2.358227e+06  \n",
       "299                     False                   False    2.572198e+07  \n",
       "\n",
       "[50 rows x 38 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with dim 4. Estimator expected <= 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18652/2603753667.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mrf_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mrf_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# get the predictions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jakub\\anaconda\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    302\u001b[0m                 \u001b[1;34m\"sparse multilabel-indicator for y is not supported.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    303\u001b[0m             )\n\u001b[1;32m--> 304\u001b[1;33m         X, y = self._validate_data(X, y, multi_output=True,\n\u001b[0m\u001b[0;32m    305\u001b[0m                                    accept_sparse=\"csc\", dtype=DTYPE)\n\u001b[0;32m    306\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jakub\\anaconda\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    431\u001b[0m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    432\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 433\u001b[1;33m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    434\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    435\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jakub\\anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jakub\\anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m    869\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"y cannot be None\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    870\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 871\u001b[1;33m     X = check_array(X, accept_sparse=accept_sparse,\n\u001b[0m\u001b[0;32m    872\u001b[0m                     \u001b[0maccept_large_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maccept_large_sparse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    873\u001b[0m                     \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jakub\\anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jakub\\anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    714\u001b[0m                     \"into decimal numbers with dtype='numeric'\") from e\n\u001b[0;32m    715\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mallow_nd\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 716\u001b[1;33m             raise ValueError(\"Found array with dim %d. %s expected <= 2.\"\n\u001b[0m\u001b[0;32m    717\u001b[0m                              % (array.ndim, estimator_name))\n\u001b[0;32m    718\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with dim 4. Estimator expected <= 2."
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf_model = RandomForestRegressor(n_estimators=1000, random_state = 0)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# get the predictions\n",
    "y_hat = rf_model.predict(X_test)\n",
    "\n",
    "# compute the mae\n",
    "mae = np.mean(np.abs(y_test - y_hat))\n",
    "\n",
    "# compute the mean percentage error\n",
    "percentage_error = np.mean(100*np.abs((y_test - y_hat) / y_test))\n",
    "\n",
    "print(mae)\n",
    "print(percentage_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "620590.9893821041\n",
      "13.228268150766247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/1003370021.py:5: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  model.fit(X_train, y_train)\n",
      "c:\\Users\\jakub\\anaconda\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:530: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\jakub\\anaconda\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 54890190761860.125, tolerance: 713662316497.8124\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "model = Lasso(alpha=0)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# get the predictions\n",
    "y_hat = model.predict(X_test)\n",
    "\n",
    "# compute the mae\n",
    "mae = np.mean(np.abs(y_test - y_hat))\n",
    "\n",
    "# compute the mean percentage error\n",
    "percentage_error = np.mean(100*np.abs((y_test - y_hat) / y_test))\n",
    "\n",
    "print(mae)\n",
    "print(percentage_error)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single network per region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "model = Sequential()\n",
    "# Resizing the images\n",
    "model.add(Resizing(300, 440, input_shape=(765, 1076, 1)))\n",
    "# Start with Convolutional layers\n",
    "model.add(Conv2D(16, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "# model.add(Conv2D(64, (3, 3), activation='relu'))  # Additional Conv layer\n",
    "# model.add(MaxPooling2D((2, 2)))\n",
    "# model.add(Conv2D(128, (3, 3), activation='relu'))  # Additional Conv layer\n",
    "# model.add(MaxPooling2D((2, 2)))\n",
    "# Flatten the results to feed into a dense layer\n",
    "model.add(Flatten())\n",
    "# Add dense layers (hidden layers)\n",
    "model.add(Dense(64, activation='relu'))  # Upscaled dense layer\n",
    "model.add(Dense(32, activation='relu'))   # Additional dense layer\n",
    "# Output layer\n",
    "model.add(Dense(1))\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(), loss='mean_absolute_error', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1/1 [==============================] - 2s 2s/step - loss: 37222.0117 - mae: 37222.0117 - val_loss: 20516.8477 - val_mae: 20516.8477\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 320ms/step - loss: 37163.3047 - mae: 37163.3047 - val_loss: 20474.7051 - val_mae: 20474.7051\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 335ms/step - loss: 37072.5625 - mae: 37072.5625 - val_loss: 20423.6094 - val_mae: 20423.6094\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 314ms/step - loss: 36961.8242 - mae: 36961.8242 - val_loss: 20362.8555 - val_mae: 20362.8555\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 318ms/step - loss: 36828.3750 - mae: 36828.3750 - val_loss: 20289.2520 - val_mae: 20289.2520\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 386ms/step - loss: 36664.7852 - mae: 36664.7852 - val_loss: 20201.1504 - val_mae: 20201.1504\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 357ms/step - loss: 36466.7031 - mae: 36466.7031 - val_loss: 20095.2207 - val_mae: 20095.2207\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 311ms/step - loss: 36226.4023 - mae: 36226.4023 - val_loss: 19969.9355 - val_mae: 19969.9355\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 330ms/step - loss: 35939.7227 - mae: 35939.7227 - val_loss: 19823.6328 - val_mae: 19823.6328\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 303ms/step - loss: 35602.6797 - mae: 35602.6797 - val_loss: 19651.6309 - val_mae: 19651.6309\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 317ms/step - loss: 35205.5000 - mae: 35205.5000 - val_loss: 19450.3789 - val_mae: 19450.3789\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 321ms/step - loss: 34737.8438 - mae: 34737.8438 - val_loss: 19216.8145 - val_mae: 19216.8145\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 296ms/step - loss: 34190.4805 - mae: 34190.4805 - val_loss: 18945.5000 - val_mae: 18945.5000\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 299ms/step - loss: 33551.1406 - mae: 33551.1406 - val_loss: 18631.6758 - val_mae: 18631.6758\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 297ms/step - loss: 32807.9336 - mae: 32807.9336 - val_loss: 18270.3281 - val_mae: 18270.3281\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 296ms/step - loss: 31948.0742 - mae: 31948.0742 - val_loss: 17855.7949 - val_mae: 17855.7949\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 305ms/step - loss: 30957.3301 - mae: 30957.3301 - val_loss: 17381.2188 - val_mae: 17381.2188\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 295ms/step - loss: 29820.1211 - mae: 29820.1211 - val_loss: 16838.2637 - val_mae: 16838.2637\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 296ms/step - loss: 28514.2500 - mae: 28514.2500 - val_loss: 16227.0234 - val_mae: 16227.0234\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 27036.9121 - mae: 27036.9121 - val_loss: 15529.7734 - val_mae: 15529.7734\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 293ms/step - loss: 25347.1523 - mae: 25347.1523 - val_loss: 14731.3984 - val_mae: 14731.3984\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 23407.1309 - mae: 23407.1309 - val_loss: 13827.8252 - val_mae: 13827.8252\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 21206.1484 - mae: 21206.1484 - val_loss: 12811.0186 - val_mae: 12811.0186\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 303ms/step - loss: 18720.6992 - mae: 18720.6992 - val_loss: 11663.7861 - val_mae: 11663.7861\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 15910.0205 - mae: 15910.0205 - val_loss: 10373.2617 - val_mae: 10373.2617\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 292ms/step - loss: 12740.1592 - mae: 12740.1592 - val_loss: 8933.6758 - val_mae: 8933.6758\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 285ms/step - loss: 9192.5840 - mae: 9192.5840 - val_loss: 7325.5195 - val_mae: 7325.5195\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 346ms/step - loss: 6113.3071 - mae: 6113.3071 - val_loss: 5581.7275 - val_mae: 5581.7275\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 4541.7197 - mae: 4541.7197 - val_loss: 3763.3911 - val_mae: 3763.3911\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 4311.8926 - mae: 4311.8926 - val_loss: 2142.8459 - val_mae: 2142.8459\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 292ms/step - loss: 7570.7231 - mae: 7570.7231 - val_loss: 3137.9639 - val_mae: 3137.9639\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 9485.7402 - mae: 9485.7402 - val_loss: 3313.5349 - val_mae: 3313.5349\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 9819.3916 - mae: 9819.3916 - val_loss: 2878.2263 - val_mae: 2878.2263\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 8975.8965 - mae: 8975.8965 - val_loss: 2212.5864 - val_mae: 2212.5864\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 297ms/step - loss: 7282.5757 - mae: 7282.5757 - val_loss: 3123.5435 - val_mae: 3123.5435\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 5080.4053 - mae: 5080.4053 - val_loss: 4137.3574 - val_mae: 4137.3574\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s 298ms/step - loss: 3943.2495 - mae: 3943.2495 - val_loss: 5091.9980 - val_mae: 5091.9980\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 4270.2988 - mae: 4270.2988 - val_loss: 5867.2529 - val_mae: 5867.2529\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 4622.9482 - mae: 4622.9482 - val_loss: 6494.4292 - val_mae: 6494.4292\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 0s 292ms/step - loss: 5231.6016 - mae: 5231.6016 - val_loss: 6934.6328 - val_mae: 6934.6328\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 5717.1309 - mae: 5717.1309 - val_loss: 7219.1646 - val_mae: 7219.1646\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 6029.9580 - mae: 6029.9580 - val_loss: 7370.8369 - val_mae: 7370.8369\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 6239.7012 - mae: 6239.7012 - val_loss: 7346.9717 - val_mae: 7346.9717\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 6212.5791 - mae: 6212.5791 - val_loss: 7166.9053 - val_mae: 7166.9053\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 5968.5986 - mae: 5968.5986 - val_loss: 6900.2017 - val_mae: 6900.2017\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 5672.8672 - mae: 5672.8672 - val_loss: 6549.7178 - val_mae: 6549.7178\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - 0s 323ms/step - loss: 5284.0044 - mae: 5284.0044 - val_loss: 6116.5337 - val_mae: 6116.5337\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - 0s 386ms/step - loss: 4802.9932 - mae: 4802.9932 - val_loss: 5600.0181 - val_mae: 5600.0181\n",
      "Epoch 49/50\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 4342.4414 - mae: 4342.4414 - val_loss: 5064.4360 - val_mae: 5064.4360\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 4068.6265 - mae: 4068.6265 - val_loss: 4507.7188 - val_mae: 4507.7188\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "Predicted GDP for Vinnytsia_Oblast in 2021: [43416.574]\n",
      "Actual GDP for Vinnytsia_Oblast in 2021: 43360.0\n",
      "Epoch 1/50\n",
      "1/1 [==============================] - 0s 363ms/step - loss: 1991.1765 - mae: 1991.1765 - val_loss: 2870.4482 - val_mae: 2870.4482\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 1767.4421 - mae: 1767.4421 - val_loss: 2502.7769 - val_mae: 2502.7769\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 1901.6472 - mae: 1901.6472 - val_loss: 2261.9272 - val_mae: 2261.9272\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 322ms/step - loss: 2058.2444 - mae: 2058.2444 - val_loss: 2216.6099 - val_mae: 2216.6099\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 293ms/step - loss: 2094.2217 - mae: 2094.2217 - val_loss: 2301.4688 - val_mae: 2301.4688\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 319ms/step - loss: 1964.1257 - mae: 1964.1257 - val_loss: 2478.8997 - val_mae: 2478.8997\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 322ms/step - loss: 1774.7627 - mae: 1774.7627 - val_loss: 2663.2478 - val_mae: 2663.2478\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 319ms/step - loss: 1619.6047 - mae: 1619.6047 - val_loss: 2870.6445 - val_mae: 2870.6445\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 317ms/step - loss: 1499.4629 - mae: 1499.4629 - val_loss: 3056.3022 - val_mae: 3056.3022\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 297ms/step - loss: 1560.7324 - mae: 1560.7324 - val_loss: 3117.2295 - val_mae: 3117.2295\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 1615.8738 - mae: 1615.8738 - val_loss: 3079.4468 - val_mae: 3079.4468\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 1554.8281 - mae: 1554.8281 - val_loss: 2959.6460 - val_mae: 2959.6460\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 1457.9502 - mae: 1457.9502 - val_loss: 2849.2344 - val_mae: 2849.2344\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 1448.2788 - mae: 1448.2788 - val_loss: 2747.4939 - val_mae: 2747.4939\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 298ms/step - loss: 1436.0374 - mae: 1436.0374 - val_loss: 2653.7231 - val_mae: 2653.7231\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 329ms/step - loss: 1421.4565 - mae: 1421.4565 - val_loss: 2567.2683 - val_mae: 2567.2683\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 297ms/step - loss: 1405.9441 - mae: 1405.9441 - val_loss: 2541.6494 - val_mae: 2541.6494\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 311ms/step - loss: 1408.8359 - mae: 1408.8359 - val_loss: 2570.6462 - val_mae: 2570.6462\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 316ms/step - loss: 1374.0195 - mae: 1374.0195 - val_loss: 2648.1064 - val_mae: 2648.1064\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 312ms/step - loss: 1346.9731 - mae: 1346.9731 - val_loss: 2717.0437 - val_mae: 2717.0437\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 299ms/step - loss: 1325.3030 - mae: 1325.3030 - val_loss: 2778.2551 - val_mae: 2778.2551\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 297ms/step - loss: 1302.4277 - mae: 1302.4277 - val_loss: 2832.4458 - val_mae: 2832.4458\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 1280.7913 - mae: 1280.7913 - val_loss: 2835.3608 - val_mae: 2835.3608\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 1291.0798 - mae: 1291.0798 - val_loss: 2750.8516 - val_mae: 2750.8516\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 1235.8035 - mae: 1235.8035 - val_loss: 2672.9595 - val_mae: 2672.9595\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 1214.4106 - mae: 1214.4106 - val_loss: 2601.0759 - val_mae: 2601.0759\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 1193.7773 - mae: 1193.7773 - val_loss: 2587.7673 - val_mae: 2587.7673\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 306ms/step - loss: 1179.9233 - mae: 1179.9233 - val_loss: 2626.9448 - val_mae: 2626.9448\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 309ms/step - loss: 1144.5684 - mae: 1144.5684 - val_loss: 2661.0825 - val_mae: 2661.0825\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 1122.2732 - mae: 1122.2732 - val_loss: 2645.9480 - val_mae: 2645.9480\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 295ms/step - loss: 1105.3357 - mae: 1105.3357 - val_loss: 2586.4797 - val_mae: 2586.4797\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 335ms/step - loss: 1087.6323 - mae: 1087.6323 - val_loss: 2583.9548 - val_mae: 2583.9548\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 330ms/step - loss: 1067.7830 - mae: 1067.7830 - val_loss: 2587.7200 - val_mae: 2587.7200\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 304ms/step - loss: 1060.1440 - mae: 1060.1440 - val_loss: 2552.7014 - val_mae: 2552.7014\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 1054.3169 - mae: 1054.3169 - val_loss: 2527.0168 - val_mae: 2527.0168\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 1049.2805 - mae: 1049.2805 - val_loss: 2555.6221 - val_mae: 2555.6221\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s 293ms/step - loss: 1038.3923 - mae: 1038.3923 - val_loss: 2587.5149 - val_mae: 2587.5149\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s 307ms/step - loss: 1045.3850 - mae: 1045.3850 - val_loss: 2577.3738 - val_mae: 2577.3738\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 0s 327ms/step - loss: 1030.1067 - mae: 1030.1067 - val_loss: 2529.2227 - val_mae: 2529.2227\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 0s 280ms/step - loss: 1016.8906 - mae: 1016.8906 - val_loss: 2491.5767 - val_mae: 2491.5767\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 0s 285ms/step - loss: 1011.7266 - mae: 1011.7266 - val_loss: 2463.5300 - val_mae: 2463.5300\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 0s 306ms/step - loss: 1012.3552 - mae: 1012.3552 - val_loss: 2491.1543 - val_mae: 2491.1543\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 995.9009 - mae: 995.9009 - val_loss: 2522.1709 - val_mae: 2522.1709\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 985.7209 - mae: 985.7209 - val_loss: 2556.2703 - val_mae: 2556.2703\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 0s 295ms/step - loss: 975.2148 - mae: 975.2148 - val_loss: 2593.1333 - val_mae: 2593.1333\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 0s 280ms/step - loss: 985.4949 - mae: 985.4949 - val_loss: 2583.6887 - val_mae: 2583.6887\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - 0s 281ms/step - loss: 971.6326 - mae: 971.6326 - val_loss: 2532.4106 - val_mae: 2532.4106\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 951.9175 - mae: 951.9175 - val_loss: 2491.9351 - val_mae: 2491.9351\n",
      "Epoch 49/50\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 946.1926 - mae: 946.1926 - val_loss: 2461.3137 - val_mae: 2461.3137\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 950.7764 - mae: 950.7764 - val_loss: 2488.1511 - val_mae: 2488.1511\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Predicted GDP for Volyn_Oblast in 2021: [25596.617]\n",
      "Actual GDP for Volyn_Oblast in 2021: 19152.0\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_18280/568650546.py:77: RuntimeWarning: overflow encountered in cast\n",
      "  y = y.astype(np.float16)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_18280/568650546.py:79: RuntimeWarning: overflow encountered in cast\n",
      "  y_test = y_test.astype(np.float16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 439ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 287ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 285ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 288ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 296ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 282ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 288ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 291ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 322ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 303ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 323ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 289ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 297ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 293ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 292ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 291ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 292ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 297ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 302ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 292ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 313ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 335ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 293ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 300ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 299ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 293ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 294ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 297ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 295ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 289ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 286ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 282ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 285ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 283ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 299ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s 288ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s 291ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s 284ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 0s 287ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 0s 293ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 0s 290ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 0s 282ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 0s 286ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 0s 289ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 0s 282ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 0s 335ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - 0s 282ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - 0s 286ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 49/50\n",
      "1/1 [==============================] - 0s 288ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - 0s 280ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Predicted GDP for Dnipropetrovsk_Oblast in 2021: [49831784.]\n",
      "Actual GDP for Dnipropetrovsk_Oblast in 2021: inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_18280/568650546.py:92: RuntimeWarning: invalid value encountered in divide\n",
      "  percentage_error = np.mean(100*np.abs((y_test - y_hat) / y_test))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1/1 [==============================] - 0s 376ms/step - loss: inf - mae: inf - val_loss: 15289260.0000 - val_mae: 15289260.0000\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 294ms/step - loss: inf - mae: inf - val_loss: 16316784.0000 - val_mae: 16316784.0000\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 287ms/step - loss: inf - mae: inf - val_loss: 17361300.0000 - val_mae: 17361300.0000\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 294ms/step - loss: inf - mae: inf - val_loss: 18424332.0000 - val_mae: 18424332.0000\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 288ms/step - loss: inf - mae: inf - val_loss: 19507770.0000 - val_mae: 19507770.0000\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 284ms/step - loss: inf - mae: inf - val_loss: 20613842.0000 - val_mae: 20613842.0000\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 297ms/step - loss: inf - mae: inf - val_loss: 21745172.0000 - val_mae: 21745172.0000\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 283ms/step - loss: inf - mae: inf - val_loss: 22904702.0000 - val_mae: 22904702.0000\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 298ms/step - loss: inf - mae: inf - val_loss: 24095560.0000 - val_mae: 24095560.0000\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 293ms/step - loss: inf - mae: inf - val_loss: 25321084.0000 - val_mae: 25321084.0000\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 299ms/step - loss: inf - mae: inf - val_loss: 26584792.0000 - val_mae: 26584792.0000\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 351ms/step - loss: inf - mae: inf - val_loss: 27890342.0000 - val_mae: 27890342.0000\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 286ms/step - loss: inf - mae: inf - val_loss: 29241516.0000 - val_mae: 29241516.0000\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 289ms/step - loss: inf - mae: inf - val_loss: 30642324.0000 - val_mae: 30642324.0000\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 292ms/step - loss: inf - mae: inf - val_loss: 32096776.0000 - val_mae: 32096776.0000\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 291ms/step - loss: inf - mae: inf - val_loss: 33609048.0000 - val_mae: 33609048.0000\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 286ms/step - loss: inf - mae: inf - val_loss: 35183472.0000 - val_mae: 35183472.0000\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 290ms/step - loss: inf - mae: inf - val_loss: 36824404.0000 - val_mae: 36824404.0000\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 281ms/step - loss: inf - mae: inf - val_loss: 38536392.0000 - val_mae: 38536392.0000\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 281ms/step - loss: inf - mae: inf - val_loss: 40323948.0000 - val_mae: 40323948.0000\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 286ms/step - loss: inf - mae: inf - val_loss: 42191732.0000 - val_mae: 42191732.0000\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 281ms/step - loss: inf - mae: inf - val_loss: 44144576.0000 - val_mae: 44144576.0000\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 284ms/step - loss: inf - mae: inf - val_loss: 46187424.0000 - val_mae: 46187424.0000\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 286ms/step - loss: inf - mae: inf - val_loss: 48325332.0000 - val_mae: 48325332.0000\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 290ms/step - loss: inf - mae: inf - val_loss: 50563524.0000 - val_mae: 50563524.0000\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 282ms/step - loss: inf - mae: inf - val_loss: 52907260.0000 - val_mae: 52907260.0000\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 318ms/step - loss: inf - mae: inf - val_loss: 55362084.0000 - val_mae: 55362084.0000\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 315ms/step - loss: inf - mae: inf - val_loss: 57933396.0000 - val_mae: 57933396.0000\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 290ms/step - loss: inf - mae: inf - val_loss: 60626812.0000 - val_mae: 60626812.0000\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 292ms/step - loss: inf - mae: inf - val_loss: 63448140.0000 - val_mae: 63448140.0000\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 299ms/step - loss: inf - mae: inf - val_loss: 66403236.0000 - val_mae: 66403236.0000\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 292ms/step - loss: inf - mae: inf - val_loss: 69498144.0000 - val_mae: 69498144.0000\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 308ms/step - loss: inf - mae: inf - val_loss: 72738968.0000 - val_mae: 72738968.0000\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 313ms/step - loss: inf - mae: inf - val_loss: 76132016.0000 - val_mae: 76132016.0000\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 301ms/step - loss: inf - mae: inf - val_loss: 79683800.0000 - val_mae: 79683800.0000\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s 374ms/step - loss: inf - mae: inf - val_loss: 83400784.0000 - val_mae: 83400784.0000\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s 322ms/step - loss: inf - mae: inf - val_loss: 87289712.0000 - val_mae: 87289712.0000\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s 298ms/step - loss: inf - mae: inf - val_loss: 91357448.0000 - val_mae: 91357448.0000\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 0s 296ms/step - loss: inf - mae: inf - val_loss: 95611000.0000 - val_mae: 95611000.0000\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 0s 288ms/step - loss: inf - mae: inf - val_loss: 100057536.0000 - val_mae: 100057536.0000\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 0s 291ms/step - loss: inf - mae: inf - val_loss: 104704280.0000 - val_mae: 104704280.0000\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 0s 286ms/step - loss: inf - mae: inf - val_loss: 109558608.0000 - val_mae: 109558608.0000\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 0s 293ms/step - loss: inf - mae: inf - val_loss: 114628064.0000 - val_mae: 114628064.0000\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 0s 293ms/step - loss: inf - mae: inf - val_loss: 119920416.0000 - val_mae: 119920416.0000\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 0s 288ms/step - loss: inf - mae: inf - val_loss: 125443520.0000 - val_mae: 125443520.0000\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 0s 292ms/step - loss: inf - mae: inf - val_loss: 131214216.0000 - val_mae: 131214216.0000\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - 0s 300ms/step - loss: inf - mae: inf - val_loss: 137236992.0000 - val_mae: 137236992.0000\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - 0s 311ms/step - loss: inf - mae: inf - val_loss: 143519648.0000 - val_mae: 143519648.0000\n",
      "Epoch 49/50\n",
      "1/1 [==============================] - 0s 308ms/step - loss: inf - mae: inf - val_loss: 150069360.0000 - val_mae: 150069360.0000\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - 0s 302ms/step - loss: inf - mae: inf - val_loss: 156895552.0000 - val_mae: 156895552.0000\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Predicted GDP for Donetsk_Oblast in 2021: [3.258329e+08]\n",
      "Actual GDP for Donetsk_Oblast in 2021: 64000.0\n",
      "Epoch 1/50\n",
      "1/1 [==============================] - 0s 376ms/step - loss: 101191240.0000 - mae: 101191240.0000 - val_loss: 52013020.0000 - val_mae: 52013020.0000\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 295ms/step - loss: 103739824.0000 - mae: 103739824.0000 - val_loss: 52769452.0000 - val_mae: 52769452.0000\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 105294304.0000 - mae: 105294304.0000 - val_loss: 53100176.0000 - val_mae: 53100176.0000\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 293ms/step - loss: 106025072.0000 - mae: 106025072.0000 - val_loss: 53075976.0000 - val_mae: 53075976.0000\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 342ms/step - loss: 106058896.0000 - mae: 106058896.0000 - val_loss: 52754768.0000 - val_mae: 52754768.0000\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 105504384.0000 - mae: 105504384.0000 - val_loss: 52186868.0000 - val_mae: 52186868.0000\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 296ms/step - loss: 104457128.0000 - mae: 104457128.0000 - val_loss: 51416464.0000 - val_mae: 51416464.0000\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 103002192.0000 - mae: 103002192.0000 - val_loss: 50482324.0000 - val_mae: 50482324.0000\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 101215040.0000 - mae: 101215040.0000 - val_loss: 49417992.0000 - val_mae: 49417992.0000\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 99162152.0000 - mae: 99162152.0000 - val_loss: 48252856.0000 - val_mae: 48252856.0000\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 96901648.0000 - mae: 96901648.0000 - val_loss: 47012496.0000 - val_mae: 47012496.0000\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 94483968.0000 - mae: 94483968.0000 - val_loss: 45721392.0000 - val_mae: 45721392.0000\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 91959536.0000 - mae: 91959536.0000 - val_loss: 44396348.0000 - val_mae: 44396348.0000\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 292ms/step - loss: 89359056.0000 - mae: 89359056.0000 - val_loss: 43051416.0000 - val_mae: 43051416.0000\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 86712216.0000 - mae: 86712216.0000 - val_loss: 41699436.0000 - val_mae: 41699436.0000\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 296ms/step - loss: 84045152.0000 - mae: 84045152.0000 - val_loss: 40351084.0000 - val_mae: 40351084.0000\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 81379664.0000 - mae: 81379664.0000 - val_loss: 39015108.0000 - val_mae: 39015108.0000\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 330ms/step - loss: 78733664.0000 - mae: 78733664.0000 - val_loss: 37698544.0000 - val_mae: 37698544.0000\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 298ms/step - loss: 76121664.0000 - mae: 76121664.0000 - val_loss: 36407032.0000 - val_mae: 36407032.0000\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 293ms/step - loss: 73555312.0000 - mae: 73555312.0000 - val_loss: 35144944.0000 - val_mae: 35144944.0000\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 71043840.0000 - mae: 71043840.0000 - val_loss: 33915636.0000 - val_mae: 33915636.0000\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 306ms/step - loss: 68594264.0000 - mae: 68594264.0000 - val_loss: 32721544.0000 - val_mae: 32721544.0000\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 66211864.0000 - mae: 66211864.0000 - val_loss: 31564372.0000 - val_mae: 31564372.0000\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 296ms/step - loss: 63900368.0000 - mae: 63900368.0000 - val_loss: 30444976.0000 - val_mae: 30444976.0000\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 61661600.0000 - mae: 61661600.0000 - val_loss: 29359288.0000 - val_mae: 29359288.0000\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 59487904.0000 - mae: 59487904.0000 - val_loss: 28310864.0000 - val_mae: 28310864.0000\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 57386608.0000 - mae: 57386608.0000 - val_loss: 27300100.0000 - val_mae: 27300100.0000\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 55358732.0000 - mae: 55358732.0000 - val_loss: 26326832.0000 - val_mae: 26326832.0000\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 53404152.0000 - mae: 53404152.0000 - val_loss: 25390560.0000 - val_mae: 25390560.0000\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 51522076.0000 - mae: 51522076.0000 - val_loss: 24490540.0000 - val_mae: 24490540.0000\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 49711232.0000 - mae: 49711232.0000 - val_loss: 23625880.0000 - val_mae: 23625880.0000\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 47969984.0000 - mae: 47969984.0000 - val_loss: 22795566.0000 - val_mae: 22795566.0000\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 46296428.0000 - mae: 46296428.0000 - val_loss: 21998486.0000 - val_mae: 21998486.0000\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 304ms/step - loss: 44688472.0000 - mae: 44688472.0000 - val_loss: 21233486.0000 - val_mae: 21233486.0000\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 43144008.0000 - mae: 43144008.0000 - val_loss: 20499388.0000 - val_mae: 20499388.0000\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s 306ms/step - loss: 41660552.0000 - mae: 41660552.0000 - val_loss: 19794952.0000 - val_mae: 19794952.0000\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s 347ms/step - loss: 40235936.0000 - mae: 40235936.0000 - val_loss: 19118968.0000 - val_mae: 19118968.0000\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 38867740.0000 - mae: 38867740.0000 - val_loss: 18470230.0000 - val_mae: 18470230.0000\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 37553616.0000 - mae: 37553616.0000 - val_loss: 17847548.0000 - val_mae: 17847548.0000\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 36291256.0000 - mae: 36291256.0000 - val_loss: 17249752.0000 - val_mae: 17249752.0000\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 35078376.0000 - mae: 35078376.0000 - val_loss: 16675718.0000 - val_mae: 16675718.0000\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 33912784.0000 - mae: 33912784.0000 - val_loss: 16124334.0000 - val_mae: 16124334.0000\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 32792156.0000 - mae: 32792156.0000 - val_loss: 15593993.0000 - val_mae: 15593993.0000\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 31713100.0000 - mae: 31713100.0000 - val_loss: 15083369.0000 - val_mae: 15083369.0000\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 0s 297ms/step - loss: 30673272.0000 - mae: 30673272.0000 - val_loss: 14591820.0000 - val_mae: 14591820.0000\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 29671750.0000 - mae: 29671750.0000 - val_loss: 14118533.0000 - val_mae: 14118533.0000\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 28706788.0000 - mae: 28706788.0000 - val_loss: 13662720.0000 - val_mae: 13662720.0000\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 27776696.0000 - mae: 27776696.0000 - val_loss: 13223548.0000 - val_mae: 13223548.0000\n",
      "Epoch 49/50\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 26879808.0000 - mae: 26879808.0000 - val_loss: 12800205.0000 - val_mae: 12800205.0000\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - 0s 281ms/step - loss: 26014540.0000 - mae: 26014540.0000 - val_loss: 12391933.0000 - val_mae: 12391933.0000\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Predicted GDP for Zhytomyr_Oblast in 2021: [24858146.]\n",
      "Actual GDP for Zhytomyr_Oblast in 2021: 30064.0\n",
      "Epoch 1/50\n",
      "1/1 [==============================] - 0s 369ms/step - loss: 28232784.0000 - mae: 28232784.0000 - val_loss: 14755021.0000 - val_mae: 14755021.0000\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 298ms/step - loss: 27453112.0000 - mae: 27453112.0000 - val_loss: 14309157.0000 - val_mae: 14309157.0000\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 322ms/step - loss: 26639224.0000 - mae: 26639224.0000 - val_loss: 13856457.0000 - val_mae: 13856457.0000\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 366ms/step - loss: 25808852.0000 - mae: 25808852.0000 - val_loss: 13402368.0000 - val_mae: 13402368.0000\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 298ms/step - loss: 24972856.0000 - mae: 24972856.0000 - val_loss: 12950545.0000 - val_mae: 12950545.0000\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 317ms/step - loss: 24138588.0000 - mae: 24138588.0000 - val_loss: 12503566.0000 - val_mae: 12503566.0000\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 23311258.0000 - mae: 23311258.0000 - val_loss: 12063287.0000 - val_mae: 12063287.0000\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 22494664.0000 - mae: 22494664.0000 - val_loss: 11631019.0000 - val_mae: 11631019.0000\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 21691538.0000 - mae: 21691538.0000 - val_loss: 11207716.0000 - val_mae: 11207716.0000\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 20903896.0000 - mae: 20903896.0000 - val_loss: 10794034.0000 - val_mae: 10794034.0000\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 306ms/step - loss: 20133146.0000 - mae: 20133146.0000 - val_loss: 10390413.0000 - val_mae: 10390413.0000\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 323ms/step - loss: 19380256.0000 - mae: 19380256.0000 - val_loss: 9996824.0000 - val_mae: 9996824.0000\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 308ms/step - loss: 18645650.0000 - mae: 18645650.0000 - val_loss: 9610186.0000 - val_mae: 9610186.0000\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 310ms/step - loss: 17922404.0000 - mae: 17922404.0000 - val_loss: 9232618.0000 - val_mae: 9232618.0000\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 316ms/step - loss: 17216082.0000 - mae: 17216082.0000 - val_loss: 8864741.0000 - val_mae: 8864741.0000\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 355ms/step - loss: 16527356.0000 - mae: 16527356.0000 - val_loss: 8506660.0000 - val_mae: 8506660.0000\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 318ms/step - loss: 15856496.0000 - mae: 15856496.0000 - val_loss: 8158342.5000 - val_mae: 8158342.5000\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 324ms/step - loss: 15203502.0000 - mae: 15203502.0000 - val_loss: 7819668.0000 - val_mae: 7819668.0000\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 309ms/step - loss: 14568198.0000 - mae: 14568198.0000 - val_loss: 7490470.5000 - val_mae: 7490470.5000\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 310ms/step - loss: 13950316.0000 - mae: 13950316.0000 - val_loss: 7170531.0000 - val_mae: 7170531.0000\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 329ms/step - loss: 13349484.0000 - mae: 13349484.0000 - val_loss: 6859611.0000 - val_mae: 6859611.0000\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 297ms/step - loss: 12765284.0000 - mae: 12765284.0000 - val_loss: 6557442.0000 - val_mae: 6557442.0000\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 339ms/step - loss: 12197242.0000 - mae: 12197242.0000 - val_loss: 6263748.5000 - val_mae: 6263748.5000\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 325ms/step - loss: 11644868.0000 - mae: 11644868.0000 - val_loss: 5978245.0000 - val_mae: 5978245.0000\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 304ms/step - loss: 11107648.0000 - mae: 11107648.0000 - val_loss: 5700644.5000 - val_mae: 5700644.5000\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 308ms/step - loss: 10585062.0000 - mae: 10585062.0000 - val_loss: 5430644.0000 - val_mae: 5430644.0000\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 328ms/step - loss: 10076556.0000 - mae: 10076556.0000 - val_loss: 5167967.0000 - val_mae: 5167967.0000\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 320ms/step - loss: 9581619.0000 - mae: 9581619.0000 - val_loss: 4912321.5000 - val_mae: 4912321.5000\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 313ms/step - loss: 9099720.0000 - mae: 9099720.0000 - val_loss: 4663418.0000 - val_mae: 4663418.0000\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 312ms/step - loss: 8630320.0000 - mae: 8630320.0000 - val_loss: 4420981.0000 - val_mae: 4420981.0000\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 333ms/step - loss: 8172915.0000 - mae: 8172915.0000 - val_loss: 4184743.7500 - val_mae: 4184743.7500\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 319ms/step - loss: 7727003.0000 - mae: 7727003.0000 - val_loss: 3955791.0000 - val_mae: 3955791.0000\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 377ms/step - loss: 7292134.5000 - mae: 7292134.5000 - val_loss: 3733244.5000 - val_mae: 3733244.5000\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 332ms/step - loss: 6868007.0000 - mae: 6868007.0000 - val_loss: 3515697.5000 - val_mae: 3515697.5000\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 316ms/step - loss: 6454749.0000 - mae: 6454749.0000 - val_loss: 3303355.0000 - val_mae: 3303355.0000\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s 313ms/step - loss: 6051904.0000 - mae: 6051904.0000 - val_loss: 3095976.5000 - val_mae: 3095976.5000\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s 329ms/step - loss: 5659056.0000 - mae: 5659056.0000 - val_loss: 2892580.7500 - val_mae: 2892580.7500\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s 329ms/step - loss: 5275316.5000 - mae: 5275316.5000 - val_loss: 2691855.7500 - val_mae: 2691855.7500\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 0s 316ms/step - loss: 4899409.5000 - mae: 4899409.5000 - val_loss: 2491633.0000 - val_mae: 2491633.0000\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 0s 297ms/step - loss: 4522789.0000 - mae: 4522789.0000 - val_loss: 2294225.2500 - val_mae: 2294225.2500\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 0s 305ms/step - loss: 4147018.7500 - mae: 4147018.7500 - val_loss: 2099527.5000 - val_mae: 2099527.5000\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 0s 320ms/step - loss: 3776034.0000 - mae: 3776034.0000 - val_loss: 1907492.5000 - val_mae: 1907492.5000\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 0s 303ms/step - loss: 3410174.0000 - mae: 3410174.0000 - val_loss: 1718143.5000 - val_mae: 1718143.5000\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 0s 307ms/step - loss: 3049474.0000 - mae: 3049474.0000 - val_loss: 1531713.7500 - val_mae: 1531713.7500\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 2693910.0000 - mae: 2693910.0000 - val_loss: 1348084.2500 - val_mae: 1348084.2500\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 0s 307ms/step - loss: 2343371.2500 - mae: 2343371.2500 - val_loss: 1166321.5000 - val_mae: 1166321.5000\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - 0s 336ms/step - loss: 1997737.6250 - mae: 1997737.6250 - val_loss: 987201.2500 - val_mae: 987201.2500\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - 0s 317ms/step - loss: 1656845.0000 - mae: 1656845.0000 - val_loss: 810510.0625 - val_mae: 810510.0625\n",
      "Epoch 49/50\n",
      "1/1 [==============================] - 0s 299ms/step - loss: 1320548.8750 - mae: 1320548.8750 - val_loss: 636165.2500 - val_mae: 636165.2500\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - 0s 350ms/step - loss: 988638.3750 - mae: 988638.3750 - val_loss: 473878.1250 - val_mae: 473878.1250\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "Predicted GDP for Zakarpattia_Oblast in 2021: [249348.78]\n",
      "Actual GDP for Zakarpattia_Oblast in 2021: 20976.0\n",
      "Epoch 1/50\n",
      "1/1 [==============================] - 0s 352ms/step - loss: 26754696.0000 - mae: 26754696.0000 - val_loss: 11016536.0000 - val_mae: 11016536.0000\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 25900966.0000 - mae: 25900966.0000 - val_loss: 10565768.0000 - val_mae: 10565768.0000\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 24878228.0000 - mae: 24878228.0000 - val_loss: 10063759.0000 - val_mae: 10063759.0000\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 23738044.0000 - mae: 23738044.0000 - val_loss: 9525792.0000 - val_mae: 9525792.0000\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 22514760.0000 - mae: 22514760.0000 - val_loss: 8963184.0000 - val_mae: 8963184.0000\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 311ms/step - loss: 21233988.0000 - mae: 21233988.0000 - val_loss: 8384725.0000 - val_mae: 8384725.0000\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 305ms/step - loss: 19915728.0000 - mae: 19915728.0000 - val_loss: 7797363.0000 - val_mae: 7797363.0000\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 18575892.0000 - mae: 18575892.0000 - val_loss: 7206668.0000 - val_mae: 7206668.0000\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 17227204.0000 - mae: 17227204.0000 - val_loss: 6617064.0000 - val_mae: 6617064.0000\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 15879886.0000 - mae: 15879886.0000 - val_loss: 6032079.5000 - val_mae: 6032079.5000\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 14542086.0000 - mae: 14542086.0000 - val_loss: 5454470.0000 - val_mae: 5454470.0000\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 13220168.0000 - mae: 13220168.0000 - val_loss: 4886356.5000 - val_mae: 4886356.5000\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 329ms/step - loss: 11919102.0000 - mae: 11919102.0000 - val_loss: 4329339.5000 - val_mae: 4329339.5000\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 10642628.0000 - mae: 10642628.0000 - val_loss: 3784567.0000 - val_mae: 3784567.0000\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 9393466.0000 - mae: 9393466.0000 - val_loss: 3252827.2500 - val_mae: 3252827.2500\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 8173488.5000 - mae: 8173488.5000 - val_loss: 2734592.7500 - val_mae: 2734592.7500\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 281ms/step - loss: 6983874.0000 - mae: 6983874.0000 - val_loss: 2230097.5000 - val_mae: 2230097.5000\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 5825209.5000 - mae: 5825209.5000 - val_loss: 1739335.5000 - val_mae: 1739335.5000\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 4697563.0000 - mae: 4697563.0000 - val_loss: 1262173.5000 - val_mae: 1262173.5000\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 3600680.7500 - mae: 3600680.7500 - val_loss: 798309.9375 - val_mae: 798309.9375\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 281ms/step - loss: 2533926.5000 - mae: 2533926.5000 - val_loss: 347334.7500 - val_mae: 347334.7500\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 285ms/step - loss: 1496427.5000 - mae: 1496427.5000 - val_loss: 162827.9688 - val_mae: 162827.9688\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 487134.5938 - mae: 487134.5938 - val_loss: 557709.3125 - val_mae: 557709.3125\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 307ms/step - loss: 495183.2500 - mae: 495183.2500 - val_loss: 864724.1250 - val_mae: 864724.1250\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 1260427.7500 - mae: 1260427.7500 - val_loss: 1100361.8750 - val_mae: 1100361.8750\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 292ms/step - loss: 1839314.2500 - mae: 1839314.2500 - val_loss: 1280627.3750 - val_mae: 1280627.3750\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 279ms/step - loss: 2257786.7500 - mae: 2257786.7500 - val_loss: 1400938.2500 - val_mae: 1400938.2500\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 280ms/step - loss: 2537811.0000 - mae: 2537811.0000 - val_loss: 1469382.2500 - val_mae: 1469382.2500\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 2698057.0000 - mae: 2698057.0000 - val_loss: 1492854.7500 - val_mae: 1492854.7500\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 321ms/step - loss: 2754343.5000 - mae: 2754343.5000 - val_loss: 1477279.3750 - val_mae: 1477279.3750\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 2720458.5000 - mae: 2720458.5000 - val_loss: 1427695.6250 - val_mae: 1427695.6250\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 2608154.5000 - mae: 2608154.5000 - val_loss: 1348448.7500 - val_mae: 1348448.7500\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 2427461.0000 - mae: 2427461.0000 - val_loss: 1243304.7500 - val_mae: 1243304.7500\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 298ms/step - loss: 2187053.5000 - mae: 2187053.5000 - val_loss: 1115459.7500 - val_mae: 1115459.7500\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 1894315.0000 - mae: 1894315.0000 - val_loss: 972195.6250 - val_mae: 972195.6250\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 1555666.2500 - mae: 1555666.2500 - val_loss: 819585.0625 - val_mae: 819585.0625\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s 298ms/step - loss: 1176578.6250 - mae: 1176578.6250 - val_loss: 652666.8750 - val_mae: 652666.8750\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 761737.8750 - mae: 761737.8750 - val_loss: 473052.7188 - val_mae: 473052.7188\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 315145.8750 - mae: 315145.8750 - val_loss: 282138.7500 - val_mae: 282138.7500\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 223530.4375 - mae: 223530.4375 - val_loss: 110829.7344 - val_mae: 110829.7344\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 0s 285ms/step - loss: 584764.8125 - mae: 584764.8125 - val_loss: 97298.0469 - val_mae: 97298.0469\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 888116.1250 - mae: 888116.1250 - val_loss: 182602.6875 - val_mae: 182602.6875\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 1083063.6250 - mae: 1083063.6250 - val_loss: 226053.3750 - val_mae: 226053.3750\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 1181710.3750 - mae: 1181710.3750 - val_loss: 232370.6719 - val_mae: 232370.6719\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 1194951.7500 - mae: 1194951.7500 - val_loss: 205880.7500 - val_mae: 205880.7500\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 0s 318ms/step - loss: 1132694.1250 - mae: 1132694.1250 - val_loss: 150418.8594 - val_mae: 150418.8594\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - 0s 285ms/step - loss: 1003784.0000 - mae: 1003784.0000 - val_loss: 69479.8672 - val_mae: 69479.8672\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 816250.3750 - mae: 816250.3750 - val_loss: 109233.9375 - val_mae: 109233.9375\n",
      "Epoch 49/50\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 577338.6250 - mae: 577338.6250 - val_loss: 222564.7812 - val_mae: 222564.7812\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - 0s 299ms/step - loss: 293680.0625 - mae: 293680.0625 - val_loss: 351553.1250 - val_mae: 351553.1250\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "Predicted GDP for Zaporizhia_Oblast in 2021: [-717731.8]\n",
      "Actual GDP for Zaporizhia_Oblast in 2021: 54816.0\n",
      "Epoch 1/50\n",
      "1/1 [==============================] - 0s 359ms/step - loss: 5044528.0000 - mae: 5044528.0000 - val_loss: 2243077.7500 - val_mae: 2243077.7500\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 4868808.5000 - mae: 4868808.5000 - val_loss: 2140055.5000 - val_mae: 2140055.5000\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 4663294.5000 - mae: 4663294.5000 - val_loss: 2025897.7500 - val_mae: 2025897.7500\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 4435849.0000 - mae: 4435849.0000 - val_loss: 1903273.6250 - val_mae: 1903273.6250\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 310ms/step - loss: 4191682.2500 - mae: 4191682.2500 - val_loss: 1780737.1250 - val_mae: 1780737.1250\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 321ms/step - loss: 3934652.5000 - mae: 3934652.5000 - val_loss: 1660042.2500 - val_mae: 1660042.2500\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 3667778.5000 - mae: 3667778.5000 - val_loss: 1535855.1250 - val_mae: 1535855.1250\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 309ms/step - loss: 3393469.2500 - mae: 3393469.2500 - val_loss: 1409143.1250 - val_mae: 1409143.1250\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 313ms/step - loss: 3113702.0000 - mae: 3113702.0000 - val_loss: 1280695.5000 - val_mae: 1280695.5000\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 349ms/step - loss: 2830093.7500 - mae: 2830093.7500 - val_loss: 1151171.1250 - val_mae: 1151171.1250\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 343ms/step - loss: 2544000.2500 - mae: 2544000.2500 - val_loss: 1021092.3125 - val_mae: 1021092.3125\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 320ms/step - loss: 2256506.5000 - mae: 2256506.5000 - val_loss: 890916.3750 - val_mae: 890916.3750\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 1968545.0000 - mae: 1968545.0000 - val_loss: 760991.3125 - val_mae: 760991.3125\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 314ms/step - loss: 1680852.6250 - mae: 1680852.6250 - val_loss: 631616.7500 - val_mae: 631616.7500\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 317ms/step - loss: 1394049.7500 - mae: 1394049.7500 - val_loss: 503028.0625 - val_mae: 503028.0625\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 1108625.7500 - mae: 1108625.7500 - val_loss: 375409.8438 - val_mae: 375409.8438\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 337ms/step - loss: 824974.0625 - mae: 824974.0625 - val_loss: 248917.2969 - val_mae: 248917.2969\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 306ms/step - loss: 543417.0000 - mae: 543417.0000 - val_loss: 123656.0312 - val_mae: 123656.0312\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 313ms/step - loss: 264180.5625 - mae: 264180.5625 - val_loss: 207533.4219 - val_mae: 207533.4219\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 40954.9609 - mae: 40954.9609 - val_loss: 329038.2500 - val_mae: 329038.2500\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 255159.2812 - mae: 255159.2812 - val_loss: 422384.9375 - val_mae: 422384.9375\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 292ms/step - loss: 441704.0000 - mae: 441704.0000 - val_loss: 491111.8750 - val_mae: 491111.8750\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 579224.7500 - mae: 579224.7500 - val_loss: 538243.6875 - val_mae: 538243.6875\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 313ms/step - loss: 673729.8750 - mae: 673729.8750 - val_loss: 566392.3750 - val_mae: 566392.3750\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 344ms/step - loss: 730414.8750 - mae: 730414.8750 - val_loss: 577825.4375 - val_mae: 577825.4375\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 306ms/step - loss: 753793.1250 - mae: 753793.1250 - val_loss: 574503.5000 - val_mae: 574503.5000\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 316ms/step - loss: 747757.5000 - mae: 747757.5000 - val_loss: 558141.9375 - val_mae: 558141.9375\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 306ms/step - loss: 715719.6250 - mae: 715719.6250 - val_loss: 530232.0625 - val_mae: 530232.0625\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 298ms/step - loss: 660651.1250 - mae: 660651.1250 - val_loss: 492079.4688 - val_mae: 492079.4688\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 295ms/step - loss: 585145.6250 - mae: 585145.6250 - val_loss: 444830.5000 - val_mae: 444830.5000\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 318ms/step - loss: 491492.0312 - mae: 491492.0312 - val_loss: 389488.6562 - val_mae: 389488.6562\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 296ms/step - loss: 381680.1562 - mae: 381680.1562 - val_loss: 326933.9688 - val_mae: 326933.9688\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 257459.1562 - mae: 257459.1562 - val_loss: 257935.5000 - val_mae: 257935.5000\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 120369.2734 - mae: 120369.2734 - val_loss: 183171.5000 - val_mae: 183171.5000\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 297ms/step - loss: 49492.5391 - mae: 49492.5391 - val_loss: 122253.1875 - val_mae: 122253.1875\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 149102.2031 - mae: 149102.2031 - val_loss: 110495.2188 - val_mae: 110495.2188\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 231540.9688 - mae: 231540.9688 - val_loss: 132015.4531 - val_mae: 132015.4531\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 279636.7500 - mae: 279636.7500 - val_loss: 139893.8281 - val_mae: 139893.8281\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 0s 285ms/step - loss: 297080.9375 - mae: 297080.9375 - val_loss: 135629.1875 - val_mae: 135629.1875\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 0s 297ms/step - loss: 287215.2500 - mae: 287215.2500 - val_loss: 120570.0000 - val_mae: 120570.0000\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 0s 333ms/step - loss: 253044.8438 - mae: 253044.8438 - val_loss: 96978.5156 - val_mae: 96978.5156\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 197301.4844 - mae: 197301.4844 - val_loss: 134413.3438 - val_mae: 134413.3438\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 0s 292ms/step - loss: 122435.7656 - mae: 122435.7656 - val_loss: 180339.8281 - val_mae: 180339.8281\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 49670.7734 - mae: 49670.7734 - val_loss: 227752.5469 - val_mae: 227752.5469\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 0s 297ms/step - loss: 67289.7188 - mae: 67289.7188 - val_loss: 261258.9375 - val_mae: 261258.9375\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 131486.8125 - mae: 131486.8125 - val_loss: 278995.1875 - val_mae: 278995.1875\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 167245.3594 - mae: 167245.3594 - val_loss: 282678.7812 - val_mae: 282678.7812\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 174958.5781 - mae: 174958.5781 - val_loss: 273835.3750 - val_mae: 273835.3750\n",
      "Epoch 49/50\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 157659.6250 - mae: 157659.6250 - val_loss: 253797.2812 - val_mae: 253797.2812\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 118008.1484 - mae: 118008.1484 - val_loss: 223744.8438 - val_mae: 223744.8438\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "Predicted GDP for Ivano-Frankivsk_Oblast in 2021: [46669.582]\n",
      "Actual GDP for Ivano-Frankivsk_Oblast in 2021: 33120.0\n",
      "Epoch 1/50\n",
      "1/1 [==============================] - 0s 353ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 284ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 310ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 310ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 285ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 283ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 282ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 287ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 289ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 287ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 279ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 284ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 291ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 296ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 287ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 279ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 284ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 282ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 288ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 343ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 283ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 284ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 287ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 285ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 288ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 277ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 284ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 291ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 283ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 287ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 289ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 285ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 315ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 312ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 281ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s 284ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s 288ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s 287ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 0s 281ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 0s 281ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 0s 286ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 0s 292ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 0s 286ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 0s 282ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 0s 285ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 0s 297ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - 0s 331ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - 0s 288ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 49/50\n",
      "1/1 [==============================] - 0s 289ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - 0s 292ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "Predicted GDP for Kyiv_Oblast in 2021: [35730348.]\n",
      "Actual GDP for Kyiv_Oblast in 2021: inf\n",
      "Epoch 1/50\n",
      "1/1 [==============================] - 0s 375ms/step - loss: 1078573.2500 - mae: 1078573.2500 - val_loss: 719452.1250 - val_mae: 719452.1250\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 1018943.9375 - mae: 1018943.9375 - val_loss: 667214.3750 - val_mae: 667214.3750\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 305ms/step - loss: 901882.1250 - mae: 901882.1250 - val_loss: 596938.6875 - val_mae: 596938.6875\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 317ms/step - loss: 741260.5000 - mae: 741260.5000 - val_loss: 512505.9062 - val_mae: 512505.9062\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 310ms/step - loss: 545983.4375 - mae: 545983.4375 - val_loss: 416744.7500 - val_mae: 416744.7500\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 308ms/step - loss: 322626.9375 - mae: 322626.9375 - val_loss: 311866.1562 - val_mae: 311866.1562\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 168897.2500 - mae: 168897.2500 - val_loss: 208009.2656 - val_mae: 208009.2656\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 292ms/step - loss: 184015.5000 - mae: 184015.5000 - val_loss: 127548.9062 - val_mae: 127548.9062\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 353815.0000 - mae: 353815.0000 - val_loss: 80538.7812 - val_mae: 80538.7812\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 304ms/step - loss: 462408.7188 - mae: 462408.7188 - val_loss: 75687.5156 - val_mae: 75687.5156\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 285ms/step - loss: 505890.6875 - mae: 505890.6875 - val_loss: 71248.3594 - val_mae: 71248.3594\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 293ms/step - loss: 494612.9375 - mae: 494612.9375 - val_loss: 87940.1406 - val_mae: 87940.1406\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 436819.3125 - mae: 436819.3125 - val_loss: 127717.8516 - val_mae: 127717.8516\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 285ms/step - loss: 339194.3750 - mae: 339194.3750 - val_loss: 181767.3438 - val_mae: 181767.3438\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 207232.4062 - mae: 207232.4062 - val_loss: 248143.2188 - val_mae: 248143.2188\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 142979.5625 - mae: 142979.5625 - val_loss: 308761.0625 - val_mae: 308761.0625\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 173442.4375 - mae: 173442.4375 - val_loss: 354770.0312 - val_mae: 354770.0312\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 226175.1562 - mae: 226175.1562 - val_loss: 382224.0312 - val_mae: 382224.0312\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 287494.9062 - mae: 287494.9062 - val_loss: 388164.1250 - val_mae: 388164.1250\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 304988.7500 - mae: 304988.7500 - val_loss: 375443.5000 - val_mae: 375443.5000\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 285ms/step - loss: 276912.5312 - mae: 276912.5312 - val_loss: 346568.5312 - val_mae: 346568.5312\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 218131.6875 - mae: 218131.6875 - val_loss: 308145.3125 - val_mae: 308145.3125\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 337ms/step - loss: 174759.4688 - mae: 174759.4688 - val_loss: 265926.2188 - val_mae: 265926.2188\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 143113.6094 - mae: 143113.6094 - val_loss: 224974.1406 - val_mae: 224974.1406\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 281ms/step - loss: 136731.8594 - mae: 136731.8594 - val_loss: 188748.0469 - val_mae: 188748.0469\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 168660.5312 - mae: 168660.5312 - val_loss: 165120.9531 - val_mae: 165120.9531\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 212642.0625 - mae: 212642.0625 - val_loss: 161086.0312 - val_mae: 161086.0312\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 285ms/step - loss: 218100.9375 - mae: 218100.9375 - val_loss: 174142.9375 - val_mae: 174142.9375\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 304ms/step - loss: 182336.6250 - mae: 182336.6250 - val_loss: 202179.4062 - val_mae: 202179.4062\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 138018.5469 - mae: 138018.5469 - val_loss: 235848.2969 - val_mae: 235848.2969\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 126631.9531 - mae: 126631.9531 - val_loss: 263359.4375 - val_mae: 263359.4375\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 135922.0469 - mae: 135922.0469 - val_loss: 285305.9062 - val_mae: 285305.9062\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 155832.3125 - mae: 155832.3125 - val_loss: 297630.5000 - val_mae: 297630.5000\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 169205.2031 - mae: 169205.2031 - val_loss: 301312.6875 - val_mae: 301312.6875\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 295ms/step - loss: 173577.8594 - mae: 173577.8594 - val_loss: 297242.9062 - val_mae: 297242.9062\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s 330ms/step - loss: 169867.0469 - mae: 169867.0469 - val_loss: 286234.1875 - val_mae: 286234.1875\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 158925.9688 - mae: 158925.9688 - val_loss: 269023.0938 - val_mae: 269023.0938\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 141507.8906 - mae: 141507.8906 - val_loss: 246278.8750 - val_mae: 246278.8750\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 121471.6875 - mae: 121471.6875 - val_loss: 223017.4375 - val_mae: 223017.4375\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 111504.7656 - mae: 111504.7656 - val_loss: 199301.9531 - val_mae: 199301.9531\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 107615.0625 - mae: 107615.0625 - val_loss: 178562.9844 - val_mae: 178562.9844\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 116509.7656 - mae: 116509.7656 - val_loss: 164370.9844 - val_mae: 164370.9844\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 0s 281ms/step - loss: 129766.0703 - mae: 129766.0703 - val_loss: 164134.0312 - val_mae: 164134.0312\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 124506.4062 - mae: 124506.4062 - val_loss: 176286.3438 - val_mae: 176286.3438\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 107983.0469 - mae: 107983.0469 - val_loss: 191643.3906 - val_mae: 191643.3906\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 0s 292ms/step - loss: 95423.6094 - mae: 95423.6094 - val_loss: 206089.1250 - val_mae: 206089.1250\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - 0s 279ms/step - loss: 95424.5781 - mae: 95424.5781 - val_loss: 216350.3125 - val_mae: 216350.3125\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - 0s 279ms/step - loss: 97764.7031 - mae: 97764.7031 - val_loss: 222825.6250 - val_mae: 222825.6250\n",
      "Epoch 49/50\n",
      "1/1 [==============================] - 0s 313ms/step - loss: 99726.6328 - mae: 99726.6328 - val_loss: 221454.4219 - val_mae: 221454.4219\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - 0s 281ms/step - loss: 98902.8359 - mae: 98902.8359 - val_loss: 213050.0312 - val_mae: 213050.0312\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "Predicted GDP for Kirovohrad_Oblast in 2021: [427689.3]\n",
      "Actual GDP for Kirovohrad_Oblast in 2021: 25136.0\n",
      "Epoch 1/50\n",
      "1/1 [==============================] - 0s 368ms/step - loss: 455042.4688 - mae: 455042.4688 - val_loss: 98547.5625 - val_mae: 98547.5625\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 359171.0625 - mae: 359171.0625 - val_loss: 77955.8984 - val_mae: 77955.8984\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 289725.4375 - mae: 289725.4375 - val_loss: 140540.3750 - val_mae: 140540.3750\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 309818.9062 - mae: 309818.9062 - val_loss: 182505.1094 - val_mae: 182505.1094\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 345745.2500 - mae: 345745.2500 - val_loss: 206524.0312 - val_mae: 206524.0312\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 364640.8125 - mae: 364640.8125 - val_loss: 214866.2500 - val_mae: 214866.2500\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 369598.8750 - mae: 369598.8750 - val_loss: 201325.7969 - val_mae: 201325.7969\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 352610.2812 - mae: 352610.2812 - val_loss: 176877.4062 - val_mae: 176877.4062\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 326162.9375 - mae: 326162.9375 - val_loss: 142827.5781 - val_mae: 142827.5781\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 293ms/step - loss: 290596.3750 - mae: 290596.3750 - val_loss: 100306.5000 - val_mae: 100306.5000\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 329ms/step - loss: 259977.5781 - mae: 259977.5781 - val_loss: 57671.2812 - val_mae: 57671.2812\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 267138.5000 - mae: 267138.5000 - val_loss: 83511.9922 - val_mae: 83511.9922\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 313027.8438 - mae: 313027.8438 - val_loss: 84594.0234 - val_mae: 84594.0234\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 313228.7500 - mae: 313228.7500 - val_loss: 64707.9453 - val_mae: 64707.9453\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 296ms/step - loss: 273244.6875 - mae: 273244.6875 - val_loss: 91084.8203 - val_mae: 91084.8203\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 235606.7969 - mae: 235606.7969 - val_loss: 130088.7188 - val_mae: 130088.7188\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 242123.0000 - mae: 242123.0000 - val_loss: 154061.1406 - val_mae: 154061.1406\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 258406.4531 - mae: 258406.4531 - val_loss: 164689.8125 - val_mae: 164689.8125\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 262258.4375 - mae: 262258.4375 - val_loss: 163458.9688 - val_mae: 163458.9688\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 255074.7500 - mae: 255074.7500 - val_loss: 151681.3281 - val_mae: 151681.3281\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 238078.5469 - mae: 238078.5469 - val_loss: 130505.4375 - val_mae: 130505.4375\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 285ms/step - loss: 212350.6562 - mae: 212350.6562 - val_loss: 100951.9766 - val_mae: 100951.9766\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 195309.9375 - mae: 195309.9375 - val_loss: 70516.0547 - val_mae: 70516.0547\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 319ms/step - loss: 222854.0000 - mae: 222854.0000 - val_loss: 64120.1367 - val_mae: 64120.1367\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 232368.5469 - mae: 232368.5469 - val_loss: 78467.2578 - val_mae: 78467.2578\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 204747.2031 - mae: 204747.2031 - val_loss: 110717.8281 - val_mae: 110717.8281\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 285ms/step - loss: 175901.0312 - mae: 175901.0312 - val_loss: 136093.9062 - val_mae: 136093.9062\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 285ms/step - loss: 181984.8281 - mae: 181984.8281 - val_loss: 148888.7188 - val_mae: 148888.7188\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 285ms/step - loss: 188047.8906 - mae: 188047.8906 - val_loss: 150470.7188 - val_mae: 150470.7188\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 307ms/step - loss: 183636.6875 - mae: 183636.6875 - val_loss: 142031.3438 - val_mae: 142031.3438\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 169883.2812 - mae: 169883.2812 - val_loss: 124641.3984 - val_mae: 124641.3984\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 152391.6875 - mae: 152391.6875 - val_loss: 105419.0312 - val_mae: 105419.0312\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 296ms/step - loss: 155360.8125 - mae: 155360.8125 - val_loss: 95564.9453 - val_mae: 95564.9453\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 161516.8906 - mae: 161516.8906 - val_loss: 99998.4766 - val_mae: 99998.4766\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 152701.8281 - mae: 152701.8281 - val_loss: 111319.8984 - val_mae: 111319.8984\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 140719.3750 - mae: 140719.3750 - val_loss: 128800.5625 - val_mae: 128800.5625\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s 338ms/step - loss: 127204.4062 - mae: 127204.4062 - val_loss: 140996.0000 - val_mae: 140996.0000\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 133226.1562 - mae: 133226.1562 - val_loss: 142254.9375 - val_mae: 142254.9375\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 130740.8672 - mae: 130740.8672 - val_loss: 127566.4375 - val_mae: 127566.4375\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 113149.6641 - mae: 113149.6641 - val_loss: 121570.5156 - val_mae: 121570.5156\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 0s 296ms/step - loss: 113483.7891 - mae: 113483.7891 - val_loss: 123374.7969 - val_mae: 123374.7969\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 0s 285ms/step - loss: 107867.9766 - mae: 107867.9766 - val_loss: 132169.5312 - val_mae: 132169.5312\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 107580.6641 - mae: 107580.6641 - val_loss: 124417.9688 - val_mae: 124417.9688\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 0s 308ms/step - loss: 99911.2344 - mae: 99911.2344 - val_loss: 112427.0938 - val_mae: 112427.0938\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 0s 280ms/step - loss: 102692.6562 - mae: 102692.6562 - val_loss: 108754.5625 - val_mae: 108754.5625\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 0s 285ms/step - loss: 100791.7734 - mae: 100791.7734 - val_loss: 112544.6094 - val_mae: 112544.6094\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - 0s 281ms/step - loss: 93262.4766 - mae: 93262.4766 - val_loss: 123022.1250 - val_mae: 123022.1250\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - 0s 313ms/step - loss: 93335.3125 - mae: 93335.3125 - val_loss: 127456.0312 - val_mae: 127456.0312\n",
      "Epoch 49/50\n",
      "1/1 [==============================] - 0s 299ms/step - loss: 97182.0625 - mae: 97182.0625 - val_loss: 116022.7344 - val_mae: 116022.7344\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - 0s 279ms/step - loss: 87081.5156 - mae: 87081.5156 - val_loss: 100785.0312 - val_mae: 100785.0312\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Predicted GDP for Luhansk_Oblast in 2021: [995587.94]\n",
      "Actual GDP for Luhansk_Oblast in 2021: 15880.0\n",
      "Epoch 1/50\n",
      "1/1 [==============================] - 0s 357ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 291ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 285ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 291ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 294ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 305ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 298ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 289ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 291ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 335ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 292ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 288ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 286ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 282ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 290ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 288ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 284ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 282ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 295ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 290ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 303ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 328ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 284ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 285ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 287ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 283ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 284ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 285ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 288ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 286ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 283ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 301ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 286ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 337ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 297ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s 292ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s 290ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s 289ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 0s 287ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 0s 280ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 0s 290ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 0s 282ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 0s 292ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 0s 290ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 0s 297ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 0s 283ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - 0s 291ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - 0s 284ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 49/50\n",
      "1/1 [==============================] - 0s 275ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - 0s 286ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "Predicted GDP for Lviv_Oblast in 2021: [1451139.4]\n",
      "Actual GDP for Lviv_Oblast in 2021: inf\n",
      "Epoch 1/50\n",
      "1/1 [==============================] - 0s 371ms/step - loss: 1973577.3750 - mae: 1973577.3750 - val_loss: 975107.1875 - val_mae: 975107.1875\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 1888104.6250 - mae: 1888104.6250 - val_loss: 904431.5625 - val_mae: 904431.5625\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 1738990.5000 - mae: 1738990.5000 - val_loss: 810031.6250 - val_mae: 810031.6250\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 1539599.5000 - mae: 1539599.5000 - val_loss: 696118.6875 - val_mae: 696118.6875\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 1298776.3750 - mae: 1298776.3750 - val_loss: 565799.1250 - val_mae: 565799.1250\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 293ms/step - loss: 1023074.1250 - mae: 1023074.1250 - val_loss: 421481.4688 - val_mae: 421481.4688\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 717556.7500 - mae: 717556.7500 - val_loss: 265072.6562 - val_mae: 265072.6562\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 386251.4375 - mae: 386251.4375 - val_loss: 98109.1406 - val_mae: 98109.1406\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 281ms/step - loss: 49551.0547 - mae: 49551.0547 - val_loss: 251039.6562 - val_mae: 251039.6562\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 296ms/step - loss: 315427.3125 - mae: 315427.3125 - val_loss: 358816.1250 - val_mae: 358816.1250\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 558978.4375 - mae: 558978.4375 - val_loss: 426557.2500 - val_mae: 426557.2500\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 712051.6250 - mae: 712051.6250 - val_loss: 459617.9062 - val_mae: 459617.9062\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 292ms/step - loss: 786583.1250 - mae: 786583.1250 - val_loss: 462671.7812 - val_mae: 462671.7812\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 792953.2500 - mae: 792953.2500 - val_loss: 439792.8125 - val_mae: 439792.8125\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 292ms/step - loss: 740276.8750 - mae: 740276.8750 - val_loss: 394556.8125 - val_mae: 394556.8125\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 279ms/step - loss: 636558.5625 - mae: 636558.5625 - val_loss: 330113.9062 - val_mae: 330113.9062\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 285ms/step - loss: 488840.5000 - mae: 488840.5000 - val_loss: 249225.1250 - val_mae: 249225.1250\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 303326.4062 - mae: 303326.4062 - val_loss: 154329.7500 - val_mae: 154329.7500\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 305ms/step - loss: 98375.0938 - mae: 98375.0938 - val_loss: 150170.9062 - val_mae: 150170.9062\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 285ms/step - loss: 149307.1562 - mae: 149307.1562 - val_loss: 223431.8594 - val_mae: 223431.8594\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 305450.1250 - mae: 305450.1250 - val_loss: 264482.5625 - val_mae: 264482.5625\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 393043.1250 - mae: 393043.1250 - val_loss: 277339.7500 - val_mae: 277339.7500\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 420637.6250 - mae: 420637.6250 - val_loss: 265427.7188 - val_mae: 265427.7188\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 296ms/step - loss: 395496.4062 - mae: 395496.4062 - val_loss: 231670.6562 - val_mae: 231670.6562\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 323822.1562 - mae: 323822.1562 - val_loss: 178578.4219 - val_mae: 178578.4219\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 210944.6250 - mae: 210944.6250 - val_loss: 108297.5312 - val_mae: 108297.5312\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 65128.2383 - mae: 65128.2383 - val_loss: 164097.8750 - val_mae: 164097.8750\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 116311.4375 - mae: 116311.4375 - val_loss: 214017.0938 - val_mae: 214017.0938\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 224134.9375 - mae: 224134.9375 - val_loss: 237625.4688 - val_mae: 237625.4688\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 278571.9062 - mae: 278571.9062 - val_loss: 237868.7188 - val_mae: 237868.7188\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 296ms/step - loss: 279042.0000 - mae: 279042.0000 - val_loss: 217460.1875 - val_mae: 217460.1875\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 231730.5938 - mae: 231730.5938 - val_loss: 178857.2812 - val_mae: 178857.2812\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 142272.7656 - mae: 142272.7656 - val_loss: 124308.0625 - val_mae: 124308.0625\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 49278.1328 - mae: 49278.1328 - val_loss: 136363.5000 - val_mae: 136363.5000\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 122927.6562 - mae: 122927.6562 - val_loss: 172773.4219 - val_mae: 172773.4219\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 200758.0312 - mae: 200758.0312 - val_loss: 183966.0938 - val_mae: 183966.0938\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 224807.9375 - mae: 224807.9375 - val_loss: 172801.5156 - val_mae: 172801.5156\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 201166.3750 - mae: 201166.3750 - val_loss: 141751.9062 - val_mae: 141751.9062\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 135090.7031 - mae: 135090.7031 - val_loss: 104168.8438 - val_mae: 104168.8438\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 44853.9609 - mae: 44853.9609 - val_loss: 153671.1094 - val_mae: 153671.1094\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 96131.0156 - mae: 96131.0156 - val_loss: 183163.7969 - val_mae: 183163.7969\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 0s 292ms/step - loss: 151858.9531 - mae: 151858.9531 - val_loss: 190316.7500 - val_mae: 190316.7500\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 168202.5312 - mae: 168202.5312 - val_loss: 177593.7969 - val_mae: 177593.7969\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 138420.3438 - mae: 138420.3438 - val_loss: 151432.4375 - val_mae: 151432.4375\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 0s 285ms/step - loss: 90881.6250 - mae: 90881.6250 - val_loss: 113326.8906 - val_mae: 113326.8906\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 38787.7812 - mae: 38787.7812 - val_loss: 120743.8516 - val_mae: 120743.8516\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 92537.4688 - mae: 92537.4688 - val_loss: 134074.4688 - val_mae: 134074.4688\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 121061.6250 - mae: 121061.6250 - val_loss: 125696.8828 - val_mae: 125696.8828\n",
      "Epoch 49/50\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 103254.4531 - mae: 103254.4531 - val_loss: 99463.1719 - val_mae: 99463.1719\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 51538.6289 - mae: 51538.6289 - val_loss: 135993.6875 - val_mae: 135993.6875\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "Predicted GDP for Mykolaiv_Oblast in 2021: [127920.99]\n",
      "Actual GDP for Mykolaiv_Oblast in 2021: 32560.0\n",
      "Epoch 1/50\n",
      "1/1 [==============================] - 0s 352ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 285ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 290ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 282ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 287ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 286ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 293ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 289ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 294ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 291ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 300ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 297ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 292ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 339ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 315ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 287ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 293ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 293ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 287ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 286ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 286ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 283ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 288ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 285ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 299ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 319ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 291ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 291ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 291ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 299ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 289ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 282ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 285ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 288ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 283ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s 321ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s 310ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s 295ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 0s 289ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 0s 297ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 0s 294ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 0s 292ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 0s 303ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 0s 302ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 0s 296ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 0s 285ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - 0s 320ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - 0s 308ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 49/50\n",
      "1/1 [==============================] - 0s 279ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - 0s 280ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "Predicted GDP for Odessa_Oblast in 2021: [76986560.]\n",
      "Actual GDP for Odessa_Oblast in 2021: inf\n",
      "Epoch 1/50\n",
      "1/1 [==============================] - 0s 371ms/step - loss: 42082064.0000 - mae: 42082064.0000 - val_loss: 17328090.0000 - val_mae: 17328090.0000\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 42894352.0000 - mae: 42894352.0000 - val_loss: 17416028.0000 - val_mae: 17416028.0000\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 43083376.0000 - mae: 43083376.0000 - val_loss: 17300822.0000 - val_mae: 17300822.0000\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 296ms/step - loss: 42758272.0000 - mae: 42758272.0000 - val_loss: 17016484.0000 - val_mae: 17016484.0000\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 42004784.0000 - mae: 42004784.0000 - val_loss: 16592816.0000 - val_mae: 16592816.0000\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 40898448.0000 - mae: 40898448.0000 - val_loss: 16056635.0000 - val_mae: 16056635.0000\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 314ms/step - loss: 39507344.0000 - mae: 39507344.0000 - val_loss: 15431975.0000 - val_mae: 15431975.0000\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 298ms/step - loss: 37892600.0000 - mae: 37892600.0000 - val_loss: 14740205.0000 - val_mae: 14740205.0000\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 36108500.0000 - mae: 36108500.0000 - val_loss: 14000039.0000 - val_mae: 14000039.0000\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 34202712.0000 - mae: 34202712.0000 - val_loss: 13227615.0000 - val_mae: 13227615.0000\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 285ms/step - loss: 32216296.0000 - mae: 32216296.0000 - val_loss: 12436649.0000 - val_mae: 12436649.0000\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 30184160.0000 - mae: 30184160.0000 - val_loss: 11638601.0000 - val_mae: 11638601.0000\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 28135344.0000 - mae: 28135344.0000 - val_loss: 10842872.0000 - val_mae: 10842872.0000\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 26093860.0000 - mae: 26093860.0000 - val_loss: 10057021.0000 - val_mae: 10057021.0000\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 24078720.0000 - mae: 24078720.0000 - val_loss: 9286967.0000 - val_mae: 9286967.0000\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 281ms/step - loss: 22105088.0000 - mae: 22105088.0000 - val_loss: 8537237.0000 - val_mae: 8537237.0000\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 307ms/step - loss: 20184184.0000 - mae: 20184184.0000 - val_loss: 7811135.5000 - val_mae: 7811135.5000\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 18324480.0000 - mae: 18324480.0000 - val_loss: 7110939.5000 - val_mae: 7110939.5000\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 306ms/step - loss: 16531478.0000 - mae: 16531478.0000 - val_loss: 6438072.0000 - val_mae: 6438072.0000\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 14808852.0000 - mae: 14808852.0000 - val_loss: 5793257.0000 - val_mae: 5793257.0000\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 13158156.0000 - mae: 13158156.0000 - val_loss: 5176643.5000 - val_mae: 5176643.5000\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 11579704.0000 - mae: 11579704.0000 - val_loss: 4587930.5000 - val_mae: 4587930.5000\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 305ms/step - loss: 10072640.0000 - mae: 10072640.0000 - val_loss: 4026454.2500 - val_mae: 4026454.2500\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 326ms/step - loss: 8635112.0000 - mae: 8635112.0000 - val_loss: 3491298.2500 - val_mae: 3491298.2500\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 326ms/step - loss: 7264677.5000 - mae: 7264677.5000 - val_loss: 2981320.5000 - val_mae: 2981320.5000\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 319ms/step - loss: 5958377.5000 - mae: 5958377.5000 - val_loss: 2495260.7500 - val_mae: 2495260.7500\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 316ms/step - loss: 4712873.0000 - mae: 4712873.0000 - val_loss: 2031733.2500 - val_mae: 2031733.2500\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 297ms/step - loss: 3524465.0000 - mae: 3524465.0000 - val_loss: 1589328.3750 - val_mae: 1589328.3750\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 336ms/step - loss: 2389605.5000 - mae: 2389605.5000 - val_loss: 1166599.2500 - val_mae: 1166599.2500\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 1304423.8750 - mae: 1304423.8750 - val_loss: 762068.2500 - val_mae: 762068.2500\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 442175.7500 - mae: 442175.7500 - val_loss: 397786.3125 - val_mae: 397786.3125\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 761022.6250 - mae: 761022.6250 - val_loss: 103472.9844 - val_mae: 103472.9844\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 285ms/step - loss: 1416722.3750 - mae: 1416722.3750 - val_loss: 263457.1250 - val_mae: 263457.1250\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 1980232.5000 - mae: 1980232.5000 - val_loss: 422756.7500 - val_mae: 422756.7500\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 2387500.7500 - mae: 2387500.7500 - val_loss: 536086.9375 - val_mae: 536086.9375\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 2658071.5000 - mae: 2658071.5000 - val_loss: 610418.6875 - val_mae: 610418.6875\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s 298ms/step - loss: 2829632.2500 - mae: 2829632.2500 - val_loss: 650816.6250 - val_mae: 650816.6250\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s 313ms/step - loss: 2915862.5000 - mae: 2915862.5000 - val_loss: 661612.2500 - val_mae: 661612.2500\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 0s 306ms/step - loss: 2927614.0000 - mae: 2927614.0000 - val_loss: 646520.8125 - val_mae: 646520.8125\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 2874219.0000 - mae: 2874219.0000 - val_loss: 608738.1250 - val_mae: 608738.1250\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 0s 281ms/step - loss: 2763682.0000 - mae: 2763682.0000 - val_loss: 551019.0000 - val_mae: 551019.0000\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 2602954.5000 - mae: 2602954.5000 - val_loss: 475740.6562 - val_mae: 475740.6562\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 2398000.0000 - mae: 2398000.0000 - val_loss: 384967.1250 - val_mae: 384967.1250\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 0s 285ms/step - loss: 2154016.7500 - mae: 2154016.7500 - val_loss: 280475.3125 - val_mae: 280475.3125\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 0s 285ms/step - loss: 1875468.7500 - mae: 1875468.7500 - val_loss: 163787.0625 - val_mae: 163787.0625\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 0s 285ms/step - loss: 1566210.5000 - mae: 1566210.5000 - val_loss: 112541.1094 - val_mae: 112541.1094\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - 0s 303ms/step - loss: 1229555.7500 - mae: 1229555.7500 - val_loss: 258018.7188 - val_mae: 258018.7188\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - 0s 310ms/step - loss: 887606.1250 - mae: 887606.1250 - val_loss: 407859.1875 - val_mae: 407859.1875\n",
      "Epoch 49/50\n",
      "1/1 [==============================] - 0s 299ms/step - loss: 584810.3125 - mae: 584810.3125 - val_loss: 562084.3125 - val_mae: 562084.3125\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 332681.0625 - mae: 332681.0625 - val_loss: 700773.8125 - val_mae: 700773.8125\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "Predicted GDP for Poltava_Oblast in 2021: [2187428.2]\n",
      "Actual GDP for Poltava_Oblast in 2021: 45920.0\n",
      "Epoch 1/50\n",
      "1/1 [==============================] - 0s 375ms/step - loss: 955634.5000 - mae: 955634.5000 - val_loss: 320370.5000 - val_mae: 320370.5000\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 970601.3750 - mae: 970601.3750 - val_loss: 313502.4688 - val_mae: 313502.4688\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 956419.6875 - mae: 956419.6875 - val_loss: 294953.4375 - val_mae: 294953.4375\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 919351.1875 - mae: 919351.1875 - val_loss: 266895.3125 - val_mae: 266895.3125\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 863663.2500 - mae: 863663.2500 - val_loss: 230982.1875 - val_mae: 230982.1875\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 296ms/step - loss: 792602.3750 - mae: 792602.3750 - val_loss: 188529.3906 - val_mae: 188529.3906\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 708749.2500 - mae: 708749.2500 - val_loss: 141046.3438 - val_mae: 141046.3438\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 312ms/step - loss: 614228.6250 - mae: 614228.6250 - val_loss: 106028.3750 - val_mae: 106028.3750\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 299ms/step - loss: 510798.7500 - mae: 510798.7500 - val_loss: 153571.4688 - val_mae: 153571.4688\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 281ms/step - loss: 399947.3438 - mae: 399947.3438 - val_loss: 203477.5156 - val_mae: 203477.5156\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 284332.8438 - mae: 284332.8438 - val_loss: 255244.8125 - val_mae: 255244.8125\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 181896.5469 - mae: 181896.5469 - val_loss: 306942.4062 - val_mae: 306942.4062\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 99957.4688 - mae: 99957.4688 - val_loss: 356844.3750 - val_mae: 356844.3750\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 87422.2188 - mae: 87422.2188 - val_loss: 401657.1250 - val_mae: 401657.1250\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 145715.0938 - mae: 145715.0938 - val_loss: 433941.5000 - val_mae: 433941.5000\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 212249.3750 - mae: 212249.3750 - val_loss: 455377.6562 - val_mae: 455377.6562\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 314ms/step - loss: 256072.2188 - mae: 256072.2188 - val_loss: 467395.8125 - val_mae: 467395.8125\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 310ms/step - loss: 280141.8125 - mae: 280141.8125 - val_loss: 471220.0938 - val_mae: 471220.0938\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 295ms/step - loss: 287002.2812 - mae: 287002.2812 - val_loss: 467907.5625 - val_mae: 467907.5625\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 293ms/step - loss: 278830.3438 - mae: 278830.3438 - val_loss: 458363.5625 - val_mae: 458363.5625\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 257520.2500 - mae: 257520.2500 - val_loss: 443379.0625 - val_mae: 443379.0625\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 224704.8594 - mae: 224704.8594 - val_loss: 423633.9062 - val_mae: 423633.9062\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 292ms/step - loss: 181799.7031 - mae: 181799.7031 - val_loss: 399722.7500 - val_mae: 399722.7500\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 315ms/step - loss: 130042.6562 - mae: 130042.6562 - val_loss: 372162.1875 - val_mae: 372162.1875\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 87004.7344 - mae: 87004.7344 - val_loss: 347290.6250 - val_mae: 347290.6250\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 87026.5781 - mae: 87026.5781 - val_loss: 324847.8438 - val_mae: 324847.8438\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 303ms/step - loss: 92783.3984 - mae: 92783.3984 - val_loss: 306103.7500 - val_mae: 306103.7500\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 310ms/step - loss: 111905.3125 - mae: 111905.3125 - val_loss: 293845.8438 - val_mae: 293845.8438\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 285ms/step - loss: 130957.6406 - mae: 130957.6406 - val_loss: 287375.4375 - val_mae: 287375.4375\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 285ms/step - loss: 140938.5469 - mae: 140938.5469 - val_loss: 286062.2188 - val_mae: 286062.2188\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 281ms/step - loss: 142830.2188 - mae: 142830.2188 - val_loss: 289336.0000 - val_mae: 289336.0000\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 297ms/step - loss: 137516.7812 - mae: 137516.7812 - val_loss: 296691.0000 - val_mae: 296691.0000\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 125791.9062 - mae: 125791.9062 - val_loss: 307667.8438 - val_mae: 307667.8438\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 108367.7891 - mae: 108367.7891 - val_loss: 321858.5312 - val_mae: 321858.5312\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 92758.7812 - mae: 92758.7812 - val_loss: 336027.5000 - val_mae: 336027.5000\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s 292ms/step - loss: 85172.4062 - mae: 85172.4062 - val_loss: 350173.0625 - val_mae: 350173.0625\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s 307ms/step - loss: 84344.3438 - mae: 84344.3438 - val_loss: 362871.4375 - val_mae: 362871.4375\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s 308ms/step - loss: 83919.9766 - mae: 83919.9766 - val_loss: 374269.5625 - val_mae: 374269.5625\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 86336.9688 - mae: 86336.9688 - val_loss: 381642.6875 - val_mae: 381642.6875\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 0s 292ms/step - loss: 94980.7969 - mae: 94980.7969 - val_loss: 383890.1875 - val_mae: 383890.1875\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 0s 285ms/step - loss: 99029.5938 - mae: 99029.5938 - val_loss: 379963.9062 - val_mae: 379963.9062\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 0s 306ms/step - loss: 92941.0000 - mae: 92941.0000 - val_loss: 372156.2500 - val_mae: 372156.2500\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 84466.2188 - mae: 84466.2188 - val_loss: 362339.5625 - val_mae: 362339.5625\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 81836.2812 - mae: 81836.2812 - val_loss: 353466.0625 - val_mae: 353466.0625\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 81531.5156 - mae: 81531.5156 - val_loss: 345443.0938 - val_mae: 345443.0938\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 81174.5938 - mae: 81174.5938 - val_loss: 338186.0000 - val_mae: 338186.0000\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - 0s 319ms/step - loss: 80771.9531 - mae: 80771.9531 - val_loss: 331620.0000 - val_mae: 331620.0000\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 82900.6719 - mae: 82900.6719 - val_loss: 327085.9375 - val_mae: 327085.9375\n",
      "Epoch 49/50\n",
      "1/1 [==============================] - 0s 279ms/step - loss: 84743.1719 - mae: 84743.1719 - val_loss: 324382.7500 - val_mae: 324382.7500\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 85626.8750 - mae: 85626.8750 - val_loss: 323326.6875 - val_mae: 323326.6875\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "Predicted GDP for Rivne_Oblast in 2021: [-317425.62]\n",
      "Actual GDP for Rivne_Oblast in 2021: 22896.0\n",
      "Epoch 1/50\n",
      "1/1 [==============================] - 0s 353ms/step - loss: 843788.5000 - mae: 843788.5000 - val_loss: 681430.0625 - val_mae: 681430.0625\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 787306.5000 - mae: 787306.5000 - val_loss: 651510.5000 - val_mae: 651510.5000\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 285ms/step - loss: 696157.1875 - mae: 696157.1875 - val_loss: 610518.1250 - val_mae: 610518.1250\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 578857.7500 - mae: 578857.7500 - val_loss: 562216.1250 - val_mae: 562216.1250\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 434772.0625 - mae: 434772.0625 - val_loss: 505581.8125 - val_mae: 505581.8125\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 300283.1875 - mae: 300283.1875 - val_loss: 440199.0000 - val_mae: 440199.0000\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 281ms/step - loss: 259822.0312 - mae: 259822.0312 - val_loss: 376916.4375 - val_mae: 376916.4375\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 258498.4219 - mae: 258498.4219 - val_loss: 324027.1250 - val_mae: 324027.1250\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 285ms/step - loss: 300221.6250 - mae: 300221.6250 - val_loss: 285063.3125 - val_mae: 285063.3125\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 292ms/step - loss: 343770.9062 - mae: 343770.9062 - val_loss: 258348.0000 - val_mae: 258348.0000\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 372502.1562 - mae: 372502.1562 - val_loss: 242650.8125 - val_mae: 242650.8125\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 293ms/step - loss: 387874.7500 - mae: 387874.7500 - val_loss: 236875.8125 - val_mae: 236875.8125\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 390153.8125 - mae: 390153.8125 - val_loss: 239785.4375 - val_mae: 239785.4375\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 285ms/step - loss: 380990.0000 - mae: 380990.0000 - val_loss: 250292.4062 - val_mae: 250292.4062\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 362109.9375 - mae: 362109.9375 - val_loss: 267215.8438 - val_mae: 267215.8438\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 280ms/step - loss: 335901.4375 - mae: 335901.4375 - val_loss: 289543.8125 - val_mae: 289543.8125\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 292ms/step - loss: 303356.6250 - mae: 303356.6250 - val_loss: 316596.5625 - val_mae: 316596.5625\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 265036.5938 - mae: 265036.5938 - val_loss: 347774.2500 - val_mae: 347774.2500\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 299ms/step - loss: 235732.4219 - mae: 235732.4219 - val_loss: 379156.7500 - val_mae: 379156.7500\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 221087.2500 - mae: 221087.2500 - val_loss: 406909.2812 - val_mae: 406909.2812\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 217782.6406 - mae: 217782.6406 - val_loss: 431399.4062 - val_mae: 431399.4062\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 229990.0938 - mae: 229990.0938 - val_loss: 449268.8125 - val_mae: 449268.8125\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 280ms/step - loss: 240291.7344 - mae: 240291.7344 - val_loss: 457728.7188 - val_mae: 457728.7188\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 285ms/step - loss: 251272.7344 - mae: 251272.7344 - val_loss: 457908.5000 - val_mae: 457908.5000\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 252282.2344 - mae: 252282.2344 - val_loss: 450798.4688 - val_mae: 450798.4688\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 244509.1562 - mae: 244509.1562 - val_loss: 437265.2812 - val_mae: 437265.2812\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 303ms/step - loss: 228999.2188 - mae: 228999.2188 - val_loss: 418077.2500 - val_mae: 418077.2500\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 285ms/step - loss: 208615.3438 - mae: 208615.3438 - val_loss: 397017.5938 - val_mae: 397017.5938\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 193634.3438 - mae: 193634.3438 - val_loss: 374295.1250 - val_mae: 374295.1250\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 182704.5469 - mae: 182704.5469 - val_loss: 353438.6875 - val_mae: 353438.6875\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 178350.3906 - mae: 178350.3906 - val_loss: 334267.3125 - val_mae: 334267.3125\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 180719.6875 - mae: 180719.6875 - val_loss: 320252.2500 - val_mae: 320252.2500\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 184115.6875 - mae: 184115.6875 - val_loss: 310873.1250 - val_mae: 310873.1250\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 184601.8125 - mae: 184601.8125 - val_loss: 305648.2500 - val_mae: 305648.2500\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 182484.5312 - mae: 182484.5312 - val_loss: 304151.6250 - val_mae: 304151.6250\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 178036.8750 - mae: 178036.8750 - val_loss: 305997.3125 - val_mae: 305997.3125\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s 293ms/step - loss: 171505.7812 - mae: 171505.7812 - val_loss: 310838.4375 - val_mae: 310838.4375\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 163114.1250 - mae: 163114.1250 - val_loss: 318360.0000 - val_mae: 318360.0000\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 153060.8750 - mae: 153060.8750 - val_loss: 328281.1250 - val_mae: 328281.1250\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 0s 292ms/step - loss: 141527.1406 - mae: 141527.1406 - val_loss: 340348.1562 - val_mae: 340348.1562\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 0s 295ms/step - loss: 136325.2344 - mae: 136325.2344 - val_loss: 350795.7500 - val_mae: 350795.7500\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 0s 292ms/step - loss: 139558.3750 - mae: 139558.3750 - val_loss: 353215.2188 - val_mae: 353215.2188\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 143461.8438 - mae: 143461.8438 - val_loss: 348538.0938 - val_mae: 348538.0938\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 0s 303ms/step - loss: 138799.8906 - mae: 138799.8906 - val_loss: 337583.3750 - val_mae: 337583.3750\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 127679.9062 - mae: 127679.9062 - val_loss: 324249.0312 - val_mae: 324249.0312\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 118784.5312 - mae: 118784.5312 - val_loss: 308783.4062 - val_mae: 308783.4062\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 117810.1094 - mae: 117810.1094 - val_loss: 297946.6250 - val_mae: 297946.6250\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 119736.0000 - mae: 119736.0000 - val_loss: 291272.4688 - val_mae: 291272.4688\n",
      "Epoch 49/50\n",
      "1/1 [==============================] - 0s 292ms/step - loss: 119029.7031 - mae: 119029.7031 - val_loss: 288334.1875 - val_mae: 288334.1875\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 115962.4531 - mae: 115962.4531 - val_loss: 288749.5938 - val_mae: 288749.5938\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "Predicted GDP for Sumy_Oblast in 2021: [-284474.47]\n",
      "Actual GDP for Sumy_Oblast in 2021: 25104.0\n",
      "Epoch 1/50\n",
      "1/1 [==============================] - 0s 366ms/step - loss: 2864013.0000 - mae: 2864013.0000 - val_loss: 1422034.7500 - val_mae: 1422034.7500\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 285ms/step - loss: 2849192.5000 - mae: 2849192.5000 - val_loss: 1409315.7500 - val_mae: 1409315.7500\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 2824471.2500 - mae: 2824471.2500 - val_loss: 1392098.1250 - val_mae: 1392098.1250\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 2791027.0000 - mae: 2791027.0000 - val_loss: 1370934.8750 - val_mae: 1370934.8750\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 2749932.0000 - mae: 2749932.0000 - val_loss: 1346322.1250 - val_mae: 1346322.1250\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 2702142.5000 - mae: 2702142.5000 - val_loss: 1318707.2500 - val_mae: 1318707.2500\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 2648526.0000 - mae: 2648526.0000 - val_loss: 1288494.0000 - val_mae: 1288494.0000\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 279ms/step - loss: 2589861.0000 - mae: 2589861.0000 - val_loss: 1256044.1250 - val_mae: 1256044.1250\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 2526845.0000 - mae: 2526845.0000 - val_loss: 1221676.6250 - val_mae: 1221676.6250\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 330ms/step - loss: 2460099.2500 - mae: 2460099.2500 - val_loss: 1185686.5000 - val_mae: 1185686.5000\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 306ms/step - loss: 2390189.0000 - mae: 2390189.0000 - val_loss: 1148328.2500 - val_mae: 1148328.2500\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 2317609.7500 - mae: 2317609.7500 - val_loss: 1109832.1250 - val_mae: 1109832.1250\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 285ms/step - loss: 2242806.2500 - mae: 2242806.2500 - val_loss: 1070403.5000 - val_mae: 1070403.5000\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 2166176.5000 - mae: 2166176.5000 - val_loss: 1030220.5000 - val_mae: 1030220.5000\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 298ms/step - loss: 2088061.0000 - mae: 2088061.0000 - val_loss: 989444.1250 - val_mae: 989444.1250\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 2008776.5000 - mae: 2008776.5000 - val_loss: 948211.8750 - val_mae: 948211.8750\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 1928587.5000 - mae: 1928587.5000 - val_loss: 906648.8125 - val_mae: 906648.8125\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 293ms/step - loss: 1847738.2500 - mae: 1847738.2500 - val_loss: 864862.5000 - val_mae: 864862.5000\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 303ms/step - loss: 1766434.5000 - mae: 1766434.5000 - val_loss: 822943.0000 - val_mae: 822943.0000\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 303ms/step - loss: 1684852.5000 - mae: 1684852.5000 - val_loss: 780974.0000 - val_mae: 780974.0000\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 306ms/step - loss: 1603157.7500 - mae: 1603157.7500 - val_loss: 739028.3125 - val_mae: 739028.3125\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 1521488.2500 - mae: 1521488.2500 - val_loss: 697160.1875 - val_mae: 697160.1875\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 293ms/step - loss: 1439949.5000 - mae: 1439949.5000 - val_loss: 655424.2500 - val_mae: 655424.2500\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 285ms/step - loss: 1358651.7500 - mae: 1358651.7500 - val_loss: 613863.6875 - val_mae: 613863.6875\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 1277677.0000 - mae: 1277677.0000 - val_loss: 572513.8750 - val_mae: 572513.8750\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 1197095.3750 - mae: 1197095.3750 - val_loss: 531401.5625 - val_mae: 531401.5625\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 1116962.5000 - mae: 1116962.5000 - val_loss: 490554.0625 - val_mae: 490554.0625\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 316ms/step - loss: 1037327.5000 - mae: 1037327.5000 - val_loss: 449994.9062 - val_mae: 449994.9062\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 958239.5000 - mae: 958239.5000 - val_loss: 409732.6250 - val_mae: 409732.6250\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 348ms/step - loss: 879714.1250 - mae: 879714.1250 - val_loss: 369782.2812 - val_mae: 369782.2812\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 801784.3750 - mae: 801784.3750 - val_loss: 330150.0312 - val_mae: 330150.0312\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 303ms/step - loss: 724460.1250 - mae: 724460.1250 - val_loss: 290838.7500 - val_mae: 290838.7500\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 285ms/step - loss: 647752.6875 - mae: 647752.6875 - val_loss: 251856.2656 - val_mae: 251856.2656\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 571672.5625 - mae: 571672.5625 - val_loss: 213199.5938 - val_mae: 213199.5938\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 496217.8750 - mae: 496217.8750 - val_loss: 174864.6094 - val_mae: 174864.6094\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s 292ms/step - loss: 421381.1875 - mae: 421381.1875 - val_loss: 136850.9219 - val_mae: 136850.9219\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s 313ms/step - loss: 347162.6875 - mae: 347162.6875 - val_loss: 99152.8125 - val_mae: 99152.8125\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 273553.0625 - mae: 273553.0625 - val_loss: 61761.8164 - val_mae: 61761.8164\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 0s 331ms/step - loss: 200535.7500 - mae: 200535.7500 - val_loss: 27026.2129 - val_mae: 27026.2129\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 128098.6406 - mae: 128098.6406 - val_loss: 12126.9160 - val_mae: 12126.9160\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 56227.7812 - mae: 56227.7812 - val_loss: 48422.1445 - val_mae: 48422.1445\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 32193.2129 - mae: 32193.2129 - val_loss: 79534.8984 - val_mae: 79534.8984\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 77089.3906 - mae: 77089.3906 - val_loss: 101873.3672 - val_mae: 101873.3672\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 124676.8203 - mae: 124676.8203 - val_loss: 118868.4688 - val_mae: 118868.4688\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 159213.1875 - mae: 159213.1875 - val_loss: 131163.9531 - val_mae: 131163.9531\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 183717.4688 - mae: 183717.4688 - val_loss: 139292.8125 - val_mae: 139292.8125\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - 0s 281ms/step - loss: 199906.7188 - mae: 199906.7188 - val_loss: 143693.8594 - val_mae: 143693.8594\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - 0s 318ms/step - loss: 208700.3125 - mae: 208700.3125 - val_loss: 144784.9531 - val_mae: 144784.9531\n",
      "Epoch 49/50\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 210853.1562 - mae: 210853.1562 - val_loss: 142913.1719 - val_mae: 142913.1719\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - 0s 281ms/step - loss: 207210.0312 - mae: 207210.0312 - val_loss: 138383.2188 - val_mae: 138383.2188\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Predicted GDP for Ternopil_Oblast in 2021: [-217837.44]\n",
      "Actual GDP for Ternopil_Oblast in 2021: 20160.0\n",
      "Epoch 1/50\n",
      "1/1 [==============================] - 0s 382ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 295ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 282ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 286ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 296ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 285ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 341ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 289ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 287ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 313ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 294ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 285ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 290ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 292ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 324ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 289ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 290ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 301ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 288ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 294ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 301ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 290ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 296ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 284ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 326ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 307ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 287ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 281ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 300ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 289ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 296ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 297ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 326ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 301ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 294ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s 285ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s 289ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s 287ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 0s 288ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 0s 290ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 0s 299ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 0s 288ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 0s 334ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 0s 292ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 0s 293ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 0s 289ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - 0s 286ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - 0s 289ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 49/50\n",
      "1/1 [==============================] - 0s 285ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - 0s 285ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "Predicted GDP for Kharkiv_Oblast in 2021: [1.0489986e+08]\n",
      "Actual GDP for Kharkiv_Oblast in 2021: inf\n",
      "Epoch 1/50\n",
      "1/1 [==============================] - 0s 372ms/step - loss: 15456020.0000 - mae: 15456020.0000 - val_loss: 7857059.0000 - val_mae: 7857059.0000\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 16318054.0000 - mae: 16318054.0000 - val_loss: 8144374.0000 - val_mae: 8144374.0000\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 16941790.0000 - mae: 16941790.0000 - val_loss: 8325735.0000 - val_mae: 8325735.0000\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 17348160.0000 - mae: 17348160.0000 - val_loss: 8409753.0000 - val_mae: 8409753.0000\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 17554384.0000 - mae: 17554384.0000 - val_loss: 8404787.0000 - val_mae: 8404787.0000\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 285ms/step - loss: 17577260.0000 - mae: 17577260.0000 - val_loss: 8319225.0000 - val_mae: 8319225.0000\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 17433740.0000 - mae: 17433740.0000 - val_loss: 8161433.5000 - val_mae: 8161433.5000\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 17140812.0000 - mae: 17140812.0000 - val_loss: 7939640.5000 - val_mae: 7939640.5000\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 16715191.0000 - mae: 16715191.0000 - val_loss: 7661767.5000 - val_mae: 7661767.5000\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 16173036.0000 - mae: 16173036.0000 - val_loss: 7335371.0000 - val_mae: 7335371.0000\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 316ms/step - loss: 15529758.0000 - mae: 15529758.0000 - val_loss: 6967525.5000 - val_mae: 6967525.5000\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 297ms/step - loss: 14799821.0000 - mae: 14799821.0000 - val_loss: 6564759.0000 - val_mae: 6564759.0000\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 13996560.0000 - mae: 13996560.0000 - val_loss: 6133048.0000 - val_mae: 6133048.0000\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 13132231.0000 - mae: 13132231.0000 - val_loss: 5677770.5000 - val_mae: 5677770.5000\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 12217838.0000 - mae: 12217838.0000 - val_loss: 5203732.0000 - val_mae: 5203732.0000\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 11263250.0000 - mae: 11263250.0000 - val_loss: 4715162.0000 - val_mae: 4715162.0000\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 10277160.0000 - mae: 10277160.0000 - val_loss: 4215751.5000 - val_mae: 4215751.5000\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 285ms/step - loss: 9267167.0000 - mae: 9267167.0000 - val_loss: 3708635.2500 - val_mae: 3708635.2500\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 336ms/step - loss: 8240129.0000 - mae: 8240129.0000 - val_loss: 3196446.2500 - val_mae: 3196446.2500\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 7200338.0000 - mae: 7200338.0000 - val_loss: 2681346.0000 - val_mae: 2681346.0000\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 6154197.0000 - mae: 6154197.0000 - val_loss: 2214129.7500 - val_mae: 2214129.7500\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 5133074.0000 - mae: 5133074.0000 - val_loss: 1749361.6250 - val_mae: 1749361.6250\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 4142627.5000 - mae: 4142627.5000 - val_loss: 1289523.0000 - val_mae: 1289523.0000\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 3178010.7500 - mae: 3178010.7500 - val_loss: 862472.1250 - val_mae: 862472.1250\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 2246049.2500 - mae: 2246049.2500 - val_loss: 621658.2500 - val_mae: 621658.2500\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 299ms/step - loss: 1419654.2500 - mae: 1419654.2500 - val_loss: 948895.7500 - val_mae: 948895.7500\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 296ms/step - loss: 1068912.8750 - mae: 1068912.8750 - val_loss: 1189142.7500 - val_mae: 1189142.7500\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 328ms/step - loss: 1074481.5000 - mae: 1074481.5000 - val_loss: 1379190.0000 - val_mae: 1379190.0000\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 295ms/step - loss: 1234264.0000 - mae: 1234264.0000 - val_loss: 1462091.6250 - val_mae: 1462091.6250\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 1437407.5000 - mae: 1437407.5000 - val_loss: 1498081.8750 - val_mae: 1498081.8750\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 295ms/step - loss: 1606425.5000 - mae: 1606425.5000 - val_loss: 1523587.6250 - val_mae: 1523587.6250\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 293ms/step - loss: 1719756.5000 - mae: 1719756.5000 - val_loss: 1540341.3750 - val_mae: 1540341.3750\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 299ms/step - loss: 1778335.7500 - mae: 1778335.7500 - val_loss: 1524184.3750 - val_mae: 1524184.3750\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 292ms/step - loss: 1861981.0000 - mae: 1861981.0000 - val_loss: 1505400.8750 - val_mae: 1505400.8750\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 292ms/step - loss: 1911037.5000 - mae: 1911037.5000 - val_loss: 1483802.1250 - val_mae: 1483802.1250\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s 303ms/step - loss: 1928725.1250 - mae: 1928725.1250 - val_loss: 1457775.0000 - val_mae: 1457775.0000\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s 353ms/step - loss: 1927816.2500 - mae: 1927816.2500 - val_loss: 1431859.8750 - val_mae: 1431859.8750\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s 309ms/step - loss: 1910571.8750 - mae: 1910571.8750 - val_loss: 1406183.6250 - val_mae: 1406183.6250\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 0s 325ms/step - loss: 1879677.7500 - mae: 1879677.7500 - val_loss: 1380954.8750 - val_mae: 1380954.8750\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 0s 326ms/step - loss: 1837062.5000 - mae: 1837062.5000 - val_loss: 1355798.7500 - val_mae: 1355798.7500\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 0s 320ms/step - loss: 1784829.2500 - mae: 1784829.2500 - val_loss: 1330412.0000 - val_mae: 1330412.0000\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 0s 306ms/step - loss: 1725215.3750 - mae: 1725215.3750 - val_loss: 1304838.2500 - val_mae: 1304838.2500\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 0s 311ms/step - loss: 1659343.6250 - mae: 1659343.6250 - val_loss: 1277956.8750 - val_mae: 1277956.8750\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 1587040.2500 - mae: 1587040.2500 - val_loss: 1248671.5000 - val_mae: 1248671.5000\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 0s 346ms/step - loss: 1509332.1250 - mae: 1509332.1250 - val_loss: 1218768.1250 - val_mae: 1218768.1250\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 1426208.8750 - mae: 1426208.8750 - val_loss: 1188292.6250 - val_mae: 1188292.6250\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 1336295.5000 - mae: 1336295.5000 - val_loss: 1157324.2500 - val_mae: 1157324.2500\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 1249915.1250 - mae: 1249915.1250 - val_loss: 1126375.2500 - val_mae: 1126375.2500\n",
      "Epoch 49/50\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 1190174.5000 - mae: 1190174.5000 - val_loss: 1095407.1250 - val_mae: 1095407.1250\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 1130374.5000 - mae: 1130374.5000 - val_loss: 1064370.0000 - val_mae: 1064370.0000\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "Predicted GDP for Kherson_Oblast in 2021: [-1328629.]\n",
      "Actual GDP for Kherson_Oblast in 2021: 20624.0\n",
      "Epoch 1/50\n",
      "1/1 [==============================] - 0s 369ms/step - loss: 30901064.0000 - mae: 30901064.0000 - val_loss: 12948469.0000 - val_mae: 12948469.0000\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 344ms/step - loss: 30553764.0000 - mae: 30553764.0000 - val_loss: 12667889.0000 - val_mae: 12667889.0000\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 29952644.0000 - mae: 29952644.0000 - val_loss: 12303966.0000 - val_mae: 12303966.0000\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 297ms/step - loss: 29169344.0000 - mae: 29169344.0000 - val_loss: 11878306.0000 - val_mae: 11878306.0000\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 28250396.0000 - mae: 28250396.0000 - val_loss: 11406985.0000 - val_mae: 11406985.0000\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 285ms/step - loss: 27230596.0000 - mae: 27230596.0000 - val_loss: 10902659.0000 - val_mae: 10902659.0000\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 26137424.0000 - mae: 26137424.0000 - val_loss: 10375558.0000 - val_mae: 10375558.0000\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 24993132.0000 - mae: 24993132.0000 - val_loss: 9833992.0000 - val_mae: 9833992.0000\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 23815870.0000 - mae: 23815870.0000 - val_loss: 9284731.0000 - val_mae: 9284731.0000\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 334ms/step - loss: 22620452.0000 - mae: 22620452.0000 - val_loss: 8733255.0000 - val_mae: 8733255.0000\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 307ms/step - loss: 21418904.0000 - mae: 21418904.0000 - val_loss: 8183987.0000 - val_mae: 8183987.0000\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 293ms/step - loss: 20220976.0000 - mae: 20220976.0000 - val_loss: 7640437.0000 - val_mae: 7640437.0000\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 19034412.0000 - mae: 19034412.0000 - val_loss: 7105356.0000 - val_mae: 7105356.0000\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 17865328.0000 - mae: 17865328.0000 - val_loss: 6580849.5000 - val_mae: 6580849.5000\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 16718407.0000 - mae: 16718407.0000 - val_loss: 6068464.5000 - val_mae: 6068464.5000\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 15597166.0000 - mae: 15597166.0000 - val_loss: 5569308.5000 - val_mae: 5569308.5000\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 293ms/step - loss: 14504105.0000 - mae: 14504105.0000 - val_loss: 5084114.5000 - val_mae: 5084114.5000\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 350ms/step - loss: 13440924.0000 - mae: 13440924.0000 - val_loss: 4613317.0000 - val_mae: 4613317.0000\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 285ms/step - loss: 12408677.0000 - mae: 12408677.0000 - val_loss: 4157069.0000 - val_mae: 4157069.0000\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 11407784.0000 - mae: 11407784.0000 - val_loss: 3715507.2500 - val_mae: 3715507.2500\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 10438234.0000 - mae: 10438234.0000 - val_loss: 3288304.2500 - val_mae: 3288304.2500\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 9499727.0000 - mae: 9499727.0000 - val_loss: 2875039.2500 - val_mae: 2875039.2500\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 8591524.0000 - mae: 8591524.0000 - val_loss: 2475299.7500 - val_mae: 2475299.7500\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 7715132.0000 - mae: 7715132.0000 - val_loss: 2088967.0000 - val_mae: 2088967.0000\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 303ms/step - loss: 6872029.0000 - mae: 6872029.0000 - val_loss: 1728595.3750 - val_mae: 1728595.3750\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 295ms/step - loss: 6060445.5000 - mae: 6060445.5000 - val_loss: 1402410.5000 - val_mae: 1402410.5000\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 333ms/step - loss: 5274372.0000 - mae: 5274372.0000 - val_loss: 1087079.1250 - val_mae: 1087079.1250\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 4521343.5000 - mae: 4521343.5000 - val_loss: 782924.0625 - val_mae: 782924.0625\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 3819196.0000 - mae: 3819196.0000 - val_loss: 488298.1562 - val_mae: 488298.1562\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 3146529.5000 - mae: 3146529.5000 - val_loss: 232656.3438 - val_mae: 232656.3438\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 297ms/step - loss: 2507615.7500 - mae: 2507615.7500 - val_loss: 242084.5625 - val_mae: 242084.5625\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 1892875.6250 - mae: 1892875.6250 - val_loss: 455946.8125 - val_mae: 455946.8125\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 1314358.2500 - mae: 1314358.2500 - val_loss: 661525.1875 - val_mae: 661525.1875\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 876717.3750 - mae: 876717.3750 - val_loss: 830035.5000 - val_mae: 830035.5000\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 285ms/step - loss: 753154.1250 - mae: 753154.1250 - val_loss: 929377.3750 - val_mae: 929377.3750\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s 324ms/step - loss: 790361.0000 - mae: 790361.0000 - val_loss: 1004122.3750 - val_mae: 1004122.3750\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 854988.4375 - mae: 854988.4375 - val_loss: 1054530.1250 - val_mae: 1054530.1250\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s 292ms/step - loss: 985837.5000 - mae: 985837.5000 - val_loss: 1082640.7500 - val_mae: 1082640.7500\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 1129984.1250 - mae: 1129984.1250 - val_loss: 1101537.5000 - val_mae: 1101537.5000\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 1235895.7500 - mae: 1235895.7500 - val_loss: 1115465.7500 - val_mae: 1115465.7500\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 0s 285ms/step - loss: 1318492.1250 - mae: 1318492.1250 - val_loss: 1125039.8750 - val_mae: 1125039.8750\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 1391315.3750 - mae: 1391315.3750 - val_loss: 1129764.7500 - val_mae: 1129764.7500\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 0s 292ms/step - loss: 1446711.7500 - mae: 1446711.7500 - val_loss: 1130284.3750 - val_mae: 1130284.3750\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 0s 334ms/step - loss: 1476588.0000 - mae: 1476588.0000 - val_loss: 1127082.5000 - val_mae: 1127082.5000\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 0s 315ms/step - loss: 1484481.1250 - mae: 1484481.1250 - val_loss: 1120599.6250 - val_mae: 1120599.6250\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 1473345.0000 - mae: 1473345.0000 - val_loss: 1111233.0000 - val_mae: 1111233.0000\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - 0s 297ms/step - loss: 1445342.6250 - mae: 1445342.6250 - val_loss: 1099319.2500 - val_mae: 1099319.2500\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 1402055.5000 - mae: 1402055.5000 - val_loss: 1085174.0000 - val_mae: 1085174.0000\n",
      "Epoch 49/50\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 1345047.5000 - mae: 1345047.5000 - val_loss: 1068685.3750 - val_mae: 1068685.3750\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 1279636.5000 - mae: 1279636.5000 - val_loss: 1051041.5000 - val_mae: 1051041.5000\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "Predicted GDP for Khmelnytskyi_Oblast in 2021: [-2014342.9]\n",
      "Actual GDP for Khmelnytskyi_Oblast in 2021: 28912.0\n",
      "Epoch 1/50\n",
      "1/1 [==============================] - 0s 421ms/step - loss: 11028323.0000 - mae: 11028323.0000 - val_loss: 3850264.0000 - val_mae: 3850264.0000\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 10491874.0000 - mae: 10491874.0000 - val_loss: 3616268.7500 - val_mae: 3616268.7500\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 9697279.0000 - mae: 9697279.0000 - val_loss: 3325512.7500 - val_mae: 3325512.7500\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 8729000.0000 - mae: 8729000.0000 - val_loss: 2993373.2500 - val_mae: 2993373.2500\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 7644679.0000 - mae: 7644679.0000 - val_loss: 2630795.2500 - val_mae: 2630795.2500\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 6502861.5000 - mae: 6502861.5000 - val_loss: 2246568.5000 - val_mae: 2246568.5000\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 5318215.0000 - mae: 5318215.0000 - val_loss: 1847064.8750 - val_mae: 1847064.8750\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 293ms/step - loss: 4129060.2500 - mae: 4129060.2500 - val_loss: 1459949.6250 - val_mae: 1459949.6250\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 3226531.2500 - mae: 3226531.2500 - val_loss: 1096310.8750 - val_mae: 1096310.8750\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 334ms/step - loss: 2837257.7500 - mae: 2837257.7500 - val_loss: 755738.7500 - val_mae: 755738.7500\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 2762357.5000 - mae: 2762357.5000 - val_loss: 447135.9375 - val_mae: 447135.9375\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 2854745.0000 - mae: 2854745.0000 - val_loss: 194881.9688 - val_mae: 194881.9688\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 2919856.5000 - mae: 2919856.5000 - val_loss: 117853.7031 - val_mae: 117853.7031\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 2992790.7500 - mae: 2992790.7500 - val_loss: 285343.1250 - val_mae: 285343.1250\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 3044068.5000 - mae: 3044068.5000 - val_loss: 431712.3438 - val_mae: 431712.3438\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 3062536.0000 - mae: 3062536.0000 - val_loss: 543842.1250 - val_mae: 543842.1250\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 3055676.5000 - mae: 3055676.5000 - val_loss: 642133.8750 - val_mae: 642133.8750\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 313ms/step - loss: 3094827.2500 - mae: 3094827.2500 - val_loss: 719367.5625 - val_mae: 719367.5625\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 285ms/step - loss: 3114165.5000 - mae: 3114165.5000 - val_loss: 767063.2500 - val_mae: 767063.2500\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 307ms/step - loss: 3113590.0000 - mae: 3113590.0000 - val_loss: 798421.0000 - val_mae: 798421.0000\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 285ms/step - loss: 3096507.5000 - mae: 3096507.5000 - val_loss: 821196.4375 - val_mae: 821196.4375\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 3063160.5000 - mae: 3063160.5000 - val_loss: 832757.6875 - val_mae: 832757.6875\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 308ms/step - loss: 3021736.5000 - mae: 3021736.5000 - val_loss: 838082.4375 - val_mae: 838082.4375\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 2973153.5000 - mae: 2973153.5000 - val_loss: 831508.0625 - val_mae: 831508.0625\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 297ms/step - loss: 2920176.7500 - mae: 2920176.7500 - val_loss: 822389.8750 - val_mae: 822389.8750\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 334ms/step - loss: 2863672.5000 - mae: 2863672.5000 - val_loss: 811159.0625 - val_mae: 811159.0625\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 312ms/step - loss: 2806430.0000 - mae: 2806430.0000 - val_loss: 797962.8750 - val_mae: 797962.8750\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 292ms/step - loss: 2746873.5000 - mae: 2746873.5000 - val_loss: 782347.8125 - val_mae: 782347.8125\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 2684557.5000 - mae: 2684557.5000 - val_loss: 763635.6875 - val_mae: 763635.6875\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 304ms/step - loss: 2620439.0000 - mae: 2620439.0000 - val_loss: 743940.3125 - val_mae: 743940.3125\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 2554925.7500 - mae: 2554925.7500 - val_loss: 723392.8750 - val_mae: 723392.8750\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 2488573.0000 - mae: 2488573.0000 - val_loss: 702102.0625 - val_mae: 702102.0625\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 2421703.0000 - mae: 2421703.0000 - val_loss: 680206.0625 - val_mae: 680206.0625\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 340ms/step - loss: 2355322.5000 - mae: 2355322.5000 - val_loss: 657860.7500 - val_mae: 657860.7500\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 2288239.0000 - mae: 2288239.0000 - val_loss: 635117.6250 - val_mae: 635117.6250\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 2220636.2500 - mae: 2220636.2500 - val_loss: 612051.6875 - val_mae: 612051.6875\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s 292ms/step - loss: 2152626.5000 - mae: 2152626.5000 - val_loss: 588692.8125 - val_mae: 588692.8125\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 2084256.5000 - mae: 2084256.5000 - val_loss: 565049.3125 - val_mae: 565049.3125\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 2015616.3750 - mae: 2015616.3750 - val_loss: 541152.3125 - val_mae: 541152.3125\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 1945997.0000 - mae: 1945997.0000 - val_loss: 516935.8125 - val_mae: 516935.8125\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 0s 312ms/step - loss: 1875928.5000 - mae: 1875928.5000 - val_loss: 492431.8125 - val_mae: 492431.8125\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 0s 317ms/step - loss: 1805592.0000 - mae: 1805592.0000 - val_loss: 467670.4062 - val_mae: 467670.4062\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 0s 306ms/step - loss: 1734816.0000 - mae: 1734816.0000 - val_loss: 442473.0938 - val_mae: 442473.0938\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 1660941.7500 - mae: 1660941.7500 - val_loss: 416801.3125 - val_mae: 416801.3125\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 1584423.0000 - mae: 1584423.0000 - val_loss: 390496.2500 - val_mae: 390496.2500\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 0s 299ms/step - loss: 1505577.0000 - mae: 1505577.0000 - val_loss: 363606.3438 - val_mae: 363606.3438\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - 0s 297ms/step - loss: 1435743.5000 - mae: 1435743.5000 - val_loss: 337901.6562 - val_mae: 337901.6562\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 1383588.7500 - mae: 1383588.7500 - val_loss: 312510.2188 - val_mae: 312510.2188\n",
      "Epoch 49/50\n",
      "1/1 [==============================] - 0s 297ms/step - loss: 1331533.7500 - mae: 1331533.7500 - val_loss: 287611.4375 - val_mae: 287611.4375\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - 0s 334ms/step - loss: 1279398.3750 - mae: 1279398.3750 - val_loss: 263728.8750 - val_mae: 263728.8750\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Predicted GDP for Cherkasy_Oblast in 2021: [-886243.9]\n",
      "Actual GDP for Cherkasy_Oblast in 2021: 32992.0\n",
      "Epoch 1/50\n",
      "1/1 [==============================] - 0s 370ms/step - loss: 12193284.0000 - mae: 12193284.0000 - val_loss: 4962551.0000 - val_mae: 4962551.0000\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 12040244.0000 - mae: 12040244.0000 - val_loss: 4858156.0000 - val_mae: 4858156.0000\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 298ms/step - loss: 11811302.0000 - mae: 11811302.0000 - val_loss: 4730634.5000 - val_mae: 4730634.5000\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 292ms/step - loss: 11524345.0000 - mae: 11524345.0000 - val_loss: 4583448.0000 - val_mae: 4583448.0000\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 11191110.0000 - mae: 11191110.0000 - val_loss: 4421208.0000 - val_mae: 4421208.0000\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 10822988.0000 - mae: 10822988.0000 - val_loss: 4247578.0000 - val_mae: 4247578.0000\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 315ms/step - loss: 10431449.0000 - mae: 10431449.0000 - val_loss: 4067264.2500 - val_mae: 4067264.2500\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 296ms/step - loss: 10019910.0000 - mae: 10019910.0000 - val_loss: 3880993.5000 - val_mae: 3880993.5000\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 295ms/step - loss: 9591684.0000 - mae: 9591684.0000 - val_loss: 3690121.5000 - val_mae: 3690121.5000\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 9151612.0000 - mae: 9151612.0000 - val_loss: 3496373.7500 - val_mae: 3496373.7500\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 8703754.0000 - mae: 8703754.0000 - val_loss: 3301173.2500 - val_mae: 3301173.2500\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 8251493.5000 - mae: 8251493.5000 - val_loss: 3105705.2500 - val_mae: 3105705.2500\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 296ms/step - loss: 7797659.5000 - mae: 7797659.5000 - val_loss: 2910931.0000 - val_mae: 2910931.0000\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 299ms/step - loss: 7344566.5000 - mae: 7344566.5000 - val_loss: 2717708.0000 - val_mae: 2717708.0000\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 6894140.5000 - mae: 6894140.5000 - val_loss: 2526630.5000 - val_mae: 2526630.5000\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 304ms/step - loss: 6447922.0000 - mae: 6447922.0000 - val_loss: 2338163.5000 - val_mae: 2338163.5000\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 6007139.0000 - mae: 6007139.0000 - val_loss: 2152691.5000 - val_mae: 2152691.5000\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 5572747.5000 - mae: 5572747.5000 - val_loss: 1970506.0000 - val_mae: 1970506.0000\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 279ms/step - loss: 5145485.0000 - mae: 5145485.0000 - val_loss: 1791814.6250 - val_mae: 1791814.6250\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 4725897.0000 - mae: 4725897.0000 - val_loss: 1616745.5000 - val_mae: 1616745.5000\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 4314659.0000 - mae: 4314659.0000 - val_loss: 1445430.5000 - val_mae: 1445430.5000\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 3912194.0000 - mae: 3912194.0000 - val_loss: 1277910.3750 - val_mae: 1277910.3750\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 3518424.0000 - mae: 3518424.0000 - val_loss: 1114190.2500 - val_mae: 1114190.2500\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 331ms/step - loss: 3133190.2500 - mae: 3133190.2500 - val_loss: 964910.8750 - val_mae: 964910.8750\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 2756129.2500 - mae: 2756129.2500 - val_loss: 825752.0000 - val_mae: 825752.0000\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 2413324.0000 - mae: 2413324.0000 - val_loss: 690556.7500 - val_mae: 690556.7500\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 2092563.2500 - mae: 2092563.2500 - val_loss: 559195.9375 - val_mae: 559195.9375\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 1781333.7500 - mae: 1781333.7500 - val_loss: 431408.2812 - val_mae: 431408.2812\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 292ms/step - loss: 1478438.2500 - mae: 1478438.2500 - val_loss: 306949.7812 - val_mae: 306949.7812\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 1183293.1250 - mae: 1183293.1250 - val_loss: 203346.3750 - val_mae: 203346.3750\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 293ms/step - loss: 895491.8750 - mae: 895491.8750 - val_loss: 105168.7578 - val_mae: 105168.7578\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 340ms/step - loss: 631186.2500 - mae: 631186.2500 - val_loss: 14338.4336 - val_mae: 14338.4336\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 392803.5625 - mae: 392803.5625 - val_loss: 105078.5938 - val_mae: 105078.5938\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 285ms/step - loss: 173651.5625 - mae: 173651.5625 - val_loss: 183192.1875 - val_mae: 183192.1875\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 135382.9844 - mae: 135382.9844 - val_loss: 251605.4375 - val_mae: 251605.4375\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 221281.0781 - mae: 221281.0781 - val_loss: 306442.9375 - val_mae: 306442.9375\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 361927.8125 - mae: 361927.8125 - val_loss: 349285.3438 - val_mae: 349285.3438\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 468996.2500 - mae: 468996.2500 - val_loss: 374073.3125 - val_mae: 374073.3125\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 548100.8750 - mae: 548100.8750 - val_loss: 386357.5625 - val_mae: 386357.5625\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 0s 339ms/step - loss: 604768.3125 - mae: 604768.3125 - val_loss: 393975.7812 - val_mae: 393975.7812\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 641731.3750 - mae: 641731.3750 - val_loss: 397468.5312 - val_mae: 397468.5312\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 661211.6250 - mae: 661211.6250 - val_loss: 397297.5625 - val_mae: 397297.5625\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 0s 280ms/step - loss: 665156.6250 - mae: 665156.6250 - val_loss: 393874.0938 - val_mae: 393874.0938\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 0s 293ms/step - loss: 655281.7500 - mae: 655281.7500 - val_loss: 387547.1250 - val_mae: 387547.1250\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 633076.6250 - mae: 633076.6250 - val_loss: 378645.0625 - val_mae: 378645.0625\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 0s 281ms/step - loss: 599916.1250 - mae: 599916.1250 - val_loss: 367448.0312 - val_mae: 367448.0312\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 556983.6250 - mae: 556983.6250 - val_loss: 354049.5625 - val_mae: 354049.5625\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - 0s 327ms/step - loss: 505328.3750 - mae: 505328.3750 - val_loss: 336100.5000 - val_mae: 336100.5000\n",
      "Epoch 49/50\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 445888.6562 - mae: 445888.6562 - val_loss: 310841.9062 - val_mae: 310841.9062\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - 0s 292ms/step - loss: 379500.4688 - mae: 379500.4688 - val_loss: 280841.7812 - val_mae: 280841.7812\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "Predicted GDP for Chernivtsi_Oblast in 2021: [-701588.25]\n",
      "Actual GDP for Chernivtsi_Oblast in 2021: 13808.0\n",
      "Epoch 1/50\n",
      "1/1 [==============================] - 0s 350ms/step - loss: 781414.1250 - mae: 781414.1250 - val_loss: 68474.7891 - val_mae: 68474.7891\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 293ms/step - loss: 751799.1250 - mae: 751799.1250 - val_loss: 87057.1719 - val_mae: 87057.1719\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 295ms/step - loss: 701108.3750 - mae: 701108.3750 - val_loss: 111080.8203 - val_mae: 111080.8203\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 644576.2500 - mae: 644576.2500 - val_loss: 135806.6719 - val_mae: 135806.6719\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 292ms/step - loss: 577859.3750 - mae: 577859.3750 - val_loss: 158146.7812 - val_mae: 158146.7812\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 293ms/step - loss: 503101.8750 - mae: 503101.8750 - val_loss: 177723.7969 - val_mae: 177723.7969\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 339ms/step - loss: 424464.8750 - mae: 424464.8750 - val_loss: 193457.0156 - val_mae: 193457.0156\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 299ms/step - loss: 363205.4375 - mae: 363205.4375 - val_loss: 204466.0312 - val_mae: 204466.0312\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 293ms/step - loss: 303391.1250 - mae: 303391.1250 - val_loss: 210706.2500 - val_mae: 210706.2500\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 246993.9375 - mae: 246993.9375 - val_loss: 217078.9062 - val_mae: 217078.9062\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 297ms/step - loss: 203860.4062 - mae: 203860.4062 - val_loss: 222939.5312 - val_mae: 222939.5312\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 280ms/step - loss: 188148.9531 - mae: 188148.9531 - val_loss: 227946.7188 - val_mae: 227946.7188\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 326ms/step - loss: 191085.2031 - mae: 191085.2031 - val_loss: 232186.7500 - val_mae: 232186.7500\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 211269.2812 - mae: 211269.2812 - val_loss: 234919.2812 - val_mae: 234919.2812\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 234196.5938 - mae: 234196.5938 - val_loss: 236328.4219 - val_mae: 236328.4219\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 249299.1875 - mae: 249299.1875 - val_loss: 236606.0781 - val_mae: 236606.0781\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 258689.8750 - mae: 258689.8750 - val_loss: 235870.4062 - val_mae: 235870.4062\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 263121.0938 - mae: 263121.0938 - val_loss: 234284.7031 - val_mae: 234284.7031\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 263019.7188 - mae: 263019.7188 - val_loss: 231949.7500 - val_mae: 231949.7500\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 259395.2812 - mae: 259395.2812 - val_loss: 228993.7031 - val_mae: 228993.7031\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 337ms/step - loss: 252518.9688 - mae: 252518.9688 - val_loss: 225413.3594 - val_mae: 225413.3594\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 242256.5469 - mae: 242256.5469 - val_loss: 221264.8125 - val_mae: 221264.8125\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 229062.2656 - mae: 229062.2656 - val_loss: 216610.1094 - val_mae: 216610.1094\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 297ms/step - loss: 213220.6719 - mae: 213220.6719 - val_loss: 211507.7188 - val_mae: 211507.7188\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 285ms/step - loss: 194757.7500 - mae: 194757.7500 - val_loss: 205989.1250 - val_mae: 205989.1250\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 295ms/step - loss: 179500.0625 - mae: 179500.0625 - val_loss: 200467.3594 - val_mae: 200467.3594\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 175448.7656 - mae: 175448.7656 - val_loss: 195286.5469 - val_mae: 195286.5469\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 172678.6562 - mae: 172678.6562 - val_loss: 190393.3906 - val_mae: 190393.3906\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 336ms/step - loss: 172991.2812 - mae: 172991.2812 - val_loss: 186149.5000 - val_mae: 186149.5000\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 295ms/step - loss: 177132.7500 - mae: 177132.7500 - val_loss: 182486.0312 - val_mae: 182486.0312\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 180008.5625 - mae: 180008.5625 - val_loss: 179346.2656 - val_mae: 179346.2656\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 182523.4375 - mae: 182523.4375 - val_loss: 176965.3750 - val_mae: 176965.3750\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 184775.3906 - mae: 184775.3906 - val_loss: 175265.0000 - val_mae: 175265.0000\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 184706.1562 - mae: 184706.1562 - val_loss: 174175.5156 - val_mae: 174175.5156\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 285ms/step - loss: 182557.4062 - mae: 182557.4062 - val_loss: 173633.3906 - val_mae: 173633.3906\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 178551.5781 - mae: 178551.5781 - val_loss: 173581.5469 - val_mae: 173581.5469\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s 346ms/step - loss: 174104.4531 - mae: 174104.4531 - val_loss: 173695.8906 - val_mae: 173695.8906\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 170111.8281 - mae: 170111.8281 - val_loss: 173959.1094 - val_mae: 173959.1094\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 165810.1094 - mae: 165810.1094 - val_loss: 174355.6875 - val_mae: 174355.6875\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 161227.6875 - mae: 161227.6875 - val_loss: 174872.2031 - val_mae: 174872.2031\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 157895.8750 - mae: 157895.8750 - val_loss: 175116.2031 - val_mae: 175116.2031\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 157625.1562 - mae: 157625.1562 - val_loss: 175100.7812 - val_mae: 175100.7812\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 157316.9375 - mae: 157316.9375 - val_loss: 174851.1250 - val_mae: 174851.1250\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 0s 304ms/step - loss: 156860.9375 - mae: 156860.9375 - val_loss: 174392.4688 - val_mae: 174392.4688\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 0s 298ms/step - loss: 156271.2812 - mae: 156271.2812 - val_loss: 173745.0938 - val_mae: 173745.0938\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 0s 316ms/step - loss: 155443.8750 - mae: 155443.8750 - val_loss: 172953.0312 - val_mae: 172953.0312\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - 0s 317ms/step - loss: 154494.0000 - mae: 154494.0000 - val_loss: 172031.5938 - val_mae: 172031.5938\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 153488.2812 - mae: 153488.2812 - val_loss: 170992.1250 - val_mae: 170992.1250\n",
      "Epoch 49/50\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 152444.0156 - mae: 152444.0156 - val_loss: 169848.2812 - val_mae: 169848.2812\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - 0s 303ms/step - loss: 151665.7188 - mae: 151665.7188 - val_loss: 168262.7656 - val_mae: 168262.7656\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "Predicted GDP for Chernihiv_Oblast in 2021: [-350486.47]\n",
      "Actual GDP for Chernihiv_Oblast in 2021: 23264.0\n",
      "Epoch 1/50\n",
      "1/1 [==============================] - 0s 379ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 344ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 295ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 291ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 301ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 300ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 293ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 293ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 289ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 335ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 290ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 307ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 294ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 285ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 287ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 286ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 336ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 289ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 292ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 289ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 288ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 293ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 293ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 332ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 288ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 294ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 288ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 292ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 309ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 289ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 321ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 284ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 292ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 292ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 296ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s 288ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s 286ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s 360ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 0s 286ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 0s 299ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 0s 289ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 0s 284ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 0s 290ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 0s 285ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 0s 288ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 0s 320ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - 0s 296ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - 0s 282ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 49/50\n",
      "1/1 [==============================] - 0s 302ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - 0s 311ms/step - loss: inf - mae: inf - val_loss: inf - val_mae: inf\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "Predicted GDP for Kyiv in 2021: [1.2607003e+09]\n",
      "Actual GDP for Kyiv in 2021: inf\n"
     ]
    }
   ],
   "source": [
    "# load clean gdp data\n",
    "gdp = pd.read_csv(\"data/tabular_data_ukraine.csv\")\n",
    "\n",
    "# initialise a dictionary to store the results\n",
    "results = {\"region\": [], \"predicted_gdp\": [], \"actual_gdp\": [], \"mae\": [], \"percentage_error\": []}\n",
    "\n",
    "# delete observations with year > 2021\n",
    "gdp = gdp[gdp[\"year\"].astype(int) <= 2021]\n",
    "\n",
    "# get unique regions and years\n",
    "regions = gdp[\"region\"].unique()\n",
    "# regions = ['Cherkasy_Oblast']\n",
    "years = gdp[\"year\"].unique()\n",
    "\n",
    "# load the snow covered and snow free images, add them together and append to the list\n",
    "for region in regions:\n",
    "\n",
    "    # initialise data\n",
    "    X = np.zeros((len(years), 765, 1076, 1))\n",
    "    X_test = np.zeros((1, 765, 1076, 1))\n",
    "    y = np.zeros(len(years))\n",
    "\n",
    "    for i in range(len(years)):\n",
    "\n",
    "        year = years[i]\n",
    "        gdp_value = gdp[(gdp[\"region\"] == region) & (gdp[\"year\"] == year)][\"real_gdp\"].values[0]\n",
    "\n",
    "        # get the file name\n",
    "        file_name = f\"{year}_{region}_hq.h5\"\n",
    "\n",
    "        # load the image\n",
    "        file_path = f\"data/annual_region_images/{file_name}\"\n",
    "    \n",
    "        with h5py.File(file_path, 'r') as annual_region:\n",
    "            # nearnad_snow_cov = annual_region[\"NearNadir_Composite_Snow_Covered\"][:]\n",
    "            # nearnad_snow_free = annual_region[\"NearNadir_Composite_Snow_Free\"][:]\n",
    "            # offnad_snow_cov = annual_region[\"OffNadir_Composite_Snow_Covered\"][:]\n",
    "            # offnad_snow_free = annual_region[\"OffNadir_Composite_Snow_Free\"][:]\n",
    "            # allangle_snow_cov = annual_region[\"AllAngle_Composite_Snow_Covered\"][:]\n",
    "            allangle_snow_free = annual_region[\"AllAngle_Composite_Snow_Free\"][:]\n",
    "\n",
    "            # add the two images together\n",
    "            # combined = snow_covered + snow_free\n",
    "\n",
    "        # add the gdp value to y\n",
    "\n",
    "        if year != 2021:\n",
    "            y[i] = gdp_value\n",
    "\n",
    "            # append both images as two channels to to X\n",
    "            # X[i, :, :, 0] = allangle_snow_cov\n",
    "            X[i, :, :, 0] = allangle_snow_free\n",
    "            # X[i, :, :, 2] = offnad_snow_cov\n",
    "            # X[i, :, :, 0] = offnad_snow_free \n",
    "            # X[i, :, :, 4] = nearnad_snow_cov\n",
    "            # X[i, :, :, 1] = nearnad_snow_free \n",
    "        else:\n",
    "            X_test[0, :, :, 0] = allangle_snow_free\n",
    "            # X_test[0, :, :, 1] = nearnad_snow_free\n",
    "\n",
    "            y_test = gdp_value\n",
    "\n",
    "        # # Normalise the images\n",
    "        # X[:, :, :, 0] = X[:, :, :, 0] / X[:, :, :, 0].max()\n",
    "        # X[:, :, :, 1] = X[:, :, :, 1] / X[:, :, :, 1].max()\n",
    "\n",
    "        # X_test[:, :, :, 0] = X_test[:, :, :, 0] / X[:, :, :, 0].max()\n",
    "        # X_test[:, :, :, 1] = X_test[:, :, :, 1] / X[:, :, :, 1].max()\n",
    "\n",
    "        # # normalise GDP values\n",
    "        # y = y / y.max()\n",
    "        # y_test = y_test / y.max()\n",
    "\n",
    "\n",
    "\n",
    "    # fit the model on the data\n",
    "    model.fit(X, y, epochs=50, batch_size=16, validation_split=0.2)  # Assuming you have a validation split of 20%\n",
    "\n",
    "    # predict the gdp for 2021\n",
    "    y_hat = model.predict(X_test).flatten()\n",
    "\n",
    "    print(f\"Predicted GDP for {region} in 2021: {y_hat}\")\n",
    "    print(f\"Actual GDP for {region} in 2021: {y_test}\")\n",
    "\n",
    "    # add the results to the dictionary\n",
    "    mae = np.mean(np.abs(y_test - y_hat))\n",
    "    percentage_error = np.mean(100*np.abs((y_test - y_hat) / y_test))\n",
    "\n",
    "    results[\"region\"].append(region)\n",
    "    results[\"predicted_gdp\"].append(y_hat)\n",
    "    results[\"actual_gdp\"].append(y_test)\n",
    "    results[\"mae\"].append(mae)\n",
    "    results[\"percentage_error\"].append(percentage_error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'region': ['Vinnytsia_Oblast',\n",
       "  'Volyn_Oblast',\n",
       "  'Dnipropetrovsk_Oblast',\n",
       "  'Donetsk_Oblast',\n",
       "  'Zhytomyr_Oblast',\n",
       "  'Zakarpattia_Oblast',\n",
       "  'Zaporizhia_Oblast',\n",
       "  'Ivano-Frankivsk_Oblast',\n",
       "  'Kyiv_Oblast',\n",
       "  'Kirovohrad_Oblast',\n",
       "  'Luhansk_Oblast',\n",
       "  'Lviv_Oblast',\n",
       "  'Mykolaiv_Oblast',\n",
       "  'Odessa_Oblast',\n",
       "  'Poltava_Oblast',\n",
       "  'Rivne_Oblast',\n",
       "  'Sumy_Oblast',\n",
       "  'Ternopil_Oblast',\n",
       "  'Kharkiv_Oblast',\n",
       "  'Kherson_Oblast',\n",
       "  'Khmelnytskyi_Oblast',\n",
       "  'Cherkasy_Oblast',\n",
       "  'Chernivtsi_Oblast',\n",
       "  'Chernihiv_Oblast',\n",
       "  'Kyiv'],\n",
       " 'predicted_gdp': [array([43416.574], dtype=float32),\n",
       "  array([25596.617], dtype=float32),\n",
       "  array([49831784.], dtype=float32),\n",
       "  array([3.258329e+08], dtype=float32),\n",
       "  array([24858146.], dtype=float32),\n",
       "  array([249348.78], dtype=float32),\n",
       "  array([-717731.8], dtype=float32),\n",
       "  array([46669.582], dtype=float32),\n",
       "  array([35730348.], dtype=float32),\n",
       "  array([427689.3], dtype=float32),\n",
       "  array([995587.94], dtype=float32),\n",
       "  array([1451139.4], dtype=float32),\n",
       "  array([127920.99], dtype=float32),\n",
       "  array([76986560.], dtype=float32),\n",
       "  array([2187428.2], dtype=float32),\n",
       "  array([-317425.62], dtype=float32),\n",
       "  array([-284474.47], dtype=float32),\n",
       "  array([-217837.44], dtype=float32),\n",
       "  array([1.0489986e+08], dtype=float32),\n",
       "  array([-1328629.], dtype=float32),\n",
       "  array([-2014342.9], dtype=float32),\n",
       "  array([-886243.9], dtype=float32),\n",
       "  array([-701588.25], dtype=float32),\n",
       "  array([-350486.47], dtype=float32),\n",
       "  array([1.2607003e+09], dtype=float32)],\n",
       " 'actual_gdp': [43360.0,\n",
       "  19150.0,\n",
       "  inf,\n",
       "  64000.0,\n",
       "  30060.0,\n",
       "  20980.0,\n",
       "  54820.0,\n",
       "  33120.0,\n",
       "  inf,\n",
       "  25140.0,\n",
       "  15880.0,\n",
       "  inf,\n",
       "  32560.0,\n",
       "  inf,\n",
       "  45920.0,\n",
       "  22900.0,\n",
       "  25100.0,\n",
       "  20160.0,\n",
       "  inf,\n",
       "  20620.0,\n",
       "  28910.0,\n",
       "  33000.0,\n",
       "  13810.0,\n",
       "  23260.0,\n",
       "  inf],\n",
       " 'mae': [56.57422,\n",
       "  6444.617,\n",
       "  inf,\n",
       "  325768900.0,\n",
       "  24828082.0,\n",
       "  228372.78,\n",
       "  772547.8,\n",
       "  13549.582,\n",
       "  inf,\n",
       "  402553.3,\n",
       "  979707.94,\n",
       "  inf,\n",
       "  95360.99,\n",
       "  inf,\n",
       "  2141508.2,\n",
       "  340321.62,\n",
       "  309578.47,\n",
       "  237997.44,\n",
       "  inf,\n",
       "  1349253.0,\n",
       "  2043254.9,\n",
       "  919235.9,\n",
       "  715396.25,\n",
       "  373750.47,\n",
       "  inf],\n",
       " 'percentage_error': [0.1304756,\n",
       "  33.649837,\n",
       "  nan,\n",
       "  509013.9,\n",
       "  82584.09,\n",
       "  1088.7336,\n",
       "  1409.3473,\n",
       "  40.910576,\n",
       "  nan,\n",
       "  1601.5011,\n",
       "  6169.4453,\n",
       "  nan,\n",
       "  292.87775,\n",
       "  nan,\n",
       "  4663.563,\n",
       "  1486.3802,\n",
       "  1233.1838,\n",
       "  1180.5428,\n",
       "  nan,\n",
       "  6542.15,\n",
       "  7067.152,\n",
       "  2786.2388,\n",
       "  5181.0273,\n",
       "  1606.5615,\n",
       "  nan]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.291629"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percentage_error = results[\"percentage_error\"]\n",
    "np.mean(percentage_error) # 7.3% mean percentage error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "\n",
    "# load clean gdp data\n",
    "gdp = pd.read_csv(\"data/clean_gdp.csv\")\n",
    "\n",
    "# Initialise a three dimensional array to store the images with the shape (number of images, height, width, channels)\n",
    "X = np.zeros((len(gdp), 765, 1076, 4))\n",
    "y = np.zeros(len(gdp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest and other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the tabular data\n",
    "data = pd.read_csv(\"data/tabular_data_ukraine.csv\")\n",
    "\n",
    "# turn the region column into a categorical variable using one hot encoding\n",
    "data = pd.get_dummies(data, columns=[\"region\"])\n",
    "\n",
    "# training data contains years until 2021\n",
    "train_data = data[data[\"year\"] <= 2021]\n",
    "test_data = data[data[\"year\"] >= 2022]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.018571927221726226\n",
      "14.78645497172508\n"
     ]
    }
   ],
   "source": [
    "# train a random forest model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# drop year and gdp columns\n",
    "X = train_data.drop(columns=[\"year\", \"real_gdp\"])\n",
    "y = train_data[\"real_gdp\"]\n",
    "\n",
    "# standardise gdp\n",
    "# y_mean = y.mean()\n",
    "# y_std = y.std()\n",
    "# y = (y - y_mean) / y_std\n",
    "\n",
    "y_max = y.max()\n",
    "y = y / y_max\n",
    "\n",
    "# take randomly 80% of the data for training\n",
    "train_size = int(0.8 * len(train_data))\n",
    "test_size = len(train_data) - train_size\n",
    "\n",
    "train_indices = np.random.choice(len(train_data), train_size, replace=False)\n",
    "test_indices = np.setdiff1d(np.arange(len(train_data)), train_indices)\n",
    "\n",
    "X_train = X.iloc[train_indices]\n",
    "y_train = y.iloc[train_indices]\n",
    "X_test = X.iloc[test_indices]\n",
    "y_test = y.iloc[test_indices]\n",
    "\n",
    "# train the model\n",
    "rf_model = RandomForestRegressor(n_estimators=1000, random_state = 0)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# get the predictions\n",
    "y_hat = rf_model.predict(X_test)\n",
    "\n",
    "# compute the mae\n",
    "mae = np.mean(np.abs(y_test - y_hat))\n",
    "\n",
    "# compute the mean percentage error\n",
    "percentage_error = np.mean(100*np.abs((y_test - y_hat) / y_test))\n",
    "\n",
    "print(mae)\n",
    "print(percentage_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_316/1922239168.py:6: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  model.fit(X_train, y_train)\n",
      "c:\\Users\\jakub\\anaconda\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:530: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01835888717836245\n",
      "12.318338447454858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jakub\\anaconda\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.03104387917313304, tolerance: 0.0006106950436903142\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "# fit a lasso regression model \n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "model = Lasso(alpha=0)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# get the predictions\n",
    "y_hat = model.predict(X_test)\n",
    "\n",
    "# compute the mae\n",
    "mae = np.mean(np.abs(y_test - y_hat))\n",
    "\n",
    "# compute the mean percentage error\n",
    "percentage_error = np.mean(100*np.abs((y_test - y_hat) / y_test))\n",
    "\n",
    "print(mae)\n",
    "print(percentage_error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01538115075906231\n",
      "11.288293273346223\n"
     ]
    }
   ],
   "source": [
    "# fit a XGBoost model\n",
    "import xgboost as xgb\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(objective=\"reg:squarederror\", random_state=0)\n",
    "\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# get the predictions\n",
    "y_hat = xgb_model.predict(X_test)\n",
    "\n",
    "# compute the mae\n",
    "mae = np.mean(np.abs(y_test - y_hat))\n",
    "\n",
    "# compute the mean percentage error\n",
    "percentage_error = np.mean(100*np.abs((y_test - y_hat) / y_test))\n",
    "\n",
    "print(mae)\n",
    "print(percentage_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[col] = X_train[col].astype(int)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[col] = X_train[col].astype(int)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[col] = X_train[col].astype(int)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[col] = X_train[col].astype(int)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[col] = X_train[col].astype(int)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[col] = X_train[col].astype(int)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[col] = X_train[col].astype(int)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[col] = X_train[col].astype(int)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[col] = X_train[col].astype(int)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[col] = X_train[col].astype(int)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[col] = X_train[col].astype(int)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[col] = X_train[col].astype(int)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[col] = X_train[col].astype(int)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[col] = X_train[col].astype(int)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[col] = X_train[col].astype(int)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[col] = X_train[col].astype(int)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[col] = X_train[col].astype(int)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[col] = X_train[col].astype(int)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[col] = X_train[col].astype(int)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[col] = X_train[col].astype(int)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[col] = X_train[col].astype(int)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[col] = X_train[col].astype(int)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[col] = X_train[col].astype(int)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[col] = X_train[col].astype(int)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[col] = X_train[col].astype(int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:From c:\\Users\\jakub\\anaconda\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "5/5 [==============================] - 2s 73ms/step - loss: 61967528099840.0000 - mae: 5316903.0000 - val_loss: 70135830282240.0000 - val_mae: 5151166.0000\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 61967184166912.0000 - mae: 5316874.5000 - val_loss: 70135326965760.0000 - val_mae: 5151121.5000\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 61966458552320.0000 - mae: 5316811.0000 - val_loss: 70134311944192.0000 - val_mae: 5151032.5000\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 61965162512384.0000 - mae: 5316689.0000 - val_loss: 70132260929536.0000 - val_mae: 5150853.0000\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 61962226499584.0000 - mae: 5316441.5000 - val_loss: 70128213426176.0000 - val_mae: 5150499.0000\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 61957075894272.0000 - mae: 5315957.0000 - val_loss: 70120185528320.0000 - val_mae: 5149804.0000\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 61946640465920.0000 - mae: 5315005.0000 - val_loss: 70104469471232.0000 - val_mae: 5148452.0000\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 61926910459904.0000 - mae: 5313211.0000 - val_loss: 70074853490688.0000 - val_mae: 5145915.5000\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 61889350467584.0000 - mae: 5309818.5000 - val_loss: 70021342560256.0000 - val_mae: 5141353.0000\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 61824644939776.0000 - mae: 5303982.0000 - val_loss: 69927771832320.0000 - val_mae: 5133415.5000\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 61705908387840.0000 - mae: 5293576.0000 - val_loss: 69768765767680.0000 - val_mae: 5119957.0000\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 61510298632192.0000 - mae: 5276333.5000 - val_loss: 69504658833408.0000 - val_mae: 5097642.5000\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 61213497098240.0000 - mae: 5249571.0000 - val_loss: 69077515108352.0000 - val_mae: 5061492.0000\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 60708465147904.0000 - mae: 5205500.0000 - val_loss: 68425049178112.0000 - val_mae: 5005767.0000\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 60017457758208.0000 - mae: 5134803.0000 - val_loss: 67455036686336.0000 - val_mae: 4921823.0000\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 58958853177344.0000 - mae: 5035533.0000 - val_loss: 66020983177216.0000 - val_mae: 4795290.5000\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 57394507808768.0000 - mae: 4880711.5000 - val_loss: 63917468418048.0000 - val_mae: 4603982.5000\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 55065092030464.0000 - mae: 4661265.0000 - val_loss: 61045720743936.0000 - val_mae: 4329786.5000\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 52275330416640.0000 - mae: 4364453.5000 - val_loss: 57265751064576.0000 - val_mae: 3941495.2500\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 48183493263360.0000 - mae: 3930869.2500 - val_loss: 52543010897920.0000 - val_mae: 3418556.7500\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 43460757291008.0000 - mae: 3499256.7500 - val_loss: 47276227559424.0000 - val_mae: 2985466.5000\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 38957257588736.0000 - mae: 3169831.5000 - val_loss: 42398533025792.0000 - val_mae: 2773229.5000\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 35193217875968.0000 - mae: 3126442.5000 - val_loss: 38898101125120.0000 - val_mae: 2929644.2500\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 34060258770944.0000 - mae: 3444804.0000 - val_loss: 37351988396032.0000 - val_mae: 3359900.5000\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 33696572768256.0000 - mae: 3629973.2500 - val_loss: 36701938384896.0000 - val_mae: 3408358.5000\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 32962032697344.0000 - mae: 3695509.5000 - val_loss: 36014022197248.0000 - val_mae: 3290199.2500\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 31580531720192.0000 - mae: 3449490.0000 - val_loss: 35592490450944.0000 - val_mae: 3096213.5000\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 31063833313280.0000 - mae: 3336971.2500 - val_loss: 35190822928384.0000 - val_mae: 3017992.0000\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 32354844278784.0000 - mae: 3349326.0000 - val_loss: 34867091865600.0000 - val_mae: 2958705.0000\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 30585126912000.0000 - mae: 3248770.5000 - val_loss: 34324804009984.0000 - val_mae: 2946596.5000\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 30136439144448.0000 - mae: 3205174.5000 - val_loss: 33630841733120.0000 - val_mae: 2957802.5000\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 29654444408832.0000 - mae: 3257899.0000 - val_loss: 33151380357120.0000 - val_mae: 2939147.0000\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 30052100079616.0000 - mae: 3261328.5000 - val_loss: 32589849034752.0000 - val_mae: 2934494.0000\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 29801201008640.0000 - mae: 3315169.2500 - val_loss: 32109175504896.0000 - val_mae: 2918576.5000\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 27664328949760.0000 - mae: 3213127.5000 - val_loss: 31609042501632.0000 - val_mae: 2906968.5000\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 27342957182976.0000 - mae: 3167449.2500 - val_loss: 31232033292288.0000 - val_mae: 2877702.2500\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 28762771030016.0000 - mae: 3254063.0000 - val_loss: 30440335343616.0000 - val_mae: 2893129.0000\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 27481660719104.0000 - mae: 3224133.2500 - val_loss: 29884176924672.0000 - val_mae: 2881265.5000\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 27991790845952.0000 - mae: 3202549.2500 - val_loss: 29330369413120.0000 - val_mae: 2868136.5000\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 26984964947968.0000 - mae: 3239939.5000 - val_loss: 28980744814592.0000 - val_mae: 2837043.2500\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 24606450647040.0000 - mae: 3055275.0000 - val_loss: 28557522763776.0000 - val_mae: 2811614.0000\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 26112449052672.0000 - mae: 3094432.5000 - val_loss: 28177132945408.0000 - val_mae: 2784619.5000\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 26000905732096.0000 - mae: 3075671.2500 - val_loss: 27679292129280.0000 - val_mae: 2772408.2500\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 24759228170240.0000 - mae: 3070260.0000 - val_loss: 26890203037696.0000 - val_mae: 2771435.2500\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 25010494242816.0000 - mae: 3099732.5000 - val_loss: 26489194020864.0000 - val_mae: 2756533.7500\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 25423438151680.0000 - mae: 3015110.5000 - val_loss: 26186050699264.0000 - val_mae: 2736633.2500\n",
      "Epoch 47/100\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 23503684239360.0000 - mae: 3006075.5000 - val_loss: 25714040504320.0000 - val_mae: 2722254.0000\n",
      "Epoch 48/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 25030140362752.0000 - mae: 3040248.0000 - val_loss: 25088248250368.0000 - val_mae: 2712660.0000\n",
      "Epoch 49/100\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 22834298486784.0000 - mae: 3031668.7500 - val_loss: 24485866504192.0000 - val_mae: 2701776.7500\n",
      "Epoch 50/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 22718940446720.0000 - mae: 3027556.0000 - val_loss: 24352573620224.0000 - val_mae: 2690159.7500\n",
      "Epoch 51/100\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 22043705737216.0000 - mae: 2886051.5000 - val_loss: 23842533670912.0000 - val_mae: 2676057.0000\n",
      "Epoch 52/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 22445318733824.0000 - mae: 2945980.7500 - val_loss: 23186380947456.0000 - val_mae: 2661384.7500\n",
      "Epoch 53/100\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 22200895668224.0000 - mae: 2975536.2500 - val_loss: 22459512258560.0000 - val_mae: 2657821.2500\n",
      "Epoch 54/100\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 22407502888960.0000 - mae: 2957141.0000 - val_loss: 22151656636416.0000 - val_mae: 2641429.5000\n",
      "Epoch 55/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 20293382832128.0000 - mae: 2902677.5000 - val_loss: 22087303430144.0000 - val_mae: 2617665.5000\n",
      "Epoch 56/100\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 22978272165888.0000 - mae: 2981542.5000 - val_loss: 21899669143552.0000 - val_mae: 2602391.5000\n",
      "Epoch 57/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 22571441455104.0000 - mae: 2938034.0000 - val_loss: 21805158891520.0000 - val_mae: 2588491.2500\n",
      "Epoch 58/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 20920584372224.0000 - mae: 2825619.7500 - val_loss: 21395547357184.0000 - val_mae: 2576968.5000\n",
      "Epoch 59/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 19933121478656.0000 - mae: 2800611.5000 - val_loss: 20734543921152.0000 - val_mae: 2566286.5000\n",
      "Epoch 60/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 20422131187712.0000 - mae: 2845140.7500 - val_loss: 20185610190848.0000 - val_mae: 2552767.5000\n",
      "Epoch 61/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 18968666439680.0000 - mae: 2816516.5000 - val_loss: 19789820985344.0000 - val_mae: 2537590.0000\n",
      "Epoch 62/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 19026564612096.0000 - mae: 2769060.7500 - val_loss: 19557968248832.0000 - val_mae: 2520814.5000\n",
      "Epoch 63/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 19283394428928.0000 - mae: 2820034.5000 - val_loss: 19112023556096.0000 - val_mae: 2505041.5000\n",
      "Epoch 64/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 17989648777216.0000 - mae: 2720193.7500 - val_loss: 18767316779008.0000 - val_mae: 2485657.5000\n",
      "Epoch 65/100\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 17444758355968.0000 - mae: 2683651.2500 - val_loss: 18311028932608.0000 - val_mae: 2468916.2500\n",
      "Epoch 66/100\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 18693301993472.0000 - mae: 2825991.5000 - val_loss: 17632979845120.0000 - val_mae: 2451691.5000\n",
      "Epoch 67/100\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 16757167226880.0000 - mae: 2702068.2500 - val_loss: 17440517914624.0000 - val_mae: 2431668.7500\n",
      "Epoch 68/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 16746511597568.0000 - mae: 2684744.7500 - val_loss: 17093326012416.0000 - val_mae: 2413734.5000\n",
      "Epoch 69/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 17126544900096.0000 - mae: 2689137.2500 - val_loss: 16687578480640.0000 - val_mae: 2394496.5000\n",
      "Epoch 70/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 17304425332736.0000 - mae: 2689955.2500 - val_loss: 16472653955072.0000 - val_mae: 2377190.0000\n",
      "Epoch 71/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 16697893322752.0000 - mae: 2707360.7500 - val_loss: 16255162515456.0000 - val_mae: 2358299.7500\n",
      "Epoch 72/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 16112673619968.0000 - mae: 2653412.0000 - val_loss: 16133031723008.0000 - val_mae: 2338552.5000\n",
      "Epoch 73/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 16220027879424.0000 - mae: 2653209.5000 - val_loss: 16247576068096.0000 - val_mae: 2327660.0000\n",
      "Epoch 74/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 16558731558912.0000 - mae: 2626236.5000 - val_loss: 15582239916032.0000 - val_mae: 2302967.2500\n",
      "Epoch 75/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 14872072945664.0000 - mae: 2502864.7500 - val_loss: 15230932353024.0000 - val_mae: 2280333.5000\n",
      "Epoch 76/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 16111057764352.0000 - mae: 2558505.2500 - val_loss: 14890838261760.0000 - val_mae: 2257928.2500\n",
      "Epoch 77/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 14995277479936.0000 - mae: 2552911.7500 - val_loss: 14821979324416.0000 - val_mae: 2240116.5000\n",
      "Epoch 78/100\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 16272500719616.0000 - mae: 2532241.7500 - val_loss: 14716453781504.0000 - val_mae: 2221859.0000\n",
      "Epoch 79/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 15601320853504.0000 - mae: 2604241.7500 - val_loss: 14659322118144.0000 - val_mae: 2206184.5000\n",
      "Epoch 80/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 15771260420096.0000 - mae: 2497339.0000 - val_loss: 14026554736640.0000 - val_mae: 2177935.5000\n",
      "Epoch 81/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 15252108345344.0000 - mae: 2518715.5000 - val_loss: 13386005872640.0000 - val_mae: 2149033.5000\n",
      "Epoch 82/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 14588681650176.0000 - mae: 2481270.5000 - val_loss: 12921814908928.0000 - val_mae: 2123688.2500\n",
      "Epoch 83/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 14198940631040.0000 - mae: 2501149.5000 - val_loss: 12699636334592.0000 - val_mae: 2102179.2500\n",
      "Epoch 84/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 13328113991680.0000 - mae: 2387065.2500 - val_loss: 12369113645056.0000 - val_mae: 2077580.6250\n",
      "Epoch 85/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 12483813179392.0000 - mae: 2336126.2500 - val_loss: 11995669594112.0000 - val_mae: 2051392.0000\n",
      "Epoch 86/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 13234544312320.0000 - mae: 2424830.0000 - val_loss: 11953789468672.0000 - val_mae: 2033024.7500\n",
      "Epoch 87/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 14334372610048.0000 - mae: 2414911.5000 - val_loss: 11700206043136.0000 - val_mae: 2007441.0000\n",
      "Epoch 88/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 10976083574784.0000 - mae: 2259399.0000 - val_loss: 11552383041536.0000 - val_mae: 1989954.2500\n",
      "Epoch 89/100\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 12023264968704.0000 - mae: 2292919.5000 - val_loss: 11626314989568.0000 - val_mae: 1987970.0000\n",
      "Epoch 90/100\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 12626639716352.0000 - mae: 2290527.5000 - val_loss: 11375087714304.0000 - val_mae: 1968959.6250\n",
      "Epoch 91/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 10162164203520.0000 - mae: 2125319.2500 - val_loss: 10796884033536.0000 - val_mae: 1928000.2500\n",
      "Epoch 92/100\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 11524256038912.0000 - mae: 2227295.0000 - val_loss: 10545914707968.0000 - val_mae: 1905091.2500\n",
      "Epoch 93/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 10784746766336.0000 - mae: 2195225.5000 - val_loss: 10216077787136.0000 - val_mae: 1882451.2500\n",
      "Epoch 94/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 11691471405056.0000 - mae: 2198781.5000 - val_loss: 10173043179520.0000 - val_mae: 1872127.6250\n",
      "Epoch 95/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 10473355345920.0000 - mae: 2162363.2500 - val_loss: 9654604136448.0000 - val_mae: 1848404.0000\n",
      "Epoch 96/100\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 10955121491968.0000 - mae: 2145363.5000 - val_loss: 9264418521088.0000 - val_mae: 1824381.6250\n",
      "Epoch 97/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 11056406593536.0000 - mae: 2188752.5000 - val_loss: 9022032838656.0000 - val_mae: 1809183.0000\n",
      "Epoch 98/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 8954667073536.0000 - mae: 2045728.6250 - val_loss: 9252488871936.0000 - val_mae: 1814025.3750\n",
      "Epoch 99/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 9320868610048.0000 - mae: 2016403.0000 - val_loss: 9330285871104.0000 - val_mae: 1811033.0000\n",
      "Epoch 100/100\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 9616468475904.0000 - mae: 1965337.6250 - val_loss: 8748979978240.0000 - val_mae: 1783583.6250\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2007276.4348418936\n",
      "61.515446011157735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test[col] = X_test[col].astype(int)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test[col] = X_test[col].astype(int)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test[col] = X_test[col].astype(int)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test[col] = X_test[col].astype(int)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test[col] = X_test[col].astype(int)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test[col] = X_test[col].astype(int)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test[col] = X_test[col].astype(int)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test[col] = X_test[col].astype(int)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test[col] = X_test[col].astype(int)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test[col] = X_test[col].astype(int)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test[col] = X_test[col].astype(int)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test[col] = X_test[col].astype(int)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test[col] = X_test[col].astype(int)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test[col] = X_test[col].astype(int)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test[col] = X_test[col].astype(int)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test[col] = X_test[col].astype(int)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test[col] = X_test[col].astype(int)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test[col] = X_test[col].astype(int)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test[col] = X_test[col].astype(int)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test[col] = X_test[col].astype(int)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test[col] = X_test[col].astype(int)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test[col] = X_test[col].astype(int)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test[col] = X_test[col].astype(int)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test[col] = X_test[col].astype(int)\n",
      "C:\\Users\\jakub\\AppData\\Local\\Temp/ipykernel_15280/3771872762.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test[col] = X_test[col].astype(int)\n"
     ]
    }
   ],
   "source": [
    "# fit a neural network model\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Resizing, Dropout, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# turn X_train bool to integers\n",
    "boolean_columns = [col for col in X_train.columns if X_train[col].dtype == bool]\n",
    "\n",
    "for col in boolean_columns:\n",
    "    X_train[col] = X_train[col].astype(int) \n",
    " \n",
    "\n",
    "# fit the model on the X_train and y_train\n",
    "model = Sequential()\n",
    "model.add(Dense(256, activation=\"relu\", input_dim=X_train.shape[1]))  # More complex first layer\n",
    "model.add(Dropout(0.3))  # Dropout to reduce overfitting\n",
    "model.add(Dense(128, activation=\"relu\"))  # Second layer\n",
    "model.add(Dropout(0.2))  # Additional Dropout layer\n",
    "model.add(Dense(64, activation=\"relu\"))  # Third layer\n",
    "model.add(Dense(32, activation=\"relu\"))  # Fourth layer\n",
    "model.add(Dropout(0.1))  # Dropout layer\n",
    "model.add(Dense(16, activation=\"relu\"))  # New additional layer\n",
    "model.add(Dense(1)) \n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"mean_squared_error\", metrics=[\"mae\"])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# get the predictions\n",
    "boolean_columns = [col for col in X_test.columns if X_test[col].dtype == bool]\n",
    "for col in boolean_columns:\n",
    "    X_test[col] = X_test[col].astype(int)\n",
    "\n",
    "y_hat = model.predict(X_test).flatten()\n",
    "\n",
    "# compute the mae\n",
    "mae = np.mean(np.abs(y_test - y_hat))\n",
    "\n",
    "# compute the mean percentage error\n",
    "percentage_error = np.mean(100*np.abs((y_test - y_hat) / y_test))\n",
    "\n",
    "print(mae)\n",
    "print(percentage_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model on the entire dataset, predict for year 2023\n",
    "best_model = rf_model\n",
    "\n",
    "# read the data\n",
    "data = pd.read_csv(\"data/tabular_data_ukraine.csv\")\n",
    "data = pd.get_dummies(data, columns=[\"region\"])\n",
    "\n",
    "# get train and test\n",
    "train_data = data[data[\"year\"] <= 2021]\n",
    "test_data = data[data[\"year\"] == 2022]\n",
    "\n",
    "X_train = train_data.drop(columns=[\"year\", \"gdp\"])\n",
    "y_train = train_data[\"gdp\"]\n",
    "\n",
    "X_test = test_data.drop(columns=[\"year\", \"gdp\"])\n",
    "\n",
    "# train \n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# predict\n",
    "y_hat = best_model.predict(X_test)\n",
    "\n",
    "# add the predictions to the test_data\n",
    "test_data = pd.read_csv(\"data/tabular_data_ukraine.csv\")\n",
    "test_data = test_data[test_data[\"year\"] == 2022]\n",
    "\n",
    "test_data[\"gdp_prediction\"] = y_hat\n",
    "\n",
    "results = test_data[[\"region\", \"gdp_prediction\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>region</th>\n",
       "      <th>gdp_prediction</th>\n",
       "      <th>gdp</th>\n",
       "      <th>change</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Vinnytsia_Oblast</td>\n",
       "      <td>2.593472e+06</td>\n",
       "      <td>4.337287e+06</td>\n",
       "      <td>-0.402052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Volyn_Oblast</td>\n",
       "      <td>2.142880e+06</td>\n",
       "      <td>1.915275e+06</td>\n",
       "      <td>0.118837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dnipropetrovsk_Oblast</td>\n",
       "      <td>2.433512e+06</td>\n",
       "      <td>1.284456e+07</td>\n",
       "      <td>-0.810542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Donetsk_Oblast</td>\n",
       "      <td>3.229062e+06</td>\n",
       "      <td>6.400417e+06</td>\n",
       "      <td>-0.495492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Zhytomyr_Oblast</td>\n",
       "      <td>2.388363e+06</td>\n",
       "      <td>3.007165e+06</td>\n",
       "      <td>-0.205776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Zakarpattia_Oblast</td>\n",
       "      <td>2.183994e+06</td>\n",
       "      <td>2.096846e+06</td>\n",
       "      <td>0.041561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Zaporizhia_Oblast</td>\n",
       "      <td>2.167940e+06</td>\n",
       "      <td>5.481753e+06</td>\n",
       "      <td>-0.604517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Ivano-Frankivsk_Oblast</td>\n",
       "      <td>2.856070e+06</td>\n",
       "      <td>3.312617e+06</td>\n",
       "      <td>-0.137820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Kyiv_Oblast</td>\n",
       "      <td>4.064309e+06</td>\n",
       "      <td>7.395152e+06</td>\n",
       "      <td>-0.450409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Kirovohrad_Oblast</td>\n",
       "      <td>2.244913e+06</td>\n",
       "      <td>2.512884e+06</td>\n",
       "      <td>-0.106639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Luhansk_Oblast</td>\n",
       "      <td>1.863142e+06</td>\n",
       "      <td>1.587944e+06</td>\n",
       "      <td>0.173304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Lviv_Oblast</td>\n",
       "      <td>5.427193e+06</td>\n",
       "      <td>7.122045e+06</td>\n",
       "      <td>-0.237973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Mykolaiv_Oblast</td>\n",
       "      <td>2.144088e+06</td>\n",
       "      <td>3.256755e+06</td>\n",
       "      <td>-0.341649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Odessa_Oblast</td>\n",
       "      <td>4.388065e+06</td>\n",
       "      <td>7.541502e+06</td>\n",
       "      <td>-0.418144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Poltava_Oblast</td>\n",
       "      <td>2.235210e+06</td>\n",
       "      <td>4.591725e+06</td>\n",
       "      <td>-0.513209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Rivne_Oblast</td>\n",
       "      <td>2.135594e+06</td>\n",
       "      <td>2.289795e+06</td>\n",
       "      <td>-0.067343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Sumy_Oblast</td>\n",
       "      <td>2.165460e+06</td>\n",
       "      <td>2.509821e+06</td>\n",
       "      <td>-0.137205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Ternopil_Oblast</td>\n",
       "      <td>1.874998e+06</td>\n",
       "      <td>2.016336e+06</td>\n",
       "      <td>-0.070096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Kharkiv_Oblast</td>\n",
       "      <td>2.184452e+06</td>\n",
       "      <td>7.574767e+06</td>\n",
       "      <td>-0.711615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Kherson_Oblast</td>\n",
       "      <td>2.141190e+06</td>\n",
       "      <td>2.063189e+06</td>\n",
       "      <td>0.037806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Khmelnytskyi_Oblast</td>\n",
       "      <td>2.442673e+06</td>\n",
       "      <td>2.891974e+06</td>\n",
       "      <td>-0.155361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Cherkasy_Oblast</td>\n",
       "      <td>2.460396e+06</td>\n",
       "      <td>3.300715e+06</td>\n",
       "      <td>-0.254587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Chernivtsi_Oblast</td>\n",
       "      <td>1.281285e+06</td>\n",
       "      <td>1.381072e+06</td>\n",
       "      <td>-0.072253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Chernihiv_Oblast</td>\n",
       "      <td>2.216583e+06</td>\n",
       "      <td>2.327018e+06</td>\n",
       "      <td>-0.047458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Kyiv</td>\n",
       "      <td>2.779582e+07</td>\n",
       "      <td>3.163222e+07</td>\n",
       "      <td>-0.121281</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    region  gdp_prediction           gdp    change\n",
       "0         Vinnytsia_Oblast    2.593472e+06  4.337287e+06 -0.402052\n",
       "1             Volyn_Oblast    2.142880e+06  1.915275e+06  0.118837\n",
       "2    Dnipropetrovsk_Oblast    2.433512e+06  1.284456e+07 -0.810542\n",
       "3           Donetsk_Oblast    3.229062e+06  6.400417e+06 -0.495492\n",
       "4          Zhytomyr_Oblast    2.388363e+06  3.007165e+06 -0.205776\n",
       "5       Zakarpattia_Oblast    2.183994e+06  2.096846e+06  0.041561\n",
       "6        Zaporizhia_Oblast    2.167940e+06  5.481753e+06 -0.604517\n",
       "7   Ivano-Frankivsk_Oblast    2.856070e+06  3.312617e+06 -0.137820\n",
       "8              Kyiv_Oblast    4.064309e+06  7.395152e+06 -0.450409\n",
       "9        Kirovohrad_Oblast    2.244913e+06  2.512884e+06 -0.106639\n",
       "10          Luhansk_Oblast    1.863142e+06  1.587944e+06  0.173304\n",
       "11             Lviv_Oblast    5.427193e+06  7.122045e+06 -0.237973\n",
       "12         Mykolaiv_Oblast    2.144088e+06  3.256755e+06 -0.341649\n",
       "13           Odessa_Oblast    4.388065e+06  7.541502e+06 -0.418144\n",
       "14          Poltava_Oblast    2.235210e+06  4.591725e+06 -0.513209\n",
       "15            Rivne_Oblast    2.135594e+06  2.289795e+06 -0.067343\n",
       "16             Sumy_Oblast    2.165460e+06  2.509821e+06 -0.137205\n",
       "17         Ternopil_Oblast    1.874998e+06  2.016336e+06 -0.070096\n",
       "18          Kharkiv_Oblast    2.184452e+06  7.574767e+06 -0.711615\n",
       "19          Kherson_Oblast    2.141190e+06  2.063189e+06  0.037806\n",
       "20     Khmelnytskyi_Oblast    2.442673e+06  2.891974e+06 -0.155361\n",
       "21         Cherkasy_Oblast    2.460396e+06  3.300715e+06 -0.254587\n",
       "22       Chernivtsi_Oblast    1.281285e+06  1.381072e+06 -0.072253\n",
       "23        Chernihiv_Oblast    2.216583e+06  2.327018e+06 -0.047458\n",
       "24                    Kyiv    2.779582e+07  3.163222e+07 -0.121281"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compare results with gdp from the previous year\n",
    "previous_year = pd.read_csv(\"data/tabular_data_ukraine.csv\")\n",
    "previous_year = previous_year[previous_year[\"year\"] == 2021]\n",
    "previous_year = previous_year[[\"region\", \"gdp\"]]\n",
    "\n",
    "# merge on region\n",
    "results = results.merge(previous_year, on=\"region\")\n",
    "\n",
    "# compute change in percentages\n",
    "results[\"change\"] = (results[\"gdp_prediction\"] - results[\"gdp\"]) / results[\"gdp\"]\n",
    "\n",
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.3221699063478681"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(results[\"change\"]) # 23% drop\n",
    "\n",
    "(results[\"gdp_prediction\"].sum() - results[\"gdp\"].sum()) / results[\"gdp\"].sum() # 31.8% drop, World Bank esimates 29.1%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>gdp</th>\n",
       "      <th>nearnad_snow_cov_mean</th>\n",
       "      <th>nearnad_snow_cov_median</th>\n",
       "      <th>nearnad_snow_cov_std</th>\n",
       "      <th>nearnad_snow_free_mean</th>\n",
       "      <th>nearnad_snow_free_median</th>\n",
       "      <th>nearnad_snow_free_std</th>\n",
       "      <th>offnad_snow_cov_mean</th>\n",
       "      <th>offnad_snow_cov_median</th>\n",
       "      <th>...</th>\n",
       "      <th>region_Odessa_Oblast</th>\n",
       "      <th>region_Poltava_Oblast</th>\n",
       "      <th>region_Rivne_Oblast</th>\n",
       "      <th>region_Sumy_Oblast</th>\n",
       "      <th>region_Ternopil_Oblast</th>\n",
       "      <th>region_Vinnytsia_Oblast</th>\n",
       "      <th>region_Volyn_Oblast</th>\n",
       "      <th>region_Zakarpattia_Oblast</th>\n",
       "      <th>region_Zaporizhia_Oblast</th>\n",
       "      <th>region_Zhytomyr_Oblast</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012</td>\n",
       "      <td>1.479700e+07</td>\n",
       "      <td>2.647959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>46.242333</td>\n",
       "      <td>1.184331</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43.000168</td>\n",
       "      <td>2.546648</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2013</td>\n",
       "      <td>1.469342e+07</td>\n",
       "      <td>2.343649</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.583948</td>\n",
       "      <td>0.961109</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.325981</td>\n",
       "      <td>1.299532</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>2014</td>\n",
       "      <td>1.397344e+07</td>\n",
       "      <td>3.100126</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.716915</td>\n",
       "      <td>0.974721</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.166740</td>\n",
       "      <td>2.828148</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>2015</td>\n",
       "      <td>1.261802e+07</td>\n",
       "      <td>2.467506</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.238729</td>\n",
       "      <td>0.930540</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.208755</td>\n",
       "      <td>2.504813</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>2016</td>\n",
       "      <td>1.241613e+07</td>\n",
       "      <td>3.240427</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.904961</td>\n",
       "      <td>1.048079</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.651880</td>\n",
       "      <td>1.986214</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>2017</td>\n",
       "      <td>1.266445e+07</td>\n",
       "      <td>2.544386</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.548876</td>\n",
       "      <td>1.243090</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52.883580</td>\n",
       "      <td>2.503581</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>2018</td>\n",
       "      <td>1.298107e+07</td>\n",
       "      <td>3.188704</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55.825475</td>\n",
       "      <td>1.250778</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.561350</td>\n",
       "      <td>2.398645</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>2019</td>\n",
       "      <td>1.346136e+07</td>\n",
       "      <td>3.057841</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.197801</td>\n",
       "      <td>1.254185</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.082681</td>\n",
       "      <td>2.261230</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>2020</td>\n",
       "      <td>1.251907e+07</td>\n",
       "      <td>3.204058</td>\n",
       "      <td>0.0</td>\n",
       "      <td>67.320126</td>\n",
       "      <td>1.225739</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.918232</td>\n",
       "      <td>1.514519</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>2021</td>\n",
       "      <td>1.284456e+07</td>\n",
       "      <td>4.045430</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.211584</td>\n",
       "      <td>1.338012</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.280639</td>\n",
       "      <td>3.359127</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>2022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.532210</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.691905</td>\n",
       "      <td>0.067109</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.584661</td>\n",
       "      <td>0.433461</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>2023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.711638</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.289871</td>\n",
       "      <td>0.193923</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.717158</td>\n",
       "      <td>0.232041</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     year           gdp  nearnad_snow_cov_mean  nearnad_snow_cov_median  \\\n",
       "2    2012  1.479700e+07               2.647959                      0.0   \n",
       "27   2013  1.469342e+07               2.343649                      0.0   \n",
       "52   2014  1.397344e+07               3.100126                      0.0   \n",
       "77   2015  1.261802e+07               2.467506                      0.0   \n",
       "102  2016  1.241613e+07               3.240427                      0.0   \n",
       "127  2017  1.266445e+07               2.544386                      0.0   \n",
       "152  2018  1.298107e+07               3.188704                      0.0   \n",
       "177  2019  1.346136e+07               3.057841                      0.0   \n",
       "202  2020  1.251907e+07               3.204058                      0.0   \n",
       "227  2021  1.284456e+07               4.045430                      0.0   \n",
       "252  2022           NaN               1.532210                      0.0   \n",
       "277  2023           NaN               0.711638                      0.0   \n",
       "\n",
       "     nearnad_snow_cov_std  nearnad_snow_free_mean  nearnad_snow_free_median  \\\n",
       "2               46.242333                1.184331                       0.0   \n",
       "27              33.583948                0.961109                       0.0   \n",
       "52              40.716915                0.974721                       0.0   \n",
       "77              50.238729                0.930540                       0.0   \n",
       "102             31.904961                1.048079                       0.0   \n",
       "127             39.548876                1.243090                       0.0   \n",
       "152             55.825475                1.250778                       0.0   \n",
       "177             51.197801                1.254185                       0.0   \n",
       "202             67.320126                1.225739                       0.0   \n",
       "227             45.211584                1.338012                       0.0   \n",
       "252             30.691905                0.067109                       0.0   \n",
       "277             13.289871                0.193923                       0.0   \n",
       "\n",
       "     nearnad_snow_free_std  offnad_snow_cov_mean  offnad_snow_cov_median  ...  \\\n",
       "2                43.000168              2.546648                     0.0  ...   \n",
       "27               30.325981              1.299532                     0.0  ...   \n",
       "52               25.166740              2.828148                     0.0  ...   \n",
       "77               26.208755              2.504813                     0.0  ...   \n",
       "102              41.651880              1.986214                     0.0  ...   \n",
       "127              52.883580              2.503581                     0.0  ...   \n",
       "152              50.561350              2.398645                     0.0  ...   \n",
       "177              41.082681              2.261230                     0.0  ...   \n",
       "202              30.918232              1.514519                     0.0  ...   \n",
       "227              31.280639              3.359127                     0.0  ...   \n",
       "252               3.584661              0.433461                     0.0  ...   \n",
       "277              13.717158              0.232041                     0.0  ...   \n",
       "\n",
       "     region_Odessa_Oblast  region_Poltava_Oblast  region_Rivne_Oblast  \\\n",
       "2                   False                  False                False   \n",
       "27                  False                  False                False   \n",
       "52                  False                  False                False   \n",
       "77                  False                  False                False   \n",
       "102                 False                  False                False   \n",
       "127                 False                  False                False   \n",
       "152                 False                  False                False   \n",
       "177                 False                  False                False   \n",
       "202                 False                  False                False   \n",
       "227                 False                  False                False   \n",
       "252                 False                  False                False   \n",
       "277                 False                  False                False   \n",
       "\n",
       "     region_Sumy_Oblast  region_Ternopil_Oblast  region_Vinnytsia_Oblast  \\\n",
       "2                 False                   False                    False   \n",
       "27                False                   False                    False   \n",
       "52                False                   False                    False   \n",
       "77                False                   False                    False   \n",
       "102               False                   False                    False   \n",
       "127               False                   False                    False   \n",
       "152               False                   False                    False   \n",
       "177               False                   False                    False   \n",
       "202               False                   False                    False   \n",
       "227               False                   False                    False   \n",
       "252               False                   False                    False   \n",
       "277               False                   False                    False   \n",
       "\n",
       "     region_Volyn_Oblast  region_Zakarpattia_Oblast  region_Zaporizhia_Oblast  \\\n",
       "2                  False                      False                     False   \n",
       "27                 False                      False                     False   \n",
       "52                 False                      False                     False   \n",
       "77                 False                      False                     False   \n",
       "102                False                      False                     False   \n",
       "127                False                      False                     False   \n",
       "152                False                      False                     False   \n",
       "177                False                      False                     False   \n",
       "202                False                      False                     False   \n",
       "227                False                      False                     False   \n",
       "252                False                      False                     False   \n",
       "277                False                      False                     False   \n",
       "\n",
       "     region_Zhytomyr_Oblast  \n",
       "2                     False  \n",
       "27                    False  \n",
       "52                    False  \n",
       "77                    False  \n",
       "102                   False  \n",
       "127                   False  \n",
       "152                   False  \n",
       "177                   False  \n",
       "202                   False  \n",
       "227                   False  \n",
       "252                   False  \n",
       "277                   False  \n",
       "\n",
       "[12 rows x 45 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select region_Dnipropetrovsk_Oblast == 1\n",
    "subset = data[data[\"region_Dnipropetrovsk_Oblast\"] == 1]\n",
    "subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>gdp</th>\n",
       "      <th>nearnad_snow_cov_mean</th>\n",
       "      <th>nearnad_snow_cov_median</th>\n",
       "      <th>nearnad_snow_cov_std</th>\n",
       "      <th>nearnad_snow_free_mean</th>\n",
       "      <th>nearnad_snow_free_median</th>\n",
       "      <th>nearnad_snow_free_std</th>\n",
       "      <th>offnad_snow_cov_mean</th>\n",
       "      <th>offnad_snow_cov_median</th>\n",
       "      <th>...</th>\n",
       "      <th>region_Odessa_Oblast</th>\n",
       "      <th>region_Poltava_Oblast</th>\n",
       "      <th>region_Rivne_Oblast</th>\n",
       "      <th>region_Sumy_Oblast</th>\n",
       "      <th>region_Ternopil_Oblast</th>\n",
       "      <th>region_Vinnytsia_Oblast</th>\n",
       "      <th>region_Volyn_Oblast</th>\n",
       "      <th>region_Zakarpattia_Oblast</th>\n",
       "      <th>region_Zaporizhia_Oblast</th>\n",
       "      <th>region_Zhytomyr_Oblast</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.945420</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.503158</td>\n",
       "      <td>0.301091</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.865808</td>\n",
       "      <td>1.307311</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.767207</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.420684</td>\n",
       "      <td>0.102880</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.273735</td>\n",
       "      <td>0.682606</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>2.647959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>46.242333</td>\n",
       "      <td>1.184331</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43.000168</td>\n",
       "      <td>2.546648</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>3.566429</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.045082</td>\n",
       "      <td>1.317057</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.056379</td>\n",
       "      <td>3.413734</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.717199</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.332430</td>\n",
       "      <td>0.201979</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.190241</td>\n",
       "      <td>1.358591</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>2021</td>\n",
       "      <td>110.225016</td>\n",
       "      <td>0.622373</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.658013</td>\n",
       "      <td>0.196125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.930659</td>\n",
       "      <td>0.655630</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>2021</td>\n",
       "      <td>105.572198</td>\n",
       "      <td>1.308531</td>\n",
       "      <td>0.0</td>\n",
       "      <td>73.238358</td>\n",
       "      <td>0.349940</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.114947</td>\n",
       "      <td>1.509102</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>2021</td>\n",
       "      <td>104.896846</td>\n",
       "      <td>0.363287</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.754161</td>\n",
       "      <td>0.155258</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.675048</td>\n",
       "      <td>0.342800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>2021</td>\n",
       "      <td>97.226457</td>\n",
       "      <td>0.404712</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.745696</td>\n",
       "      <td>0.107963</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.970843</td>\n",
       "      <td>0.414831</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>2021</td>\n",
       "      <td>114.740464</td>\n",
       "      <td>2.208380</td>\n",
       "      <td>0.0</td>\n",
       "      <td>64.008319</td>\n",
       "      <td>0.955585</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.319962</td>\n",
       "      <td>2.248791</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     year         gdp  nearnad_snow_cov_mean  nearnad_snow_cov_median  \\\n",
       "0    2012  100.000000               0.945420                      0.0   \n",
       "1    2012  100.000000               0.767207                      0.0   \n",
       "2    2012  100.000000               2.647959                      0.0   \n",
       "3    2012  100.000000               3.566429                      0.0   \n",
       "4    2012  100.000000               0.717199                      0.0   \n",
       "..    ...         ...                    ...                      ...   \n",
       "245  2021  110.225016               0.622373                      0.0   \n",
       "246  2021  105.572198               1.308531                      0.0   \n",
       "247  2021  104.896846               0.363287                      0.0   \n",
       "248  2021   97.226457               0.404712                      0.0   \n",
       "249  2021  114.740464               2.208380                      0.0   \n",
       "\n",
       "     nearnad_snow_cov_std  nearnad_snow_free_mean  nearnad_snow_free_median  \\\n",
       "0               13.503158                0.301091                       0.0   \n",
       "1                6.420684                0.102880                       0.0   \n",
       "2               46.242333                1.184331                       0.0   \n",
       "3               39.045082                1.317057                       0.0   \n",
       "4               11.332430                0.201979                       0.0   \n",
       "..                    ...                     ...                       ...   \n",
       "245              8.658013                0.196125                       0.0   \n",
       "246             73.238358                0.349940                       0.0   \n",
       "247              4.754161                0.155258                       0.0   \n",
       "248              7.745696                0.107963                       0.0   \n",
       "249             64.008319                0.955585                       0.0   \n",
       "\n",
       "     nearnad_snow_free_std  offnad_snow_cov_mean  offnad_snow_cov_median  ...  \\\n",
       "0                 3.865808              1.307311                     0.0  ...   \n",
       "1                 2.273735              0.682606                     0.0  ...   \n",
       "2                43.000168              2.546648                     0.0  ...   \n",
       "3                14.056379              3.413734                     0.0  ...   \n",
       "4                 3.190241              1.358591                     0.0  ...   \n",
       "..                     ...                   ...                     ...  ...   \n",
       "245               3.930659              0.655630                     0.0  ...   \n",
       "246              38.114947              1.509102                     0.0  ...   \n",
       "247               3.675048              0.342800                     0.0  ...   \n",
       "248               3.970843              0.414831                     0.0  ...   \n",
       "249              18.319962              2.248791                     0.0  ...   \n",
       "\n",
       "     region_Odessa_Oblast  region_Poltava_Oblast  region_Rivne_Oblast  \\\n",
       "0                   False                  False                False   \n",
       "1                   False                  False                False   \n",
       "2                   False                  False                False   \n",
       "3                   False                  False                False   \n",
       "4                   False                  False                False   \n",
       "..                    ...                    ...                  ...   \n",
       "245                 False                  False                False   \n",
       "246                 False                  False                False   \n",
       "247                 False                  False                False   \n",
       "248                 False                  False                False   \n",
       "249                 False                  False                False   \n",
       "\n",
       "     region_Sumy_Oblast  region_Ternopil_Oblast  region_Vinnytsia_Oblast  \\\n",
       "0                 False                   False                     True   \n",
       "1                 False                   False                    False   \n",
       "2                 False                   False                    False   \n",
       "3                 False                   False                    False   \n",
       "4                 False                   False                    False   \n",
       "..                  ...                     ...                      ...   \n",
       "245               False                   False                    False   \n",
       "246               False                   False                    False   \n",
       "247               False                   False                    False   \n",
       "248               False                   False                    False   \n",
       "249               False                   False                    False   \n",
       "\n",
       "     region_Volyn_Oblast  region_Zakarpattia_Oblast  region_Zaporizhia_Oblast  \\\n",
       "0                  False                      False                     False   \n",
       "1                   True                      False                     False   \n",
       "2                  False                      False                     False   \n",
       "3                  False                      False                     False   \n",
       "4                  False                      False                     False   \n",
       "..                   ...                        ...                       ...   \n",
       "245                False                      False                     False   \n",
       "246                False                      False                     False   \n",
       "247                False                      False                     False   \n",
       "248                False                      False                     False   \n",
       "249                False                      False                     False   \n",
       "\n",
       "     region_Zhytomyr_Oblast  \n",
       "0                     False  \n",
       "1                     False  \n",
       "2                     False  \n",
       "3                     False  \n",
       "4                      True  \n",
       "..                      ...  \n",
       "245                   False  \n",
       "246                   False  \n",
       "247                   False  \n",
       "248                   False  \n",
       "249                   False  \n",
       "\n",
       "[250 rows x 45 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
